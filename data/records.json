{"papers":[{"url":"https://www.semanticscholar.org/paper/b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels","venue":"ArXiv","year":2022,"referenceCount":136,"citationCount":4,"influentialCitationCount":0,"authors":"Phillip Rust,Jonas F. Lotz,Emanuele Bugliarello,Elizabeth Salesky,Miryam de Lhoneux,Desmond Elliott","citations":[{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"fb41147776fd3ef8c3370ce2574efc15486c9a0f","title":"Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding"},{"paperId":"527603f0d3c120bbe56753ce4cd7e5a41e0d5e6a","title":"Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems"},{"paperId":"134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?"}],"references":[{"paperId":"33ebe01c8705a19def511040686d198871145409","title":"Story Beyond the Eye: Glyph Positions Break PDF Text Redaction"},{"paperId":"18c92da1b7f7f8ea6877e5b3a0d5a6df14a09e00","title":"SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners"},{"paperId":"8f26262437bde0ff8fe5e14d5a6cb4cc05a495ef","title":"Multimodal Masked Autoencoders Learn Transferable Representations"},{"paperId":"0d273fe1504a6b4251a773796853c75c2514df03","title":"Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models"},{"paperId":"88b15679a7e60a99d5da1f1fde8aa332b368e1b8","title":"Masked Autoencoders As Spatiotemporal Learners"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"b05f5006a879fab3668365918a15dd35acd77204","title":"Language Contamination Explains the Cross-lingual Capabilities of English Pretrained Models"},{"paperId":"ac6e85cc3f105567e20e019f5752ed38a0a34a25","title":"Breaking Character: Are Subwords Good Enough for MRLs After All?"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"3682a8716817743c6341003b2a121fbd183d6bbc","title":"MAE-AST: Masked Autoencoding Audio Spectrogram Transformer"},{"paperId":"a41196670d894ac8304d06f9bfeb0dd84fa5cb5d","title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"},{"paperId":"a8fffb507d1790851550af5e4ebdd06e5bae1cee","title":"Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation"},{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"b28e23afe988ad83ab5bd9ce7f826c140c533be8","title":"Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages"},{"paperId":"2d439ec2c301d058bd4a8b4743328e3d9939625e","title":"Should You Mask 15% in Masked Language Modeling?"},{"paperId":"2ec3fb471f059fe401c7e1d7d7a199ba4feae814","title":"JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension"},{"paperId":"44f75282915d468413e38ac0e1f59b3ee6860485","title":"Does Transliteration Help Multilingual Language Modeling?"},{"paperId":"d0336e5be72e97e0493b1ba77ef8ec3c349d496a","title":"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"5b540745f4b51f95bf90fb3420e51edb037fc51a","title":"The MultiBERTs: BERT Reproductions for Robustness Analysis"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"dab46dd3985d1de5cd6549319797ab3705b6a801","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"7a670e9c4cf6655cfbcacf169565d4d645c0d475","title":"Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":null,"title":"Story beyond the eye"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"7f631586a368f1762866b01ff9f43c265851d52e","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"03aa38229a6c853d96430edfc8f1542598f2116e","title":"AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain"},{"paperId":"dbd9fab59832369040661bb050daef6de405230c","title":"Allocating Large Vocabulary Capacity for Cross-Lingual Language Model Pre-Training"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"59c0c6b62e33850cda08663d4c9ecabcf5d21596","title":"IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization"},{"paperId":"8dad3a12d2a652122458dd377f62025354b17a2a","title":"Subword Mapping and Anchoring across Languages"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"964f079b80e6f3ce3164dd883dd1836299f0dba3","title":"GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph"},{"paperId":"06431546c21d7c2528aaa170c2e1078e0a82d12e","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer"},{"paperId":"a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study"},{"paperId":"c89ff2a0416a6d0870e724953a61a798deee238f","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages"},{"paperId":"a76c98c6814ce8de07707b81c18520af508b7184","title":"BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"13bcfb944779165983aaef22cec8a3bbd3e98e62","title":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"29599829d26b4acaaf0ad88698ed4362d33b2ae7","title":"When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"575ac3f36e9fddeb258e2f639e26a6a7ec35160a","title":"Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"3ba96d6f6f4a828b6b32812d7867d86897b88df1","title":"Morphologically-Guided Segmentation For Translation of Agglutinative Low-Resource Languages"},{"paperId":"b62a56f629f77acce9ea69456d67dd77de1c7dfb","title":"Clustering Monolingual Vocabularies to Improve Cross-Lingual Generalization"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"f5dfed82b0c8747e41a1206f52a6d0ea3dce4a5c","title":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"85dc7829455819283270eb643817bcf97133464d","title":"From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"0a0bb62e56929d2c55ad6b9f18386f4fd7ba520a","title":"Towards End-to-End In-Image Neural Machine Translation"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"335a05c16fcbc7a2c2bf5221299de608b08030f0","title":"From Hero to Zéroe: A Benchmark of Low-Level Adversarial Attacks"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"e4ad5c24985127e015be86a53d57e1bb43876b4c","title":"On Romanization for Model Transfer Between Scripts in Neural Machine Translation"},{"paperId":"df29486c04eafd004f2f0816e84c798783802cdf","title":"Transliteration for Cross-Lingual Morphological Inflection"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"14489ec7893e373a0dcc9555c52b99b2b3a429f6","title":"Are All Languages Created Equal in Multilingual BERT?"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"b805693c17961af2cc7f859c1a54320b26036f46","title":"Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"0495d9df8eb84dcdab4e5536179823cd26279949","title":"Big Transfer (BiT): General Visual Representation Learning"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"21079df28af037f93620a0bd63f04fe49cf5df7d","title":"On the Importance of Tokenization in Arabic Embedding Models"},{"paperId":null,"title":"BERT: Pre-training of deep"},{"paperId":"e4f19271b9491b0297a03318a96b8a5157873acd","title":"Writing Across the World's Languages: Deep Internationalization for Gboard, the Google Keyboard"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"6ffb1cc32ddde6ae01e2fc0286eafa116ade0ffb","title":"KorQuAD1.0: Korean QA Dataset for Machine Reading Comprehension"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"c0aaee2337e5af680e5dca1bfc349a737dfec573","title":"Fixing the train-test resolution discrepancy"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"541263935bfde368af9a5c4537fd1578e75d36d7","title":"Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"e547f4c0728552c1c41df44b7c6469198cf44924","title":"MorphoBERT: a Persian NER System with BERT and Morphological Analysis"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Exploring BERT’s Vocabulary"},{"paperId":null,"title":"Exploring BERT's Vocabulary. Blog Post"},{"paperId":"4fcec7ea62825953ac8d483cfa5c748b4daa4e7e","title":"The Coptic Universal Dependency Treebank"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e74ad670a6cbd693e225b0150f3a33fc1931faf7","title":"Universal Dependencies Version 2 for Japanese"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"2c4574c7f42ac3e887670876a37ea6708a77966e","title":"AROMA: A Recursive Deep Learning Model for Opinion Mining in Arabic as a Low Resource Language"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2","title":"Deep Biaffine Attention for Neural Dependency Parsing"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"b022f2a277a4bf5f42382e86e4380b96340b9e86","title":"SGDR: Stochastic Gradient Descent with Warm Restarts"},{"paperId":"3a29aa4eff48624752c07059a44d3288a678c8ab","title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"4361e64f2d12d63476fdc88faf72a0f70d9a2ffb","title":"Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":null,"title":"Ud_chinese-gsd. GitHub repository, 2016"},{"paperId":null,"title":"Ud_chinese-gsd. GitHub repository"},{"paperId":"f04df4e20a18358ea2f689b4c129781628ef7fc1","title":"A large annotated corpus for learning natural language inference"},{"paperId":"753e30826f1908a62a8d251fc6b1b598f86d2bb2","title":"Shared Tasks of the 2015 Workshop on Noisy User-generated Text: Twitter Lexical Normalization and Named Entity Recognition"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"82cf69e48ede65b9d1f419da786c0349342d449d","title":"A Gold Standard Dependency Corpus for English"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"77ef6c449d3b7f5f5c55a06564b79eb4438c85b9","title":"Prague Dependency Style Treebank for Tamil"},{"paperId":"2fa38d8b67c7f59ac2a2bf1b53173a973422d8e2","title":"Prague Arabic Dependency Treebank 1.0"},{"paperId":"a7d1c7979b2d8205e9382e9f4fc39eb999fcd955","title":"Building a Large Syntactically-Annotated Corpus of Vietnamese"},{"paperId":"6f59b8056fd7367f5b5088d9c566d6849ea8a663","title":"Hindi Syntax: Annotating Dependency, Lexical Predicate-Argument Structure, and Phrase Structure"},{"paperId":null,"title":"Hassanová. Prague arabic dependency treebank"},{"paperId":"bb7d7dfad94e6a94bf0926fd28e6a0dd7d13b278","title":"Advances on natural language processing"},{"paperId":"944e7729088c5f36cbe0b0801ca0fea432dc5326","title":"Pango, an open-source Unicode text layout engine"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"for an overview of the rendering pipeline"},{"paperId":null,"title":"Transformers: State-of-the-art natural Preprint. Work in progress. language processing"},{"paperId":null,"title":"European Language Resources Association (ELRA"}],"id":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","summary":"PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels, and is more robust to noisy text inputs than BERT, further confirming the benefits of modelling language with pixels."},{"url":"https://www.semanticscholar.org/paper/e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization","venue":"ICLR","year":2021,"referenceCount":69,"citationCount":39,"influentialCitationCount":7,"authors":"Yi Tay,V. Tran,Sebastian Ruder,Jai Gupta,Hyung Won Chung,Dara Bahri,Zhen Qin,Simon Baumgartner,Cong Yu,Donald Metzler","citations":[{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"ecb788b406e4e605526c5127b20bdcc5f8e55d39","title":"Small Character Models Match Large Word Models for Autocomplete Under Memory Constraints"},{"paperId":"110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward"},{"paperId":"487f9e74fff5e29b8aa37ba551edab71cfcbd256","title":"L2-GEN: A Neural Phoneme Paraphrasing Approach to L2 Speech Synthesis for Mispronunciation Diagnosis"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"},{"paperId":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"c13b69672ebee01914b0be69d052161350f653e8","title":"CONSENT: Context Sensitive Transformer for Bold Words Classification"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"645b10b7802a035e034488e3640fc0bc415de34c","title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"2ffacbeeebd3d9e7467610057b4308635a165b6b","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"0b9e130c6305de7766697ba7655f56010aaffd61","title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction"},{"paperId":"a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia"},{"paperId":"7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers"},{"paperId":"b0c0c742789dfc98dfc2293c6ef7c7944f2c9122","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"2d4f66046bb436864cd6bf589e3a931c405f9f44","title":"Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color"},{"paperId":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data"},{"paperId":"e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE"},{"paperId":"e34a59a7391a28200ff9052c13bd4498a8eaa4af","title":"S CALE E FFICIENTLY : I NSIGHTS FROM P RE - TRAINING AND F INE - TUNING T RANSFORMERS"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"2365410a710b421b2295cdca0074946cb50bb1d4","title":"Are Pre-trained Convolutions Better than Pre-trained Transformers?"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"7e9ff94476f41041c75e253e84f487db00e9c861","title":"Long Range Arena: A Benchmark for Efficient Transformers"},{"paperId":"bc87279d4b32a425377ff18ab63f7ecf95ff228c","title":"Rethinking embedding coupling in pre-trained language models"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"466f7cfe7442738ee974b12939b4c2e32ee098bf","title":"Rethinking Attention with Performers"},{"paperId":"26b277f2d7cd86b67bc3572257557edc4640b8e9","title":"Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"4ca3b0ea12f02e2dea01a4aa505956bae5500a09","title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"bdbf780dfd6b3eb0c9e980887feae5f23af15bc4","title":"GLU Variants Improve Transformer"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"b611a8095630557229dc5fb6b07c272f1cd614da","title":"Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"2270b8628fd8ca67ae39d277f45bc3c38ac63d5f","title":"Mesh-TensorFlow: Deep Learning for Supercomputers"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"1db9bd18681b96473f3c82b21edc9240b44dc329","title":"Image Transformer"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":null,"title":"FRAGE: FrequencyAgnostic Word Representation"},{"paperId":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"deff3b0635e141297238797d924d7a9aba3a132a","title":"Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU"},{"paperId":"6db294e9309349d82df47a912ccc47eab1dc1358","title":"Sequence Modeling via Segmentations"},{"paperId":"6f35b070c250507dbec8a7365cf01b25eb66d792","title":"Ex Machina: Personal Attacks Seen at Scale"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"649d03490ef72c5274e3bccd03d7a299d2f8da91","title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"3bf2e6941dbb87ac0d2c771c159e1e27366a26e3","title":"Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics"},{"paperId":"0c09f282d78f42f77d73a52b08cb425e1cabb9b2","title":"Are Vowels and Consonants Processed Differently? Event-related Potential Evidence with a Delayed Letter Paradigm"},{"paperId":"475354f10798f110d34792b6d88f31d6d5cb099e","title":"Automatically Constructing a Corpus of Sentential Paraphrases"},{"paperId":"084c55d6432265785e3ff86a2e900a49d501c00a","title":"Foundations of statistical natural language processing"},{"paperId":"98b405fd0153f24e8359448d79acc49c3ffc8f4c","title":"The relative contribution of consonants and vowels to word identification during reading"},{"paperId":null,"title":"Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee"}],"id":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","summary":"This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion and paves the way for highly performant token-free models that are trained completely end-to-end."},{"url":"https://www.semanticscholar.org/paper/9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information","venue":"ArXiv","year":2021,"referenceCount":32,"citationCount":2,"influentialCitationCount":0,"authors":"Yuval Pinter,A. Stent,Mark Dredze,Jacob Eisenstein","citations":[{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"188cd686fb2200f237f688dbda7f64ffc75e67ac","title":"Subword Pooling Makes a Difference"},{"paperId":"e7a2ffd26cd76e5b662ecb8624ecb3e177dbb8da","title":"Pitfalls of Static Language Modelling"},{"paperId":null,"title":"Recent Advances in Language Model Fine-tuning"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"641250e235d81e5b5c0023c32e71731aa0b0027c","title":"Multi-resolution Annotations for Emoji Prediction"},{"paperId":"d7a793e7ce1e42e1feaadb026633e481131a8692","title":"Char2Subword: Extending the Subword Embedding Space from Pre-trained Models Using Robust Character Compositionality"},{"paperId":"9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2","title":"TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"b08545e1281c1eb748e4474687eb61fd3b25d1a6","title":"NYTWIT: A Dataset of Novel Words in the New York Times"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a022bda79947d1f656a1164003c1b3ae9a843df9","title":"How to Fine-Tune BERT for Text Classification?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"3e9674a344db5e4e7aa02222d659e58e4307b084","title":"SemEval 2018 Task 2: Multilingual Emoji Prediction"},{"paperId":"a235999a41cfcc5868ddacb94b47cda933f00a8c","title":"Peperomia at SemEval-2018 Task 2: Vector Similarity Based Approach for Emoji Prediction"},{"paperId":"8245fc60669776991dcb8ecd53a107f37bec29d2","title":"Hatching Chick at SemEval-2018 Task 2: Multilingual Emoji Prediction"},{"paperId":"321f91528af535cefa1b6971df31c609673f463f","title":"Backpropagating through Structured Argmax using a SPIGOT"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"72e87913632d0f1295fbcfa46795c5bb26d0a422","title":"Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"28ac5c728449c19be229e8839f5b5d6acd896f15","title":"Results of the WNUT16 Named Entity Recognition Shared Task"},{"paperId":"a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee","title":"MS MARCO: A Human Generated MAchine Reading COmprehension Dataset"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"d895647b4a80861703851ef55930a2627fe19492","title":"Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"}],"id":"9c2e4e5ee224c20a45c37244924138b50f3fe603","summary":"It is shown that incorporating XRAYEMB’s learned vectors into sequences of pre- trained token embeddings helps performance on both autoregressive and masked pre-trained transformer architectures and on both sequence-level and sequence tagging tasks, particularly on nonstandard English text."},{"url":"https://www.semanticscholar.org/paper/937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens","venue":"NAACL","year":2021,"referenceCount":16,"citationCount":5,"influentialCitationCount":1,"authors":"I. Itzhak,Omer Levy","citations":[{"paperId":"97a11ec5431d84cc91c2d50d88bd88844791ac80","title":"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"8b723be33e62bf5bd9278769244f1c13a9510898","title":"Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP"}],"references":[{"paperId":"7694aae9766d5f1fe74d900cd82aee898cb6e8e9","title":"How to Train BERT with an Academic Budget"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"e9d59dd33698adcdafabeb0f9ea873b2ed36f20b","title":"Farasa: A New Fast and Accurate Arabic Word Segmenter"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"01a660ec8aa995a88a00bfb41cb86c022047a9db","title":"NLTK: The Natural Language Toolkit"},{"paperId":"b2f8876482c97e804bb50a5e2433881ae31d0cdd","title":"Binary codes capable of correcting deletions, insertions, and reversals"}],"id":"937b177d2ed7cee27ee45300c690f2f60c81bae5","summary":"The results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell doesn’t appear to enhance its performance on such tasks."},{"url":"https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models","venue":"TACL","year":2021,"referenceCount":75,"citationCount":95,"influentialCitationCount":24,"authors":"Linting Xue,Aditya Barua,Noah Constant,Rami Al-Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Colin Raffel","citations":[{"paperId":"d2b1405ac8d17f8643ed1f8f7d801e9e406d8da6","title":"SLING: Sino Linguistic Evaluation of Large Language Models"},{"paperId":"6eaa0aa5cc9d3eaa9f578a07fcaa1878cfd21cbc","title":"Graphemic Normalization of the Perso-Arabic Script"},{"paperId":"1ec5e884973aee3946a1871899d30ea7cf031be3","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"539b6862f7e6ce9f98043f00a4ccdb4a9ff8de72","title":"Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning"},{"paperId":"265ebfc1074c73917233dbecad802c0c64921a1c","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks"},{"paperId":"595cc15b9db4e985b30a2e175399a38c021d4ce7","title":"Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino"},{"paperId":"2a8aac78df5e1e9658a02d381662ed26c6577713","title":"Disease diagnostic method based on cascade backbone network for apple leaf disease classification"},{"paperId":"110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward"},{"paperId":"9fc5878d49c41beb12b62032b0afe9a8501fe9db","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?"},{"paperId":"e4437293a703fcf282bf1b38bf7900ed8ca711bb","title":"Training a T5 Using Lab-sized Resources"},{"paperId":"a806041621acb5cf648fc780f1ff14939aa3a721","title":"How Much Does Lookahead Matter for Disambiguation? Partial Arabic Diacritization Case Study"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"d6df842f67fedd63acdf43df8fe449c8c4c66def","title":"Sequence to sequence pretraining for a less-resourced Slovenian language"},{"paperId":"a67914caeaf00a0ce3b073508105cf78b39d0253","title":"Confident Adaptive Language Modeling"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"a766ef0678aade6a9798552618819cf4d0fac406","title":"TEVR: Improving Speech Recognition by Token Entropy Variance Reduction"},{"paperId":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"32afdb07021fda775ceaedd231c58bfed0aa980a","title":"Automated Crossword Solving"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"645b10b7802a035e034488e3640fc0bc415de34c","title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"063d9fa4861356500219b7e81d5a654aa921da6f","title":"A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation"},{"paperId":"7267812178393b8ae0b99648f02661ca1ff2b412","title":"Bilingual End-to-End ASR with Byte-Level Subwords"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"2ffacbeeebd3d9e7467610057b4308635a165b6b","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment"},{"paperId":"0b9e130c6305de7766697ba7655f56010aaffd61","title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"880c8973ea1376c6bff23226b3f64792657aa666","title":"ByT5 model for massively multilingual grapheme-to-phoneme conversion"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"1ed66e048bb025e75aa5ea660545285212e5341f","title":"Scaling Up Models and Data with t5x and seqio"},{"paperId":"a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia"},{"paperId":"8a4eec70e3d0c43abf26757252ad6210c9717f6c","title":"Towards Lithuanian grammatical error correction"},{"paperId":"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation"},{"paperId":"7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers"},{"paperId":"b0c0c742789dfc98dfc2293c6ef7c7944f2c9122","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"},{"paperId":"f357b35e068bbfbb8468b48198ab63241713629d","title":"Correcting diacritics and typos with ByT5 transformer model"},{"paperId":"7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models"},{"paperId":"e53f455b3300d95738dac117419c88fbde58ac4e","title":"Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"e35357ac461a669fe7e4b877ee1fad0dfda26303","title":"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings"},{"paperId":"a70fc86508bd0133d5d984a4e777abef1934d76c","title":"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification"},{"paperId":"7072db6eddb85ecd2c117365d91bd694760f726e","title":"Position Information in Transformers: An Overview"},{"paperId":"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","title":"CHARFORMER: FAST CHARACTER TRANSFORMERS"},{"paperId":"5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9","title":"On the Influence of Tokenizers in NLP Pipelines"},{"paperId":"9758782feed4b4b9bf0ec18b802462e8023a7f83","title":"AfriTeVA: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"0ffb0b109acddf0067aec252221e0ea04d317ddc","title":"A Framework for Sexism Detection on Social Media via ByT5 and TabNet"},{"paperId":"6102fe88a512290b80e83ed2fe17606b166e505a","title":"Transformer-based Part-of-Speech Tagging and Lemmatization for Latin"},{"paperId":"143659fcf93f33e9139f594cf8111c6bc85c04fa","title":"Overview of the EvaLatin 2022 Evaluation Campaign"},{"paperId":"2005311276a1a90dc17cf6e2ca7725fee2e76e1e","title":"BasqueGLUE: A Natural Language Understanding Benchmark for Basque"},{"paperId":"b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83","title":"A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference"},{"paperId":"32290d428b50f64d1cfee6ea658dc6ba977b26e9","title":"Sammaan@LT-EDI-ACL2022: Ensembled Transformers Against Homophobia and Transphobia"},{"paperId":"2b6b38b66fcca218cb2f381e5d4711b5c99a784f","title":"Training Text-to-Text Transformers with Privacy Guarantees"},{"paperId":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"5f77157038dc7f189db7e72ea58567039222d9df","title":"Examining Single Sentence Label Leakage in Natural Language Inference Datasets"},{"paperId":"db3690379953f2ea4f2a015615de02e643d437ea","title":"PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers"},{"paperId":"9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e","title":"Large Dual Encoders Are Generalizable Retrievers"},{"paperId":"6e2682eb2fbec93b329028b23764c1164e232c41","title":"ÚFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-tuning ByT5"},{"paperId":"7c496490abcd8d5c6e90aa3d3c82bde02680f5b0","title":"MutFormer: A context-dependent transformer-based model to predict deleterious missense mutations from protein sequences in the human genome"},{"paperId":"de0e1f9980afa7949df64d53b8ae7a2f59c55579","title":"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages"},{"paperId":"c107835a05ca6fda6e73b64e2ed9884de4fcec0f","title":"A proposed conceptual framework for a representational approach to information retrieval"},{"paperId":"4318c9f87221b9f504f286540c9a6d5c1cc4e4c8","title":"Rethnicity: Predicting Ethnicity from Names"},{"paperId":"9d13bde760d8e77f059436d60160881becd2d2e0","title":"Predicting Ethnicity from Names with rethnicity: Methodology and Application"},{"paperId":"39262814fa3e47905a2e5facf13465a1f70706b9","title":"Single-Read Reconstruction for DNA Data Storage Using Transformers"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"972808b92159a782f5ca1c1ab3a8ed3867acfb2d","title":"mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset"},{"paperId":"89a4f4d1f8c93da85d829a1acfe8cafea2b50c00","title":"Transfer Learning for Multi-lingual Tasks - a Survey"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"14dcb7dd36960eb712fa1fad06ddf771638e02a3","title":"How Optimal is Greedy Decoding for Extractive Question Answering?"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning"},{"paperId":"06431546c21d7c2528aaa170c2e1078e0a82d12e","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"},{"paperId":"7175caf7568d46c857380d0e5b64653819d5cc45","title":"MultiLexNorm: A Shared Task on Multilingual Lexical Normalization"},{"paperId":"e873ddaf58ff92ae0492983cf57221fb25837f4e","title":"Strategies in subword tokenization: humans vs. algorithms"},{"paperId":"ae7fc32df9401a86353185515f4fd5e2020ac922","title":"MutFormer: A context-dependent transformer-based model to predict pathogenic missense mutations"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"}],"references":[{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"},{"paperId":"2b66a0d8a00b0ba7b585e11ab34879420ad351cb","title":"Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution"},{"paperId":"824cd8db8a68732db04f4d8b7139eb4475e59ff2","title":"The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"3fd0f34117cf9395130e08c3f02ac2dadcca7206","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"41efaa96494c0ae19db23700826e4b35d401c8d8","title":"Neural Machine Translation without Embeddings"},{"paperId":null,"title":"The GEM benchmark: Natural language"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"387d59877a641815d79516a7264ff5d20384c596","title":"Question Directed Graph Attention Network for Numerical Reasoning over Text"},{"paperId":"e46c97b2d3bea4c21eb462d5771647a0de99b81c","title":"Ensemble Self-Training for Low-Resource Languages: Grapheme-to-Phoneme Conversion and Morphological Inflection"},{"paperId":"8c122ff82dedf72fa7ef5342622667bf5848916e","title":"One-Size-Fits-All Multilingual Models"},{"paperId":"278f4f68309fc4ffb83bfc4ba86a0ea005106682","title":"The SIGMORPHON 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion"},{"paperId":"b044395ed89de33b43d304c008d3c5a7727d423d","title":"SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection"},{"paperId":"c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c","title":"Processing South Asian Languages Written in the Latin Script: the Dakshina Dataset"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"f4061bd225b3be5b3f5b18eb1a229ce991efefeb","title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":null,"title":"Language ID in the wild"},{"paperId":null,"title":"Neural machine translation with bytelevel subwords"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"5b78c2476984452c372e7fce6ac8246d15c97efa","title":"TWEETQA: A Social Media Focused Question Answering Dataset"},{"paperId":"be564b861e5a9bf5d1dec531807e711000027380","title":"One Size Does Not Fit All: Comparing NMT Representations of Different Granularities"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"5e93a30656ef091f55ce42374d498f2b881e0c47","title":"Bytes Are All You Need: End-to-end Multilingual Speech Recognition and Synthesis with Bytes"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":null,"title":"Characterlevel language modeling with deeper selfattention"},{"paperId":null,"title":"Characterlevel language modeling with deeper selfattention"},{"paperId":"1125d986433735ffc63dc0a8be466043e4f5c5b4","title":"Interpreting Word-Level Hidden State Behaviour of Character-Level LSTM Language Models"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"305b2cf37e5dece81e95c92883d5a6e28ac93b22","title":"Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"f0b6c1ffed9984317050d0c1dfb005cb65582f13","title":"On the State of the Art of Evaluation in Neural Language Models"},{"paperId":"89cb259f39eb8350dd337fc1d02e2f4128c8398c","title":"Byte-based Neural Machine Translation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"563783de03452683a9206e85fe6d661714436686","title":"HyperNetworks"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"7dba53e72c182e25e98e8f73a99d75ff69dda0c2","title":"Recurrent Highway Networks"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":"98445f4172659ec5e891e031d8202c102135c644","title":"Neural Machine Translation in Linear Time"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"4dabd6182ce2681c758f654561d351739e8df7bf","title":"Multilingual Language Processing From Bytes"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":null,"title":"Neural machine"},{"paperId":null,"title":"Aäron van den Oord, Alex Graves, and Koray Kavukcuoglu"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":null,"title":"GLUE : A multitask benchmark and analysis platform for natural language understanding"},{"paperId":null,"title":"Bowman . 2019 b . GLUE : A multitask benchmark and analysis platform for natural language understanding"},{"paperId":null,"title":"Le . 2017 . Hypernetworks"},{"paperId":null,"title":"Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-totext transformer"}],"id":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","summary":"This paper shows that a standard Transformer architecture can be used with minimal modifications to process byte sequences, characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and shows that byte-level models are competitive with their token-level counterparts."},{"url":"https://www.semanticscholar.org/paper/3b34e79610e5acaba352def4323a59f6d531fac7","title":"Macro-Average: Rare Types Are Important Too","venue":"NAACL","year":2021,"referenceCount":42,"citationCount":4,"influentialCitationCount":1,"authors":"Thamme Gowda,Weiqiu You,Constantine Lignos,Jonathan May","citations":[{"paperId":"e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7","title":"Checks and Strategies for Enabling Code-Switched Machine Translation"},{"paperId":"cb48e3bcc185ae94899007e1ad3cdb49ff39428b","title":"Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video"},{"paperId":"0a82ffe5c889d4defbd6300fffab4f3fa3dade99","title":"Deep convolution neural network with weighted loss to detect rice seeds vigor based on hyperspectral imaging under the sample-imbalanced condition"},{"paperId":"04cd4c224f61e8f25a405103d4210f161d091d1c","title":"Look It Up: Bilingual Dictionaries Improve Neural Machine Translation"}],"references":[{"paperId":"868207797e69df5055f5c3fd4aa78a33e5a7ca45","title":"Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics"},{"paperId":"de02a7e9c5e0eeb2cab4d7dd810e5e573327a2f5","title":"Corpora for Cross-Language Information Retrieval in Six Less-Resourced Languages"},{"paperId":"4ae52766028e69186052ea8f33a137fbbbdb986a","title":"BLEURT: Learning Robust Metrics for Text Generation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":null,"title":"XLM-UNMT-Models"},{"paperId":null,"title":"Multilingual denoising"},{"paperId":"dce91eb862d19a646b8f5171ec66e61a987f3b3c","title":"Putting Evaluation in Context: Contextual Embeddings Improve Machine Translation Evaluation"},{"paperId":"145b8b5d99a2beba6029418ca043585b90138d12","title":"MASS: Masked Sequence to Sequence Pre-training for Language Generation"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"9237d6efc603465765e80eb5ca1268c2bd7b5c24","title":"compare-mt: A Tool for Holistic Comparison of Language Generation Systems"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"c590d2c8c2fb6ce5d32ee9165ab24171165f2b70","title":"Assessing gender bias in machine translation: a case study with Google Translate"},{"paperId":"03853267d98af5ec5fdc3dc5cb85cc0681e435e4","title":"Human vs Automatic Metrics: on the Importance of Correlation Design"},{"paperId":"c2a7afbb5609a723f8eea91bfde4b02579b048d6","title":"Unsupervised Neural Machine Translation"},{"paperId":null,"title":"2020), which is fine-tuned on WMT Metrics ratings data from 2015-2018. The BLEURT model is retrieved from https://storage.googleapis.com/ bleurt-oss/bleurt-base-128.zip"},{"paperId":"dc4b3112000e151583da2532b63e0b1e59cbff8a","title":"Results of the WMT17 Metrics Shared Task"},{"paperId":"19a632b17b2ee5f64df41bdd23755316a02fb939","title":"Creating Training Corpora for NLG Micro-Planners"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"1a327709cc53ff9e52454e50a643abf4a0ac92af","title":"Findings of the 2016 Conference on Machine Translation"},{"paperId":"6c35070b6824ae5a2af4c667860e87f2c7ef6a9a","title":"Complementarity, F-score, and NLP Evaluation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"41ae4078cc698edd8dbb510dbfbb0f14b998d132","title":"BEER: BEtter Evaluation as Ranking"},{"paperId":null,"title":"Findings of the 2014 workshop"},{"paperId":"6836eadca4e506b580fc3f7c3374bff363fe0664","title":"The TAO of ATWV: Probing the mysteries of keyword search performance"},{"paperId":"0f8992ee6418d367d8e50ecbb59b08ea15e8431f","title":"Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems"},{"paperId":"b82756edb00e92f7314f1a3e036bf292664ad3a5","title":"A similarity measure for indefinite rankings"},{"paperId":"7bb21d4d4505db402d70c383962d19c42d9a7cbe","title":"Influence functions of the Spearman and Kendall correlation measures"},{"paperId":"20c11546a035d2fa2fa1121a7b31e890d20d6b6b","title":"(Meta-) Evaluation of Machine Translation"},{"paperId":"51951073580f6995e55be873db9a7f6a9736ca86","title":"A Study of Translation Edit Rate with Targeted Human Annotation"},{"paperId":"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7","title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":null,"title":"Europarl: A parallel corpus"},{"paperId":"4f09e6ec1b7d4390d23881852fd7240994abeb58","title":"A statistical interpretation of term specificity and its application in retrieval"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f","title":"Automatic evaluation of machine translation quality using n-gram co-occurrence statistics"},{"paperId":null,"title":"Les irréductibles du M23, soit quelques centaines de combattants, étaient retranchés à près de 2000 mètres d’altitude"},{"paperId":"1f7e50d220f41f4fac985a991c8d5187323aab4c","title":"Applications and Explanations of Zipf’s Law"},{"paperId":"3dcda1bec36586b46b1dc67a477beca2c5a105be","title":"MUC-4 evaluation metrics"},{"paperId":null,"title":"Information Retrieval, 2nd edition"},{"paperId":"2bcf3e6c2b45c052a0bd0183cc29c03acc4b49ac","title":"Human behavior and the principle of least effort"},{"paperId":null,"title":"2-3 octombrie), Iasi (Palas, 6-7 octombrie), Brasov (Casa Armatei, 14-15 octombrie), Cluj Global (Sala Polivalenta, 20-21 octombrie), Cluj IT (Sala Polivalenta, 22-23 octombrie)"}],"id":"3b34e79610e5acaba352def4323a59f6d531fac7","summary":"It is found that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance, and can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods’ outputs."},{"url":"https://www.semanticscholar.org/paper/59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization","venue":"FINDINGS","year":2020,"referenceCount":46,"citationCount":23,"influentialCitationCount":7,"authors":"Xinsong Zhang,Hang Li","citations":[{"paperId":"31d6e59dc275f33da71a72b6b86b38daeed9ffaa","title":"mmLayout: Multi-grained MultiModal Transformer for Document Understanding"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"f8a2428bee464ae69cdb25ce5c2259e88ce576b4","title":"Clickbait Detection of Indonesian News Headlines using Fine-Tune Bidirectional Encoder Representations from Transformers (BERT)"},{"paperId":"05ea28584e5db18c0c31d1aac40e9c1905327557","title":"Effectiveness of Fine-tuned BERT Model in Classification of Helpful and Unhelpful Online Customer Reviews"},{"paperId":"2ffacbeeebd3d9e7467610057b4308635a165b6b","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"8e1227b0d58b769a0919f8debdfd093ff6f6a65a","title":"MarkBERT: Marking Word Boundaries Improves Chinese BERT"},{"paperId":"18dc24da603896db2264e9174116407a95c593d5","title":"“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"35eeeefcbea6fa8edae4c310ee5ee717861c7e60","title":"Improving Constituent Representation with Hypertree Neural Networks"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"976f47bade21fd787f029142b39631cb17f16ec2","title":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation"},{"paperId":"5722d101859846a6a023b8fa00830742203cd0c1","title":"Span Fine-tuning for Pre-trained Language Models"},{"paperId":"27c39dd62635791a0ec3c0c81c2690e7a9bd62ad","title":"LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization"},{"paperId":"4690eb050572a279f94560b6bbdccaae577b45f5","title":"MVP-BERT: Multi-Vocab Pre-training for Chinese BERT"},{"paperId":"7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining"},{"paperId":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"111dbe14083359ab39886790632e7f1421732a8a","title":"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"964da4ed9ac61b12bc2a7adc1e94e3964cc63861","title":"Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question Answering Data"},{"paperId":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"},{"paperId":"518cb6d4247bdebf21e2811f296b0c7372602a0a","title":"PMI-Masking: Principled masking of correlated spans"},{"paperId":"923d2376103dbc9e9b2af4518b56299d7630b46b","title":"J URASSIC -1: T ECHNICAL D ETAILS AND E VALUATION"}],"references":[{"paperId":"7c5c149699a0ba54b52cd5b9e291077f4a1f9d13","title":"Synthesizer: Rethinking Self-Attention in Transformer Models"},{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training With Whole Word Masking for Chinese BERT"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"913e9384a1440dbf860f6d8597953187237fa1b4","title":"Disagreement-Regularized Imitation Learning"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"6007bd2a34385132a7885b934d90b519a1f65bba","title":"ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"d56c1fc337fb07ec004dc846f80582c327af717c","title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"},{"paperId":"80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef","title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"1b07a24b81834116f6ad1d0232485ba81b9445f3","title":"Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"},{"paperId":null,"title":"2e-5 1.0 C DATA AUGMENTATION To enhance the performance, we conduct data augmentation for the three Chinese classification tasks of TNEWS, CSL, and CLUEWSC2020"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"335613303ebc5eac98de757ed02a56377d99e03a","title":"What Does BERT Learn about the Structure of Language?"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"0de0a44b859a3719d11834479112314b4caba669","title":"A Multiscale Visualization of Attention in the Transformer Model"},{"paperId":"e278e072774f23675266881750e20bca74804cb9","title":"ChID: A Large-scale Chinese IDiom Dataset for Cloze Test"},{"paperId":"5f994dc8cae24ca9d1ed629e517fcc652660ddde","title":"ERNIE: Enhanced Language Representation with Informative Entities"},{"paperId":"ae43556f2cc1cecb8b48762b4e09df319fbaa4d9","title":"Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"c34b7796a666f5e205693f6bd1e25e993db19075","title":"Probing Prior Knowledge Needed in Challenging Chinese Machine Reading Comprehension"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"cb0f3ee1e98faf92429d601cdcd76c69c1e484eb","title":"Neural Network Acceptability Judgments"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Openwebtext corpus"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"2019) is a large-scale Chinese IDiom cloze test. C3 (Sun et al., 2019a) is a free-form multiple-choice machine reading comprehension for Chinese"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"883d1d06d857a85a0e64bb19f0b17d56f2cc9d7b","title":"KenLM: Faster and Smaller Language Model Queries"},{"paperId":"0f8468de03ee9f12d693237bec87916311bf1c24","title":"The Seventh PASCAL Recognizing Textual Entailment Challenge"},{"paperId":"db8885a0037fe47d973ade79d696586453710233","title":"The Sixth PASCAL Recognizing Textual Entailment Challenge"},{"paperId":"475354f10798f110d34792b6d88f31d6d5cb099e","title":"Automatically Constructing a Corpus of Sentential Paraphrases"},{"paperId":null,"title":"We adopt the standard hyper-parameters of BERT in pre-training of the models. Table 8 shows the hyper-parameters in our"}],"id":"59c0076b3d814588e320820b95563965733d1875","summary":"This paper proposes a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine- grained and coarse-grains tokenizations, which outperforms the existing best performing models in almost all cases."},{"url":"https://www.semanticscholar.org/paper/969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation","venue":"TACL","year":2021,"referenceCount":82,"citationCount":55,"influentialCitationCount":12,"authors":"J. Clark,Dan Garrette,Iulia Turc,J. Wieting","citations":[{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"379722c04fb3b54215f82512eb86398cb02d42dd","title":"Subword Segmental Language Modelling for Nguni Languages"},{"paperId":"265ebfc1074c73917233dbecad802c0c64921a1c","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks"},{"paperId":"110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward"},{"paperId":"695ea88d22e7d39fa3d0b9f53e75d41e82be81d8","title":"Transformers with Learnable Activation Functions"},{"paperId":"ba9a592438447c2a35afc203be40f7f0a2d3fb5a","title":"A Compact Pretraining Approach for Neural Language Models"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"0376c01a9320027694f2dca57236c27f0c5b36eb","title":"Multilingual Abusiveness Identification on Code-Mixed Social Media Text"},{"paperId":"7ed63a4f928fc126f6780d27e75fa6447a00a8c4","title":"Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference"},{"paperId":"12809bcb734beafeb47876f42e7b438e27fe99fe","title":"General-purpose, long-context autoregressive modeling with Perceiver AR"},{"paperId":"7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models"},{"paperId":"2ec281d654f622da7267d7da8145e96bb77a9ede","title":"An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","title":"CHARFORMER: FAST CHARACTER TRANSFORMERS"},{"paperId":"610088e1d7a8e44f921b2c4894fe1a7b204ce5a9","title":"Vicomtech at LivingNER2022"},{"paperId":"e11d8663ccf2cc412f408853fa5f19ebac75df54","title":"How to encode arbitrarily complex morphology in word embeddings, no corpus needed"},{"paperId":"b1be10b76314ea8569249f2310f1e6eead8a5adc","title":"Building Domain-specific Corpora from the Web: the Case of European Digital Service Infrastructures"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"12fec03c538c7aaeca1a4e1ab8d66aed5f793fc0","title":"reamtchka at SemEval-2022 Task 6: Investigating the effect of different loss functions for Sarcasm detection for unbalanced datasets"},{"paperId":"e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"e43a360728e34e43934707665db0f7be02c8e5a7","title":"Interpreting BERT-based stance classification: a case study about the Brazilian COVID vaccination"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5","title":"Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study"},{"paperId":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"dca4128a33ca22c02031b5c0c28548a0df022d80","title":"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"b62a56f629f77acce9ea69456d67dd77de1c7dfb","title":"Clustering Monolingual Vocabularies to Improve Cross-Lingual Generalization"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"},{"paperId":"58403c6bc061670f7ce6267a3a0cd01ba1bb8e50","title":"Intérêt des modèles de caractères pour la détection d’événements (The interest of character-level models for event detection)"}],"references":[{"paperId":"b57da3ccf214e8dad49116c8db9590c2c89629f5","title":"MasakhaNER: Named Entity Recognition for African Languages"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"9ed25f101f19ea735ca300848948ed64064b97ca","title":"Random Feature Attention"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"466f7cfe7442738ee974b12939b4c2e32ee098bf","title":"Rethinking Attention with Performers"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"88340786475bf1649b83b7ac0ad7f57e60a20b52","title":"Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"2a218786f4615b82389f78472e7ff22e6ce57490","title":"ConvBERT: Improving BERT with Span-based Dynamic Convolution"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"4ca3b0ea12f02e2dea01a4aa505956bae5500a09","title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"b1a71677a13299755a12375f0c982484088aa9ef","title":"English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"56676aef356ebb13cba77fc9e4d70760fbc151f5","title":"ETC: Encoding Long and Structured Inputs in Transformers"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b","title":"RobBERT: a Dutch RoBERTa-based Language Model"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"bc789aef715498e79a74f857fa090ece9e383bf1","title":"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"},{"paperId":"d619570b03e629653d69b5157764be183e8521bf","title":"UnihanLM: Coarse-to-Fine Chinese-Japanese Language Model Pretraining with the Unihan Database"},{"paperId":"8912eaad7d9c779bd698c73409e410fdf8c5e3c2","title":"PRADO: Projection Attention Networks for Document Classification On-Device"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"8a7e9f91d949764d6159c114d4feb961ec07b32d","title":"Training Hybrid Language Models by Marginalizing over Segmentations"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"1d51b59fcf0297e8df931c8b614bd55165b24172","title":"Better Character Language Modeling through Morphology"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"e07f2c383a34217a9403a4d58a4e7d61e57d9163","title":"Character Eyes: Seeing Language through Character-Level Taggers"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"9852ae077c7da6a9d178acaa2b44a335289507a6","title":"Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"bae055edb1d552c24bbea56556bafdde9ef61c50","title":"On the Strength of Character Language Models for Multilingual Named Entity Recognition"},{"paperId":"b23300ff8ba9fcd82e349da03a6f13386bff0077","title":"What do character-level models learn about morphology? The case of dependency parsing"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"5b1516c87818084dc5d195cc274e1ee8923210d2","title":"Neural Cross-Lingual Named Entity Recognition with Minimal Resources"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"4dd9d67a0259eef54ca32770db24fab5e42d362a","title":"Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction"},{"paperId":"771c1cb5fb161231e9aaa0a189caba672256a880","title":"Using Morphological Knowledge in Open-Vocabulary Neural Language Models"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"fc7e6502dace26305e3e062e426034f61a18095e","title":"Byte-Level Machine Reading Across Morphologically Varied Languages"},{"paperId":"0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b","title":"Neural Lattice Language Models"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"b959e905c093e1aa4a27eda0c0f1b4f727b93e63","title":"Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification"},{"paperId":"647979987ebfdf4f566add387bf3318fa8190305","title":"Hash Embeddings for Efficient Word Representations"},{"paperId":"07466fb914982f55051cc0b236fd524bdcd8bdc7","title":"Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"45e3f381d16e6d6db5449b7a44b7bdf294a8a822","title":"Multiscale sequence modeling with a learned dictionary"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a921e014249395496c22783fe58d293746af852b","title":"From Characters to Words to in Between: Do We Capture Morphology?"},{"paperId":"ed5003e6a2b97dd4b65c5190ccb88b9c45b032b8","title":"Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling"},{"paperId":"664ec878de4b7170712baae4a7821fc2602bba25","title":"Learning to Generate Reviews and Discovering Sentiment"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"12e9d005c77f76e344361f79c4b008034ae547eb","title":"Charagram: Embedding Words and Sentences via Character n-grams"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"4dabd6182ce2681c758f654561d351739e8df7bf","title":"Multilingual Language Processing From Bytes"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"fdb813d8b927bdd21ae1858cafa6c34b66a36268","title":"Learning deep structured semantic models for web search using clickthrough data"},{"paperId":"31100d7f8fdd3d263238624a888dab0f7aba3d5a","title":"Adaptor Grammars for Learning Non-Concatenative Morphology"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"727209a57c643472c8fc166ba3cc373936acc8d0","title":"Learning a Part-of-Speech Tagger from Two Hours of Annotation"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"9fae0f18e23db076e27b23f17a417f3149f63e2e","title":"TweetMotif: Exploratory Search and Topic Summarization for Twitter"},{"paperId":"e9d100404934e025a2df61882faf37ae2031af03","title":"Bloom Maps"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"502858989368adae91e0f4a643ebc2aa6efd5738","title":"Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":null,"title":"Language-wise breakdown for TYDI QA primary tasks"}],"id":"969287b8a96e242793b11f0dbb99ec341228106f","summary":"Canine is presented, a neural encoder that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias."},{"url":"https://www.semanticscholar.org/paper/485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation","venue":"Machine Translation","year":2018,"referenceCount":25,"citationCount":23,"influentialCitationCount":1,"authors":"Elizabeth Salesky,Andrew Runge,Alex Coda,J. Niehues,Graham Neubig","citations":[{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"379722c04fb3b54215f82512eb86398cb02d42dd","title":"Subword Segmental Language Modelling for Nguni Languages"},{"paperId":"3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2","title":"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class"},{"paperId":"8a1c54f6e2c5f1453fddb9f15e769a099286f677","title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"24fd022750ea88e44e6790c7c7e1923635885c71","title":"Translation Mechanism of Neural Machine Algorithm for Online English Resources"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"6dc710b46bc510a42af487a3ac220e4fabf4d518","title":"VOLT: Improving Vocabularization via Optimal Transport for Machine Translation"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"38b40ae531ddca434de07015637b78413370d15a","title":"The Roles of Language Models and Hierarchical Models in Neural Sequence-to-Sequence Prediction"},{"paperId":"405cd5cd6e056675d4545eba12742174cc75d7e7","title":"Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"debb3877b778eeb8689729d37e2b90f9f000d877","title":"Neural Machine Translation with Imbalanced Classes"},{"paperId":"8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","title":"Adversarial Subword Regularization for Robust Neural Machine Translation"},{"paperId":"4d08dcd2cc1e9691defe664a10f021424a896a1e","title":"Neural Machine Translation: A Review"},{"paperId":"201fae97e51fb6aea7ed8120147e806e43834de6","title":"Neural Machine Translation: A Review and Survey"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"fec6def294027a2ce9094267ce7b7d57f78daf74","title":"Multitask Learning For Different Subword Segmentations In Neural Machine Translation"},{"paperId":"95236a88cc959656958d7a49422f4004015d594d","title":"Comparison between NMT and PBSMT Performance for Translating Noisy User-Generated Content"},{"paperId":"fbd47a815c73a83e8a47ee2ed38826c82ffc0c2a","title":"Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation"},{"paperId":"bc39d16c108057e062ad6f1d0e8154df52cafc6a","title":"CMU’s Machine Translation System for IWSLT 2019"}],"references":[{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"491d29cb0018578b47e9abe19b93d5b65dc59113","title":"Improving Character-Based Decoding Using Target-Side Morphological Information for Neural Machine Translation"},{"paperId":"ac7c04a668bdb0e3eff168e65cb85689b4f7ef57","title":"Lifelong Learning with Dynamically Expandable Networks"},{"paperId":"b36a5bb1707bb9c70025294b3a310138aae8327a","title":"Automatic differentiation in PyTorch"},{"paperId":"69a037af886a6235d9af3cdeef4c7233d34c86ce","title":"Growing a Brain: Fine-Tuning by Increasing Model Capacity"},{"paperId":"1f080959faf9bf2a282c0aadbd8584d8b32f6e24","title":"Modeling Target-Side Inflection in Neural Machine Translation"},{"paperId":"69ffe3277011087b55afaeaef0a3933160beee9a","title":"Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English"},{"paperId":"100104b980d56a40cadfbd7034fa7807ce49b3fb","title":"Nematus: a Toolkit for Neural Machine Translation"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":null,"title":"A (2017) Autodiff Workshop: the future of gradient-based machine learning software and techniques"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"16cb6876666f3a7b56a636c1d85ad00bd0d98bf3","title":"Net2Net: Accelerating Learning via Knowledge Transfer"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"6dab1c6491929d396e9e5463bc2e87af88602aa2","title":"Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"88b66f705a329da8292e7b8aa4bfe26de4759cfa","title":"Machine Translation without Words through Substring Alignment"},{"paperId":"791c0df5557890e7a4d8c81b12cd966531e7b42c","title":"Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability"},{"paperId":"8f4987d648d0daf3e36172e1abcda796fe8cf865","title":"An exponential translation model for target language morphology"},{"paperId":"42fc4c6580bfa54d57b5d6c56b5dfde58c6f6abb","title":"English-to-Czech Factored Machine Translation"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"319af0958e268b3243975b8628262ebc1980ce40","title":"of the European Chapter of the Association for Computational Linguistics"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":null,"title":"Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations"}],"id":"485b3f77b9913e151e7ca897d99497e70e7f30d1","summary":"This paper incrementally introduces new BPE vocabulary online based on the held-out validation loss, and matches the results found with grid search, optimizing segmentation granularity while significantly reducing overall training time."},{"url":"https://www.semanticscholar.org/paper/ab139e341c005929848a7326f3d44f8a6aa9863c","title":"Improving Neural Machine Translation by Incorporating Hierarchical Subword Features","venue":"COLING","year":2018,"referenceCount":17,"citationCount":16,"influentialCitationCount":0,"authors":"Makoto Morishita,Jun Suzuki,Masaaki Nagata","citations":[{"paperId":"f6167794a9fc7a58b05618091bf08e57a4e2507b","title":"SIT at MixMT 2022: Fluent Translation Built on Giant Pre-trained Models"},{"paperId":"2e6028f8b156c8b344e1a68d15d88403a978c71d","title":"Learning Multiscale Transformer Models for Sequence Generation"},{"paperId":"c68959f4da94fd35da5e8649465177b24d0dd351","title":"Byte-based Multilingual NMT for Endangered Languages"},{"paperId":"bfbfeb0613beae296d76077b2adc9e38a6b678a4","title":"Sub-Subword N-Gram Features for Subword-Level Neural Machine Translation"},{"paperId":"188f913be48a218b44b0bd964662b4926fd0b0b8","title":"Neural Machine Translation: A Review of Methods, Resources, and Tools"},{"paperId":"ca83469977ab49baae02ef1e12f419120927ecdc","title":"Mixed-Level Neural Machine Translation"},{"paperId":"fac7f9b04fba4889b445e309d384644d15ee7e88","title":"Trends and Advances in Neural Machine Translation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"debb3877b778eeb8689729d37e2b90f9f000d877","title":"Neural Machine Translation with Imbalanced Classes"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"fec6def294027a2ce9094267ce7b7d57f78daf74","title":"Multitask Learning For Different Subword Segmentations In Neural Machine Translation"},{"paperId":"9f3f6deeb1f03ebd52f9ab44275ad382fe60d073","title":"Latent Part-of-Speech Sequences for Neural Machine Translation"},{"paperId":"0aef56962035a79101821480f51897fdc4443945","title":"Character n-gram Embeddings to Improve RNN Language Models"},{"paperId":"0ab0fda8774c303be8f8f8c8f684a890dcf5d455","title":"Lattice-Based Transformer Encoder for Neural Machine Translation"},{"paperId":"bc39d16c108057e062ad6f1d0e8154df52cafc6a","title":"CMU’s Machine Translation System for IWSLT 2019"},{"paperId":"247328a082d86199ed5a98e1d726aa205c1da9df","title":"Neural Machine Translation"}],"references":[{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"56cf69bbe6598471d1655a4d0ccd4a61e946a532","title":"NTT Neural Machine Translation Systems at WAT 2017"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627","title":"Six Challenges for Neural Machine Translation"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"503981515e1a209f57f8119f616cc0f0c6bf168d","title":"Kyoto University Participation to WAT 2017"},{"paperId":"3bc6bcb60c7c00efcb471191fb62fd3ebd231d66","title":"Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions"},{"paperId":"46d0aa6b357c5427f46c7f8ff7053617c4309649","title":"Linguistic Input Features Improve Neural Machine Translation"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"09cd7876b72d6105c83db59052572433a0a2b36c","title":"WIT3: Web Inventory of Transcribed and Translated Talks"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"ab139e341c005929848a7326f3d44f8a6aa9863c","summary":"It is confirmed that incorporating hierarchical subword features in the encoder consistently improves BLEU scores on the IWSLT evaluation datasets and the assumption that in the NMT model, the appropriate subword units for the following three modules can differ is confirmed."},{"url":"https://www.semanticscholar.org/paper/473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters","venue":"COLING","year":2020,"referenceCount":36,"citationCount":65,"influentialCitationCount":7,"authors":"Hicham El Boukkouri,Olivier Ferret,T. Lavergne,Hiroshi Noji,Pierre Zweigenbaum,Junichi Tsujii","citations":[{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"1e2a5dca6f310241dd8a9b56873edf8ca211c781","title":"Enriching Biomedical Knowledge for Low-resource Language Through Translation"},{"paperId":"8297a3b52eea4e8c3ab8e97910839b4a3b917891","title":"On the State of the Art in Authorship Attribution and Authorship Verification"},{"paperId":"8f541cf5b193654d262dbdaea2b8a0d7d913e8dc","title":"Survey of NLP in Pharmacology: Methodology, Tasks, Resources, Knowledge, and Tools"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"7c206d541552a57b4c0cfe5b5113d3cc8e31df01","title":"IMSE: interaction information attention and molecular structure based drug drug interaction extraction"},{"paperId":"c75b1fb2348c0f54259319b3595ececaf1d98430","title":"Medical terminology-based computing system: a lightweight post-processing solution for out-of-vocabulary multi-word terms"},{"paperId":"305062aa036ffeafdd6300303898dff8ed59a7dc","title":"Cross-lingual Approaches for the Detection of Adverse Drug Reactions in German from a Patient’s Perspective"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"f1d8dbe4e217d2221c5276f1e0c616ba5f82d1d3","title":"A review on Natural Language Processing Models for COVID-19 research"},{"paperId":"91141410b0bd141bc3b1ac0c190c3867181d699c","title":"Multi-task learning in under-resourced Dravidian languages"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"9727fa4acdc5312ff86745875ef3db8578f153ac","title":"Decorate the Examples: A Simple Method of Prompt Design for Biomedical Relation Extraction"},{"paperId":"6058ce3819d72c3e429bea58d78d80c719cb4bdb","title":"Data Augmentation for Biomedical Factoid Question Answering"},{"paperId":"bff16ea31d5bc6b25b6e48327a2d7026e92a788d","title":"CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos"},{"paperId":"b3b1659c992cbbd233038522ddd170887c033fe7","title":"Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech"},{"paperId":"9af10e3d4ad1c6f8a0ff8fac8dec52703faa8e7e","title":"vTTS: visual-text to speech"},{"paperId":"e68d1aa5e78226bb868a43842a87a5532b813b0b","title":"CancerBERT: a cancer domain-specific language model for extracting breast cancer phenotypes from electronic health records"},{"paperId":"16f0acab166e8ac756927fc8f2e8f68be641605b","title":"Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models"},{"paperId":"e4645f7d08b5bc878bd38c3db19c3b1a9978bd43","title":"Imputing Out-of-Vocabulary Embeddings with LOVE Makes LanguageModels Robust with Little Cost"},{"paperId":"7746ac2e1e84c2f1d82a4593e2cdfdd4c93a04d8","title":"An open-source natural language processing toolkit to support software development: addressing automatic bug detection, code summarisation and code search"},{"paperId":"1b012686cf42baed47c11f0454b20a4d820a1db1","title":"The EMory BrEast imaging Dataset (EMBED): A Racially Diverse, Granular Dataset of 3.5M Screening and Diagnostic Mammograms"},{"paperId":"e923c415140a095711857420976b41e7a07cfe9c","title":"Towards improving the robustness of sequential labeling models against typographical adversarial examples using triplet loss"},{"paperId":"ec72909b75b2389efd588aa38d9c664e654d90d3","title":"Automatic Classification of Cancer Pathology Reports: A Systematic Review"},{"paperId":"2ec281d654f622da7267d7da8145e96bb77a9ede","title":"An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification"},{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"aeeb168feb05da1b3d31c0389e7a9196bc8970f6","title":"DravidianCodeMix: sentiment analysis and offensive language identification dataset for Dravidian languages in code-mixed text"},{"paperId":"420c897bc67e6f438db522d919d925df1a10aa8c","title":"AMMU - A Survey of Transformer-based Biomedical Pretrained Language Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"edc17c862b074fb986f9b9a144f10decb843f5fa","title":"Décontextualiser des plongements contextuels pour construire des thésaurus distributionnels (Decontextualizing contextual embeddings for building distributional thesauri )"},{"paperId":"abb47fb115a420fc363ed3acbd8abb1a37f0dcd5","title":"DeepREF: A Framework for Optimized Deep Learning-based Relation Classification"},{"paperId":"d5c2119aec38b8e8e154ec2bf0106dfac1ecc80f","title":"Building Static Embeddings from Contextual Ones: Is It Useful for Building Distributional Thesauri?"},{"paperId":"04bc7d5ac048133a1aaead7ab1e8021b055359c4","title":"Design principles of an open-source language modeling microservice package for AAC text-entry applications"},{"paperId":"0826af7c0f221878a142dd1681a63adb3f7e4569","title":"RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining"},{"paperId":"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","title":"CHARFORMER: FAST CHARACTER TRANSFORMERS"},{"paperId":"b42e3a759348f27cca2f918a6bd0b139a5312e44","title":"A Survey of Pretrained Language Models Based Text Generation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"cdc71d86bb0b822d73a8359338db0492a2c89b89","title":"Gated Character-aware Convolutional Neural Network for Effective Automated Essay Scoring"},{"paperId":"a3d24d9d8ca41a0b87957819a7a9095d1cdb8c15","title":"Transferring BERT-like Transformers' Knowledge for Authorship Verification"},{"paperId":"597b1387f721fa8f7524b257aca920c7d373ed48","title":"Using Distributional Principles for the Semantic Study of Contextual Language Models"},{"paperId":"11a583c33887e8fc16c0ee8c458fad92d5641dd5","title":"Switch Point biased Self-Training: Re-purposing Pretrained Models for Code-Switching"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"705554c3532b7be9c1eb7c993132ffb940282e1b","title":"Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models"},{"paperId":"395000b02848b666ebd2433c93127fac5113df9f","title":"BERT Cannot Align Characters"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"c0cd91d975df23ae3d7c088c4ddd375271dfe17c","title":"Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning"},{"paperId":"d85d0c5d218b22366abd2bd4ee35df179dda1d85","title":"CancerBERT: a BERT model for Extracting Breast Cancer Phenotypes from Electronic Health Records"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"e6a6c7c1e231e745021d2312f7f88f3ea24bcba8","title":"Benchmarking Multi-Task Learning for Sentiment Analysis and Offensive Language Identification in Under-Resourced Dravidian Languages"},{"paperId":"82d9823db8d3ab364f04f51914919e0a7c6b9b44","title":"Character-Level Syntax Infusion in Pre-Trained Models for Chinese Semantic Role Labeling"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"972793927b9b1dd0805ee36b24f37c7f2c3ba691","title":"Combining formal and machine learning techniques for the generation of JML specifications"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"87bca71f182bdb942adbe3f6898894cadef6776e","title":"CodemixedNLP: An Extensible and Open NLP Toolkit for Code-Mixing"},{"paperId":"3ca6eb7b69da71899ff2d20d6a65f3e668b97312","title":"IIITT@LT-EDI-EACL2021-Hope Speech Detection: There is always hope in Transformers"},{"paperId":"d33713b55f6e79270a529cdcce4843c70a051f83","title":"UniParma at SemEval-2021 Task 5: Toxic Spans Detection Using CharacterBERT and Bag-of-Words Model"},{"paperId":"7dcb6a4b7b1a1d2317c3058f529c5dbcceebbcae","title":"Easy-to-use Combination of POS and BERT Model for Domain-Specific and Misspelled Terms"},{"paperId":"c3152c57f2c3960f9423f2b7278a5f98d149b3d8","title":"Differential Evaluation: a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing"},{"paperId":"71ea59f202562410f7342764265a29a27778da69","title":"Type- and Token-based Word Embeddings in the Digital Humanities"},{"paperId":"9b6e51039f57cabdb77ca70cbbdd6ff80bdf1c3a","title":"FETD2: A Framework for Enabling Textual Data Denoising via Robust Contextual Embeddings"},{"paperId":"58403c6bc061670f7ce6267a3a0cd01ba1bb8e50","title":"Intérêt des modèles de caractères pour la détection d’événements (The interest of character-level models for event detection)"},{"paperId":"1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","title":"Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings"},{"paperId":"8ce62f8d83f02e8ce9d7a9e1b5b7affd5c13bb7d","title":"Sensitive Data Detection with High-Throughput Neural Network Models for Financial Institutions"}],"references":[{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training With Whole Word Masking for Chinese BERT"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef","title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"},{"paperId":"bc789aef715498e79a74f857fa090ece9e383bf1","title":"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"7304afb491930d520e194f4ca8b91a9652f0e658","title":"MedSTS: a resource for clinical semantic textual similarity"},{"paperId":"2b8f3c930478053444a12a889bc1ee26f8f0bff2","title":"BERT Goes to Law School: Quantifying the Competitive Advantage of Access to Large Legal Corpora in Contract Understanding"},{"paperId":"d8d7e338e528bdf89c589ab5926fc9ec4339c78b","title":"Embedding Strategies for Specialized Domains: Application to Clinical Entity Recognition"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"6a7769116c6733dffa347444b2835e50129e0143","title":"Deep Dominance - How to Properly Compare Deep Neural Models"},{"paperId":"347bac45298f37cd83c3e79d99b826dc65a70c46","title":"Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"},{"paperId":"5f994dc8cae24ca9d1ed629e517fcc652660ddde","title":"ERNIE: Enhanced Language Representation with Informative Entities"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"06b36e744dca445863c9f9aefe76aea95ba95999","title":"Enhancing Clinical Concept Extraction with Contextual Embedding"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"OpenWebText corpus"},{"paperId":null,"title":"2019), which is the original model using the “base-uncased” architecture, and BlueBERT (Peng et al., 2019), a medical BERT pre-trained from the former using MIMIC-III and the abstracts of PMC OA"},{"paperId":null,"title":"2019) using the “base-uncased” architecture, and BlueBERT (Peng et al., 2019) a medical BERT that is the result of re-training the former model on MIMIC-III and PubMed abstracts"},{"paperId":"f2588de5173fb047192dbb93d62ce6636bdf46bd","title":"Lessons from Natural Language Inference in the Clinical Domain"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"eed781f498b563df5a9e8a241c67d63dd1d92ad5","title":"Overview of the BioCreative VI chemical-protein interaction Track"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"95cd83603a0d2b6918a8e34a5637a8f382da96f5","title":"MIMIC-III, a freely accessible critical care database"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"b92aa7024b87f50737b372e5df31ef091ab54e62","title":"Training Very Deep Networks"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"53ca064b9f1b92951c1997e90b776e95b0880e52","title":"Learning word embeddings efficiently with noise-contrastive estimation"},{"paperId":"6ea353ada2b89763f58d8068a74b2e6def526948","title":"The DDI corpus: An annotated corpus with pharmacological substances and drug-drug interactions"},{"paperId":"5e095981ebf4d389e9356bd56e59e0ade1b42e88","title":"2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text"},{"paperId":"a8e8f3c8d4418c8d62e306538c9c1292635e9d27","title":"Backpropagation Applied to Handwritten Zip Code Recognition"}],"id":"473921de1b52f98f34f37afd507e57366ff7d1ca","summary":"This work proposes CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters, and shows that this new model improves the performance of Bert on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations."}]}