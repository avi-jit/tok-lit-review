"id","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"b82f43182aa7a188c12eb9cb6992b6f82cca2120","Language Modelling with Pixels","PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels, and is more robust to noisy text inputs than BERT, further confirming the benefits of modelling language with pixels.","ArXiv",2022,"Phillip Rust,Jonas F. Lotz,Emanuele Bugliarello,Elizabeth Salesky,Miryam de Lhoneux,Desmond Elliott",4,136,0,"https://www.semanticscholar.org/paper/b82f43182aa7a188c12eb9cb6992b6f82cca2120"
"e79d1206292bc5e67ba19737d87d4b2ea4a37105","Charformer: Fast Character Transformers via Gradient-based Subword Tokenization","This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion and paves the way for highly performant token-free models that are trained completely end-to-end.","ICLR",2021,"Yi Tay,V. Tran,Sebastian Ruder,Jai Gupta,Hyung Won Chung,Dara Bahri,Zhen Qin,Simon Baumgartner,Cong Yu,Donald Metzler",39,69,7,"https://www.semanticscholar.org/paper/e79d1206292bc5e67ba19737d87d4b2ea4a37105"
"9c2e4e5ee224c20a45c37244924138b50f3fe603","Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information","It is shown that incorporating XRAYEMB’s learned vectors into sequences of pre- trained token embeddings helps performance on both autoregressive and masked pre-trained transformer architectures and on both sequence-level and sequence tagging tasks, particularly on nonstandard English text.","ArXiv",2021,"Yuval Pinter,A. Stent,Mark Dredze,Jacob Eisenstein",2,32,0,"https://www.semanticscholar.org/paper/9c2e4e5ee224c20a45c37244924138b50f3fe603"
"937b177d2ed7cee27ee45300c690f2f60c81bae5","Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens","The results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell doesn’t appear to enhance its performance on such tasks.","NAACL",2021,"I. Itzhak,Omer Levy",5,16,1,"https://www.semanticscholar.org/paper/937b177d2ed7cee27ee45300c690f2f60c81bae5"
"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models","This paper shows that a standard Transformer architecture can be used with minimal modifications to process byte sequences, characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and shows that byte-level models are competitive with their token-level counterparts.","TACL",2021,"Linting Xue,Aditya Barua,Noah Constant,Rami Al-Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Colin Raffel",95,75,24,"https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7"
"3b34e79610e5acaba352def4323a59f6d531fac7","Macro-Average: Rare Types Are Important Too","It is found that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance, and can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods’ outputs.","NAACL",2021,"Thamme Gowda,Weiqiu You,Constantine Lignos,Jonathan May",4,42,1,"https://www.semanticscholar.org/paper/3b34e79610e5acaba352def4323a59f6d531fac7"
"59c0076b3d814588e320820b95563965733d1875","AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization","This paper proposes a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine- grained and coarse-grains tokenizations, which outperforms the existing best performing models in almost all cases.","FINDINGS",2020,"Xinsong Zhang,Hang Li",23,46,7,"https://www.semanticscholar.org/paper/59c0076b3d814588e320820b95563965733d1875"
"969287b8a96e242793b11f0dbb99ec341228106f","Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation","Canine is presented, a neural encoder that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias.","TACL",2021,"J. Clark,Dan Garrette,Iulia Turc,J. Wieting",55,82,12,"https://www.semanticscholar.org/paper/969287b8a96e242793b11f0dbb99ec341228106f"
"485b3f77b9913e151e7ca897d99497e70e7f30d1","Optimizing segmentation granularity for neural machine translation","This paper incrementally introduces new BPE vocabulary online based on the held-out validation loss, and matches the results found with grid search, optimizing segmentation granularity while significantly reducing overall training time.","Machine Translation",2018,"Elizabeth Salesky,Andrew Runge,Alex Coda,J. Niehues,Graham Neubig",23,25,1,"https://www.semanticscholar.org/paper/485b3f77b9913e151e7ca897d99497e70e7f30d1"
"ab139e341c005929848a7326f3d44f8a6aa9863c","Improving Neural Machine Translation by Incorporating Hierarchical Subword Features","It is confirmed that incorporating hierarchical subword features in the encoder consistently improves BLEU scores on the IWSLT evaluation datasets and the assumption that in the NMT model, the appropriate subword units for the following three modules can differ is confirmed.","COLING",2018,"Makoto Morishita,Jun Suzuki,Masaaki Nagata",16,17,0,"https://www.semanticscholar.org/paper/ab139e341c005929848a7326f3d44f8a6aa9863c"
"473921de1b52f98f34f37afd507e57366ff7d1ca","CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters","This work proposes CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters, and shows that this new model improves the performance of Bert on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations.","COLING",2020,"Hicham El Boukkouri,Olivier Ferret,T. Lavergne,Hiroshi Noji,Pierre Zweigenbaum,Junichi Tsujii",65,36,7,"https://www.semanticscholar.org/paper/473921de1b52f98f34f37afd507e57366ff7d1ca"