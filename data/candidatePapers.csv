"id","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"d617f51833860dc50d202af7f80be71304b2e994",7,"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP","This survey connects several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subwordbased approaches based on learned segmentation have been proposed and evaluated.","ArXiv",2021,"Sabrina J. Mielke,Zaid Alyafeai,Elizabeth Salesky,Colin Raffel,Manan Dey,Matthias Gallé,Arun Raja,Chenglei Si,Wilson Y. Lee,Benoît Sagot,Samson Tan",12,186,1,"https://www.semanticscholar.org/paper/d617f51833860dc50d202af7f80be71304b2e994"
"d87647784c12517d31964cc508d5b8423cc24f50",4,"Integrating Approaches to Word Representation","A survey of the distributional, compositional, and relational approaches to addressing the problem of representing the atomic elements of language in modern neural learning systems is presented, with special emphasis on the word level and the out-of-vocabulary phenomenon.","ArXiv",2021,"Yuval Pinter",3,96,0,"https://www.semanticscholar.org/paper/d87647784c12517d31964cc508d5b8423cc24f50"
"f332a615c33c69f54dfcb9a8b14f96e8b5725def",4,"Sub-Character Tokenization for Chinese Pretrained Language Models","Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency, and 2) Pronunciation-based SubChartokenization can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to all homophone typos.","",2021,"Chenglei Si,Zhengyan Zhang,Yingfa Chen,Fanchao Qi,Xiaozhi Wang,Zhiyuan Liu,Yasheng Wang,Qun Liu,Maosong Sun",1,42,0,"https://www.semanticscholar.org/paper/f332a615c33c69f54dfcb9a8b14f96e8b5725def"
"6c761cfdb031701072582e434d8f64d436255da6",4,"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing","This comprehensive survey paper explains various core concepts like pretraining, Pretraining methods, pretraining tasks, embeddings and downstream adaptation methods, presents a new taxonomy of T-PTLMs and gives brief overview of various benchmarks including both intrinsic and extrinsic.","ArXiv",2021,"Katikapalli Subramanyam Kalyan,A. Rajasekharan,S. Sangeetha",40,301,5,"https://www.semanticscholar.org/paper/6c761cfdb031701072582e434d8f64d436255da6"
"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5",4,"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?","This work shows that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models.","WNUT",2021,"Arij Riabi,Benoît Sagot,Djamé Seddah",4,59,0,"https://www.semanticscholar.org/paper/dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5"
"62ece609555bf833a2afd25ef796d72b5f59e767",4,"Local Byte Fusion for Neural Machine Translation","This paper proposes a Lo cal B yt e F usion (LOBEF) method for byte-based machine translation—utilizing byte n -gram and word boundaries—to aggregate local semantic information and indicates that the byte- based models are parameter-efﬁcient and can be trained faster than subword models while showcasing competitive performance.","ArXiv",2022,"Makesh Narsimhan Sreedhar,Xiangpeng Wan,Yu-Jie Cheng,Junjie Hu",0,42,0,"https://www.semanticscholar.org/paper/62ece609555bf833a2afd25ef796d72b5f59e767"
"54123c8de8ecacb30ae2e9fb8c4635f4030965b5",4,"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models","A novel method of retroactively adding resilience to misspellings to transformer-based NLP models without the need for re-training of the original NLP model is proposed, which significantly reduces the cost needed to evaluate a model’s resilience to adversarial attacks.","COLING",2022,"Jan Jezabek,A. Singh",0,26,0,"https://www.semanticscholar.org/paper/54123c8de8ecacb30ae2e9fb8c4635f4030965b5"
"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1",4,"HashFormers: Towards Vocabulary-independent Pre-trained Transformers","It is empirically demonstrated that H ASH F ORMERS are more memory efﬁcient compared to standard pre-trained transformers while achieving comparable predictive performance when ﬁne-tuned on multiple text classiﬂcation tasks.","ArXiv",2022,"Hui-li Xue,Nikolaos Aletras",0,36,0,"https://www.semanticscholar.org/paper/12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1"
"023fd42f0867a88a2206f906c7f127701058feb6",4,"Incorporating Context into Subword Vocabularies","SAGE is presented, a tokenizer that tailors subwords for their downstream use by baking in the contextualized signal at the vocabulary creation phase, and does a better job than current widespread tokenizers in keeping token contexts cohesive, while not incurring a large price in terms of encoding efficiency or domain robustness.","ArXiv",2022,"Shaked Yehezkel,Yuval Pinter",1,41,0,"https://www.semanticscholar.org/paper/023fd42f0867a88a2206f906c7f127701058feb6"
"2f07f97563a73d9b691ec6144e4bba25a347ab87",3,"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers","FLOTA (Few Longest Token Approximation) leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.","ACL",2022,"Valentin Hofmann,Hinrich Schütze,J. Pierrehumbert",3,54,0,"https://www.semanticscholar.org/paper/2f07f97563a73d9b691ec6144e4bba25a347ab87"
"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01",3,"C HARFORMER : F AST C HARACTER T RANSFORMERS VIA G RADIENT - BASED S UBWORD T OKENIZATION","This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion that paves the way for highly performant token-free models that are trained completely end-to-end.","",,"",0,0,0,"https://www.semanticscholar.org/paper/6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01"
"e5f6506f9332fcdb574f13a791e4f3c42b80ca90",3,"Demystifying Neural Language Models' Insensitivity to Word-Order","The insensitivity of natural language models to word-order is investigated by quantifying perturbations and analysing their effect on neural models’ performance on language understanding tasks in GLUE benchmark and it is found that neural language models — pretrained and non-pretrained Transformers, LSTMs, and Convolutional architectures — require local ordering more than the global ordering of tokens.","ArXiv",2021,"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar",9,72,1,"https://www.semanticscholar.org/paper/e5f6506f9332fcdb574f13a791e4f3c42b80ca90"
"b639124771f9c62cd656a24e8e685a456918e0ff",3,"Character-Level Encoding with S4 for QA","A question answering model that encodes text inputs at character level to utilize subword structures and mitigate the out-of-vocabulary problem is proposed, and both S4 and character-level encoding improve the model performance on the question answering task.","",2022,"Peng Chen",0,21,0,"https://www.semanticscholar.org/paper/b639124771f9c62cd656a24e8e685a456918e0ff"
"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa",3,"Local Structure Matters Most: Perturbation Study in NLU","It is empirically shown that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure.","FINDINGS",2021,"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar",2,70,0,"https://www.semanticscholar.org/paper/91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa"
"9933a5af7895354087baf6c96b64dc8a8973eaed",3,"Perceiver IO: A General Architecture for Structured Inputs & Outputs","The primary focus of this work is generality, rather than speed on images, and Perceiver IO uses comparable FLOPs to attention-based image classiﬁcation models, especially for the more compact conﬂguration B pretrained on JFT.","ICLR",2021,"Andrew Jaegle,Sebastian Borgeaud,Jean-Baptiste Alayrac,Carl Doersch,Catalin Ionescu,David Ding,Skanda Koppula,Andrew Brock,Evan Shelhamer,Olivier J. H'enaff,M. Botvinick,Andrew Zisserman,Oriol Vinyals,João Carreira",126,105,15,"https://www.semanticscholar.org/paper/9933a5af7895354087baf6c96b64dc8a8973eaed"
"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0",3,"Why don’t people use character-level machine translation?","It is shown that even with recent modeling innovations in character-level natural language processing, character- level MT systems still struggle to match their subword-based counterparts, and show neither better domain robustness, nor better morphological generalization.","FINDINGS",2021,"Jindřich Libovický,Helmut Schmid,Alexander Fraser",3,78,0,"https://www.semanticscholar.org/paper/f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0"
"231e768f0cd280faa0f725bb353262cb4fed08d1",3,"Hierarchical Transformers Are More Efficient Language Models","Hourglass is a hierarchical Transformer language model that sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efﬁciency on the widely studied enwik8 benchmark.","NAACL-HLT",2021,"P. Nawrot,Szymon Tworkowski,Michal Tyrolski,Lukasz Kaiser,Yuhuai Wu,Christian Szegedy,H. Michalewski",8,38,1,"https://www.semanticscholar.org/paper/231e768f0cd280faa0f725bb353262cb4fed08d1"
"2ffacbeeebd3d9e7467610057b4308635a165b6b",3,"Impact of Tokenization on Language Models: An Analysis for Turkish","Morphological and word-level tokenizers outperform de-facto tok- 017 enizers in particular cases and mini models can be competitive to larger state-of-the-art models, such that a 14-times smaller model can recover 94% of the performance of a larger model.","ArXiv",2022,"Cagri Toraman,E. Yilmaz,Furkan cSahinucc,Oguzhan Ozcelik",1,56,0,"https://www.semanticscholar.org/paper/2ffacbeeebd3d9e7467610057b4308635a165b6b"
"17e2977b907aad2532c45185947539e83ac639cd",3,"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?","This work analyzes how translation performance changes as the data ratios among languages vary in the tokenizer training corpus and finds that while relatively better performance is often observed when languages are more equally sampled, the downstream performance is more robust to language imbalance than the authors usually expected.","AMTA",2022,"Shiyue Zhang,Vishrav Chaudhary,Naman Goyal,James Cross,Guillaume Wenzek,Mohit Bansal,Francisco Guzmán",1,37,0,"https://www.semanticscholar.org/paper/17e2977b907aad2532c45185947539e83ac639cd"
"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6",3,"Lifting the Curse of Multilinguality by Pre-training Modular Transformers","This work introduces language-specific modules of their Cross-lingual Modular models from the start, which allows them to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant.","NAACL",2022,"Jonas Pfeiffer,Naman Goyal,Xi Victoria Lin,Xian Li,James Cross,Sebastian Riedel,Mikel Artetxe",8,69,0,"https://www.semanticscholar.org/paper/c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6"
"062cfd5cbafa47950a5ebbd306cd0059dfd79f75",3,"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations","This work proposes a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations, and implicitly encodes the coarse-grained information into the fine-Grained representations through contrastive learning on multi-grains information.","COLING",2022,"Borun Chen,Hongyin Tang,Jingang Wang,Qifan Wang,Haitao Zheng,Wei Yu Wu,Liqian Yu",0,39,0,"https://www.semanticscholar.org/paper/062cfd5cbafa47950a5ebbd306cd0059dfd79f75"
"110250df2a6ca0e0e609eaa800a21c17abeedd77",3,"NLP for Language Varieties of Italy: Challenges and the Path Forward","","ArXiv",2022,"Alan Ramponi",1,85,0,"https://www.semanticscholar.org/paper/110250df2a6ca0e0e609eaa800a21c17abeedd77"
"bc39d16c108057e062ad6f1d0e8154df52cafc6a",2,"CMU’s Machine Translation System for IWSLT 2019","Block Multitask Learning (BMTL) is presented, a novel NMT architecture that predicts multiple targets of different granularities simulta- neously, removing the need to search for the optimal subword segmentation strategy.","IWSLT",2019,"Tejas Srinivasan,Ramon Sanabria,Florian Metze",3,31,0,"https://www.semanticscholar.org/paper/bc39d16c108057e062ad6f1d0e8154df52cafc6a"
"fec6def294027a2ce9094267ce7b7d57f78daf74",2,"Multitask Learning For Different Subword Segmentations In Neural Machine Translation","Block Multitask Learning (BMTL), a novel NMT architecture that predicts multiple targets of different granularities simultaneously, removing the need to search for the optimal segmentation strategy, is presented.","IWSLT",2019,"Tejas Srinivasan,Ramon Sanabria,Florian Metze",3,32,0,"https://www.semanticscholar.org/paper/fec6def294027a2ce9094267ce7b7d57f78daf74"
"035df9ecf84da7ae475175f326095ab16b97dd47",2,"Investigating the Effectiveness of BPE: The Power of Shorter Sequences","The experiments show that - given a fixed vocabulary size budget - the fewer tokens an algorithm needs to cover the test set, the better the translation (as measured by BLEU).","EMNLP",2019,"Matthias Gallé",13,37,0,"https://www.semanticscholar.org/paper/035df9ecf84da7ae475175f326095ab16b97dd47"
"debb3877b778eeb8689729d37e2b90f9f000d877",2,"Neural Machine Translation with Imbalanced Classes","This work casts neural machine translation as a classification task in an autoregressive setting and analyzes the limitations of both classification and autoregression components, and investigates the effect of class imbalance on NMT.","ArXiv",2020,"Thamme Gowda,Jonathan May",3,32,0,"https://www.semanticscholar.org/paper/debb3877b778eeb8689729d37e2b90f9f000d877"
"5e788c833321b12671206b96a438c0e5b1202027",2,"Finding the Optimal Vocabulary Size for Neural Machine Translation","This work casts neural machine translation as a classification task in an autoregressive setting and analyzes the limitations of both classification and autoregression components, and reveals an explanation for why certain vocabulary sizes are better than others.","FINDINGS",2020,"Thamme Gowda,Jonathan May",28,36,1,"https://www.semanticscholar.org/paper/5e788c833321b12671206b96a438c0e5b1202027"
"58403c6bc061670f7ce6267a3a0cd01ba1bb8e50",2,"Intérêt des modèles de caractères pour la détection d’événements (The interest of character-level models for event detection)","D’intégrer des plongements de caractères, qui peuvent capturer des informations morphologiques et de forme sur les mots, à un modèle convolutif pour la détection d’événements, évaluons deux stratégies pour réaliser une telle intégration.","JEPTALNRECITAL",2021,"Emanuela Boros,Romaric Besançon,Olivier Ferret,B. Grau",0,31,0,"https://www.semanticscholar.org/paper/58403c6bc061670f7ce6267a3a0cd01ba1bb8e50"
"5c3005e22e6fb218aa76fea49971f3f991993b32",2,"Robust Open-Vocabulary Translation from Visual Text Representations","This work proposes the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows, and demonstrates significant robustness to varied types of noise.","EMNLP",2021,"Elizabeth Salesky,David Etter,Matt Post",10,52,1,"https://www.semanticscholar.org/paper/5c3005e22e6fb218aa76fea49971f3f991993b32"
"2ffbe6040369a82d5a003c2bb835e221c9d2f896",2,"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?","It is found that learning to analyse and generate morphologically complex surface representations is still challenging, especially for nonconcatenative morphological phenomena like reduplication or vowel harmony and for rare word stems.","EMNLP",2021,"Chantal Amrhein,Rico Sennrich",8,50,1,"https://www.semanticscholar.org/paper/2ffbe6040369a82d5a003c2bb835e221c9d2f896"
"2ec281d654f622da7267d7da8145e96bb77a9ede",2,"An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification","The experiments demonstrate that the transformer model with one transformer block layer surpass the performance of the widely used base architecture, LSTM, and BERT or CANINE, the pre-trained transformer models, outperforms in classifying highly imbalanced malware families according to evaluation metrics: F1-score and AUC score.","Comput. Secur.",2021,"Ferhat Demirkiran,Aykut Çayir,U. Ünal,Hasan Dag",0,61,0,"https://www.semanticscholar.org/paper/2ec281d654f622da7267d7da8145e96bb77a9ede"
"379722c04fb3b54215f82512eb86398cb02d42dd",2,"Subword Segmental Language Modelling for Nguni Languages","A subword segmental language model (SSLM) that learns how to segment words while being trained for autoregressive language modelling, enabling the model to discover morpheme-like subwords that improve its LM capabilities.","ArXiv",2022,"F. Meyer,Jan Buys",0,37,0,"https://www.semanticscholar.org/paper/379722c04fb3b54215f82512eb86398cb02d42dd"
"31852f9fc732c0868af12d631c72693702d80521",2,"Text Data Augmentation for Deep Learning","The major motifs of Data Augmentation are summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form.","J. Big Data",2021,"Connor Shorten,T. Khoshgoftaar,B. Furht",42,134,1,"https://www.semanticscholar.org/paper/31852f9fc732c0868af12d631c72693702d80521"
"373588ff1fb9f7590db000a04de8d838b1516e5a",2,"On the Effectiveness of Quasi Character-Level Models for Machine Translation","This work suggests that quasi-character-level models have practically the same generalization capabilities as character-based models but at lower computational costs and appear to help achieve greater consistency between domains than standard subword- level models, although the catastrophic forgetting problem is not mitigated.","AMTA",2022,"Salvador Carrión-Ponz,F. Casacuberta",0,38,0,"https://www.semanticscholar.org/paper/373588ff1fb9f7590db000a04de8d838b1516e5a"
"7e3081b0d698f8abf16dee626d782f3339482fe7",2,"An Assessment of the Impact of OCR Noise on Language Models","It is found that OCR noise poses a significant obstacle to language modelling, with language models increasingly diverging from their noiseless targets as OCR quality lowers, and simpler models including PPMI and Word2Vec consistently outperform transformer-based models in this respect.","ICAART",2022,"Konstantin Todorov,Giovanni Colavizza",0,51,0,"https://www.semanticscholar.org/paper/7e3081b0d698f8abf16dee626d782f3339482fe7"
"265ebfc1074c73917233dbecad802c0c64921a1c",2,"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks","This work evaluates whether the information stored within different VEs is complementary, i.e. if providing the model with features from multiple VEs can improve the performance on a target task, and suggests that diverse VEs complement each other, resulting in improved downstream V+L task performance.","ArXiv",2022,"Gregor Geigle,Chen Liu,Jonas Pfeiffer,Iryna Gurevych",0,41,0,"https://www.semanticscholar.org/paper/265ebfc1074c73917233dbecad802c0c64921a1c"
"70dd68c07b322b68836eded1fb4f78c0efcad685",2,"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models","Surprisingly, it is found that subword-based models might still be the most practical choice in many settings, achieving better performance for lower inference latency and memory usage.","ArXiv",2022,"Jimin Sun,Patrick Fernandes,Xinyi Wang,Graham Neubig",0,22,0,"https://www.semanticscholar.org/paper/70dd68c07b322b68836eded1fb4f78c0efcad685"
"9a1a9ae2fc2911f1f275702bef38aa8f79c86986",2,"What do tokens know about their characters and how do they know it?","The mechanisms through which PLMs acquire English-language character information during training are investigated and it is argued that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings.","NAACL",2022,"Ayush Kaushal,Kyle Mahowald",1,79,0,"https://www.semanticscholar.org/paper/9a1a9ae2fc2911f1f275702bef38aa8f79c86986"
"e6b73466bab5e52ce0db19dd06d9353c26557dae",2,"C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE","This work proposes subtokenziation that reduces average length by 17–40% without downstream performance drop, and shows that a carefully chosen subtokenization may significantly improve quality by 0.5-2%, possibly with some length increase.","",2022,"N. Chirkova,Sergey Troshin",0,43,0,"https://www.semanticscholar.org/paper/e6b73466bab5e52ce0db19dd06d9353c26557dae"
"b54edea6cac055d8ff9e35c2781f5e000ebdff89",2,"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data","Initial experiments using Swahili and Kinyarwanda data suggest the viability of the multi-modal approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6% F1-score above models that are trained from scratch.","ACL",2022,"Colin Leong,Daniel Whitenack",0,45,0,"https://www.semanticscholar.org/paper/b54edea6cac055d8ff9e35c2781f5e000ebdff89"
"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe",2,"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color","This cross-lingual analysis shows that textual character representations correlate strongly with sound representations for languages using an alphabetic script, while shape correlates with featural scripts.","ACL",2022,"Sidsel Boldsen,Manex Agirrezabal,Nora Hollenstein",0,77,0,"https://www.semanticscholar.org/paper/cde3d57c8f38e5dfb50edbb31faf02c23f1507fe"
"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c",2,"Evaluating Various Tokenizers for Arabic Text Classification","This paper introduces three new tokenization algorithms for Arabic and compares them to three other baselines using unsupervised evaluations and shows that the performance of suchtokenization algorithms depends on the size of the dataset, type of the task, and the amount of morphology that exists in the dataset.","Neural Processing Letters",2021,"Zaid Alyafeai,Maged S. Al-shaibani,Mustafa Ghaleb,Irfan Ahmad",4,52,0,"https://www.semanticscholar.org/paper/4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c"
"66d735987a31d666a6459566ae026c40ab9a1c3a",2,"The Efficiency Misnomer","It is demonstrated how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models, and suggestions to improve reporting of efficiency metrics are presented.","ICLR",2021,"M. Dehghani,Anurag Arnab,L. Beyer,Ashish Vaswani,Yi Tay",27,87,3,"https://www.semanticscholar.org/paper/66d735987a31d666a6459566ae026c40ab9a1c3a"
"b0c0c742789dfc98dfc2293c6ef7c7944f2c9122",2,"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language","Data2vec is a framework that uses the same learning method for either speech, NLP or computer vision to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture.","ICML",2022,"Alexei Baevski,Wei-Ning Hsu,Qiantong Xu,Arun Babu,Jiatao Gu,Michael Auli",129,79,25,"https://www.semanticscholar.org/paper/b0c0c742789dfc98dfc2293c6ef7c7944f2c9122"
"7016eb4f34611f97fe8c99176246e314678e03f4",2,"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers","This paper presents the fundamentals behind the next version of the Perspective API from Google Jigsaw, and presents a single multilingual token-free Charformer model that is applicable across a range of languages, domains, and tasks.","KDD",2022,"Alyssa Lees,V. Tran,Yi Tay,Jeffrey Scott Sorensen,Jai Gupta,Donald Metzler,Lucy Vasserman",5,37,1,"https://www.semanticscholar.org/paper/7016eb4f34611f97fe8c99176246e314678e03f4"
"a747e8f2659df479c0092301b9658fc582423df1",2,"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia","An overview of the current state of NLP research for Indonesia's 700+ languages is provided and general recommendations are provided to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.","ACL",2022,"A. F. Aji,Genta Indra Winata,Fajri Koto,Samuel Cahyawijaya,A. Romadhony,Rahmad Mahendra,Kemal Kurniawan,David Moeljadi,Radityo Eko Prasojo,Timothy Baldwin,Jey Han Lau,Sebastian Ruder",4,204,0,"https://www.semanticscholar.org/paper/a747e8f2659df479c0092301b9658fc582423df1"
"0b9e130c6305de7766697ba7655f56010aaffd61",2,"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction","A Hierarchical N -gram framework for Z ero-S hot L ink P rediction (HNZSLP) that lever-ages character n-gram information for ZSLP and achieves state-of-the-art performance on two standard Z SLP datasets.","ArXiv",2022,"Mingchen Li,Junfan Chen,Samuel Mensah,Nikolaos Aletras,Xiulong Yang,Yang Ye",0,24,0,"https://www.semanticscholar.org/paper/0b9e130c6305de7766697ba7655f56010aaffd61"
"ad6c25a46a083e02dbfcdd4b6341945d517b5e31",2,"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning","This work proposes a vocabulary-free neural tokenizer by distilling segmentation information from heuristic-based subword tokenization, which allows end-to-end task learning, resulting in optimal task-specific tokenization.","REPL4NLP",2022,"Md. Mofijul Islam,Gustavo Aguilar,Pragaash Ponnusamy,Clint Solomon Mathialagan,Chengyuan Ma,Chenlei Guo",1,23,0,"https://www.semanticscholar.org/paper/ad6c25a46a083e02dbfcdd4b6341945d517b5e31"
"f40aeae3e522ada1f6a9f326841b01ef5c8657b6",2,"Unifying Language Learning Paradigms","UL2 achieves SOTA performance on 50 well-established supervised NLP tasks ranging from language generation, language understanding, text classiﬁcation, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval.","ArXiv",2022,"Yi Tay,M. Dehghani,V. Tran,Xavier García,Dara Bahri,Tal Schuster,Huaixiu Zheng,N. Houlsby,Donald Metzler",18,113,3,"https://www.semanticscholar.org/paper/f40aeae3e522ada1f6a9f326841b01ef5c8657b6"
"645b10b7802a035e034488e3640fc0bc415de34c",2,"UL2: Unifying Language Learning Paradigms","By scaling the model up to 20B parameters, this paper achieves SOTA performance on 50 well-established supervised NLP tasks ranging from language generation, language understanding, text classiﬁcation, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval.","",2022,"Yi Tay,M. Dehghani,V. Tran,Xavier García,Jason Wei,Xuezhi Wang,Hyung Won Chung,Dara Bahri,Tal Schuster,Huaixiu Zheng,Denny Zhou,N. Houlsby,Donald Metzler",0,125,0,"https://www.semanticscholar.org/paper/645b10b7802a035e034488e3640fc0bc415de34c"
"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309",2,"Patching Leaks in the Charformer for Efficient Character-Level Generation","The GBST method from Charformer groups (aka downsamples) characters is used, thereby enabling character grouping in the decoder, and promising performance on English– Turkish translation indicate the potential of character-level models for morphologically rich languages.","ArXiv",2022,"Lukas Edman,Antonio Toral,Gertjan van Noord",0,13,0,"https://www.semanticscholar.org/paper/bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309"
"134e4d72e23bca51e290db171d063989883020f4",2,"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?","It is shown that an embedding-based metric that has the highest correlation with human evaluations on a standard benchmark can have the lowest correlation if the amount of input noise or unknown tokens increases, and the highest robustness is achieved when using character-level embeddings, instead of token-based embeddments, from the first layer of the pretrained model.","COLING",2022,"Doan Nam Long Vu,N. Moosavi,Steffen Eger",1,28,0,"https://www.semanticscholar.org/paper/134e4d72e23bca51e290db171d063989883020f4"
"8ce62f8d83f02e8ce9d7a9e1b5b7affd5c13bb7d",1,"Sensitive Data Detection with High-Throughput Neural Network Models for Financial Institutions","The experimental results show that the CNN model is simple yet effective with respect to accuracy and throughput and thus, is the most suitable candidate model to be deployed in the production environment(s), and several lessons learned on data limitations, data labelling and the intrinsic overlap of data entities are provided.","ArXiv",2020,"A. Truong,A. Walters,Jeremy Goodsitt",1,25,0,"https://www.semanticscholar.org/paper/8ce62f8d83f02e8ce9d7a9e1b5b7affd5c13bb7d"
"1babe379f8547a9dc43251e5c5a7bea4a3de5ea1",1,"Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings","A robust Hangeul word embedding model against typos is proposed that utilizes a Convolutional Neural Network architecture with a channel attention mechanism that learns to infer the original word embeddings.","EACL",2021,"O-Yeon Kwon,Dohyun Kim,Soobee Lee,Junyoung Choi,SangKeun Lee",1,18,0,"https://www.semanticscholar.org/paper/1babe379f8547a9dc43251e5c5a7bea4a3de5ea1"
"9b6e51039f57cabdb77ca70cbbdd6ff80bdf1c3a",1,"FETD2: A Framework for Enabling Textual Data Denoising via Robust Contextual Embeddings","","TPDL",2021,"Govind,Céline Alec,Jean-Luc Manguin,M. Spaniol",0,7,0,"https://www.semanticscholar.org/paper/9b6e51039f57cabdb77ca70cbbdd6ff80bdf1c3a"
"71ea59f202562410f7342764265a29a27778da69",1,"Type- and Token-based Word Embeddings in the Digital Humanities","The main goal of this contribution is to make an evidence-based argument that research on static embeddings should be continued not only because it needs less computing power and smaller corpora, but also because for this specific set of applications their performance is on par with that of dynamicembeddings.","CHR",2021,"Anton Ehrmanntraut,Thora Hagen,Leonard Konle,Fotis Jannidis",1,47,0,"https://www.semanticscholar.org/paper/71ea59f202562410f7342764265a29a27778da69"
"c3152c57f2c3960f9423f2b7278a5f98d149b3d8",1,"Differential Evaluation: a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing","The notion of differential evaluation is introduced which effectively defines a pragmatic partition of instances into gradually more difficult bins by leveraging the predictions made by a set of systems.","EVAL4NLP",2021,"L. Gianola,Hicham El Boukkouri,Cyril Grouin,T. Lavergne,P. Paroubek,Pierre Zweigenbaum",1,22,0,"https://www.semanticscholar.org/paper/c3152c57f2c3960f9423f2b7278a5f98d149b3d8"
"7dcb6a4b7b1a1d2317c3058f529c5dbcceebbcae",1,"Easy-to-use Combination of POS and BERT Model for Domain-Specific and Misspelled Terms","BERT-POS, a simple method for encoding syntax into BERT embeddings without re-training or finetuning data, based on Part-Of-Speech (POS), works at the preprocessing level and relies on POS tagging sentences.","NL4AI@AI*IA",2021,"Alexandra Benamar,Meryl Bothua,Cyril Grouin,Anne Vilnat",1,34,0,"https://www.semanticscholar.org/paper/7dcb6a4b7b1a1d2317c3058f529c5dbcceebbcae"
"d33713b55f6e79270a529cdcce4843c70a051f83",1,"UniParma at SemEval-2021 Task 5: Toxic Spans Detection Using CharacterBERT and Bag-of-Words Model","A state-of-the-art pre-trained language model (CharacterBERT), fed into the well-known BERT architecture, is used to extract features based on the word characters and improves upon that by making sure that some frequently used toxic words get labeled accordingly.","SEMEVAL",2021,"Akbar Karimi,L. Rossi,A. Prati",3,12,0,"https://www.semanticscholar.org/paper/d33713b55f6e79270a529cdcce4843c70a051f83"
"3ca6eb7b69da71899ff2d20d6a65f3e668b97312",1,"IIITT@LT-EDI-EACL2021-Hope Speech Detection: There is always hope in Transformers","This paper portrays the work for the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021- EACL 2021 and works with several transformer-based models to classify social media comments as hope speech or not hope speech in English, Malayalam, and Tamil languages.","LTEDI",2021,"Karthik Puranik,Adeep Hande,R. Priyadharshini,Sajeetha Thavareesan,Bharathi Raja Chakravarthi",42,56,1,"https://www.semanticscholar.org/paper/3ca6eb7b69da71899ff2d20d6a65f3e668b97312"
"87bca71f182bdb942adbe3f6898894cadef6776e",1,"CodemixedNLP: An Extensible and Open NLP Toolkit for Code-Mixing","CodemixedNLP is an open-source library that consists of tools to develop and benchmark versatile model architectures that are tailored for mixed texts, methods to expand training sets, techniques to quantify mixing styles, and fine-tuned state-of-the-art models for 7 tasks in Hinglish.","CALCS",2021,"Sai Muralidhar Jayanthi,Kavya Nerella,Khyathi Raghavi Chandu,A. Black",0,35,0,"https://www.semanticscholar.org/paper/87bca71f182bdb942adbe3f6898894cadef6776e"
"972793927b9b1dd0805ee36b24f37c7f2c3ba691",1,"Combining formal and machine learning techniques for the generation of JML specifications","It is shown how to generate JML annotations using a combination of 1) automatic generation of minimal predicates, 2) Natural Language Processing (NLP) based predicates generator, and 3) manual refinement and correction, to instrument and enhance code and documentation.","FTfJP@ECOOP",2021,"A. Puccetti,G. de Chalendar,P. Gibello",0,22,0,"https://www.semanticscholar.org/paper/972793927b9b1dd0805ee36b24f37c7f2c3ba691"
"82d9823db8d3ab364f04f51914919e0a7c6b9b44",1,"Character-Level Syntax Infusion in Pre-Trained Models for Chinese Semantic Role Labeling","The Character-Level Syntax-Infused network for Chinese SRL is proposed, which effectively incorporates the syntactic information between Chinese characters into pre-trained models and achieves state-of-the-art results.","Int. J. Mach. Learn. Cybern.",2021,"Yuxuan Wang,Zhilin Lei,Wanxiang Che",0,83,0,"https://www.semanticscholar.org/paper/82d9823db8d3ab364f04f51914919e0a7c6b9b44"
"e6a6c7c1e231e745021d2312f7f88f3ea24bcba8",1,"Benchmarking Multi-Task Learning for Sentiment Analysis and Offensive Language Identification in Under-Resourced Dravidian Languages","Analysis of fine-tuned models indicates the preference of multi-task learning over single- task learning resulting in a higher weighted F1-score on all three languages, including Kannada, Malayalam and Tamil.","ArXiv",2021,"Adeep Hande,Siddhanth U Hegde,R. Priyadharshini,R. Ponnusamy,P. Kumaresan,Sajeetha Thavareesan,Bharathi Raja Chakravarthi",14,123,0,"https://www.semanticscholar.org/paper/e6a6c7c1e231e745021d2312f7f88f3ea24bcba8"
"d85d0c5d218b22366abd2bd4ee35df179dda1d85",1,"CancerBERT: a BERT model for Extracting Breast Cancer Phenotypes from Electronic Health Records","The developed and evaluated cancer domain pre-trained CancerBERT models that are able to extract comprehensive collections of breast cancer related phenotypes and validated that using customized vocabulary may further improve the performances of domain specific BERT models in clinical NLP tasks.","ArXiv",2021,"Sicheng Zhou,Liwei Wang,Nan Wang,Hongfang Liu,Rui Zhang",1,46,0,"https://www.semanticscholar.org/paper/d85d0c5d218b22366abd2bd4ee35df179dda1d85"
"c0cd91d975df23ae3d7c088c4ddd375271dfe17c",1,"Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning","A novel contrastive learning framework is proposed that trains sentence embeddings to encode the relations in a graph structure to achieve state-of-the-art results on the relation extraction task using only a simple KNN classifier.","CONLL",2021,"Christos Theodoropoulos,J. Henderson,Andrei Catalin Coman,Marie-Francine Moens",0,40,0,"https://www.semanticscholar.org/paper/c0cd91d975df23ae3d7c088c4ddd375271dfe17c"
"395000b02848b666ebd2433c93127fac5113df9f",1,"BERT Cannot Align Characters","It is shown that the closer two languages are, the better BERT can align them on the character level, and the similarity matrices for natural languages show weaker relations the further apart two language are.","INSIGHTS",2021,"Antonis Maronikolakis,Philipp Dufter,Hinrich Schütze",0,27,0,"https://www.semanticscholar.org/paper/395000b02848b666ebd2433c93127fac5113df9f"
"705554c3532b7be9c1eb7c993132ffb940282e1b",1,"Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models","It is found that infrequent names are more self-similar across contexts, indicating that models rely on less context-informed representations of uncommon and minority names which are overfit to a lower number of observed contexts.","EMNLP",2021,"Robert Wolfe,Aylin Caliskan",15,50,0,"https://www.semanticscholar.org/paper/705554c3532b7be9c1eb7c993132ffb940282e1b"
"11a583c33887e8fc16c0ee8c458fad92d5641dd5",1,"Switch Point biased Self-Training: Re-purposing Pretrained Models for Code-Switching","This work proposes a self training method to repurpose the existing pretrained models using a switch-point bias by leveraging unannotated data and demonstrates that this approach performs well on both sequence labeling tasks.","EMNLP",2021,"P. Chopra,Sai Krishna Rallabandi,A. Black,Khyathi Raghavi Chandu",3,41,0,"https://www.semanticscholar.org/paper/11a583c33887e8fc16c0ee8c458fad92d5641dd5"
"597b1387f721fa8f7524b257aca920c7d373ed48",1,"Using Distributional Principles for the Semantic Study of Contextual Language Models","This article focuses on semantic similarity properties for English by exploiting the distributional principle of substitution as a probing mechanism in the controlled context of SemCor and WordNet paradigmatic relations and proposes to adapt the same method to a more open setting for characterizing the differences between static and contextual language models.","PACLIC",2021,"Olivier Ferret",0,48,0,"https://www.semanticscholar.org/paper/597b1387f721fa8f7524b257aca920c7d373ed48"
"a3d24d9d8ca41a0b87957819a7a9095d1cdb8c15",1,"Transferring BERT-like Transformers' Knowledge for Authorship Verification","This work studies the effectiveness of several BERT-like transformers for the task of authorship verification and shows that those splits can enhance the models’ capability to transfer knowledge over a new, significantly different dataset.","ArXiv",2021,"Andrei Manolache,Florin Brad,Elena Burceanu,Antonio Bărbălău,R. Ionescu,M. Popescu",3,33,1,"https://www.semanticscholar.org/paper/a3d24d9d8ca41a0b87957819a7a9095d1cdb8c15"
"cdc71d86bb0b822d73a8359338db0492a2c89b89",1,"Gated Character-aware Convolutional Neural Network for Effective Automated Essay Scoring","The experimental results show that the proposed GCCNN model outperforms the baseline deep learning models and the qualitative analysis demonstrates the importance of character-level information for tackling the out-of-vocabulary problem in grading essays.","WI/IAT",2021,"Huanyu Bai,Zhilin Huang,Anran Hao,S. C. Hui",0,26,0,"https://www.semanticscholar.org/paper/cdc71d86bb0b822d73a8359338db0492a2c89b89"
"b42e3a759348f27cca2f918a6bd0b139a5312e44",1,"A Survey of Pretrained Language Models Based Text Generation","This survey presents the recent advances achieved in the topic of PLMs for text generation and introduces three key points of applying PLMs to text generation: how to encode the input data as representations preserving input semantics which can be fused into PLMs.","ArXiv",2022,"Junyi Li,Tianyi Tang,Wayne Xin Zhao,J. Nie,Ji-rong Wen",9,237,1,"https://www.semanticscholar.org/paper/b42e3a759348f27cca2f918a6bd0b139a5312e44"
"0826af7c0f221878a142dd1681a63adb3f7e4569",1,"RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining","RoCBert is a pretrained Chinese Bert that is robust to various forms of adversarial attacks like word perturbation, synonyms, typos, etc, and is pretrained with the contrastive learning objective which maximizes the label consistency under different synthesized adversarial examples.","ACL",2022,"Hui Su,Weiwei Shi,Xiaoyu Shen,Zhou Xiao,Tuo Ji,Jiarui Fang,Jie Zhou",1,50,0,"https://www.semanticscholar.org/paper/0826af7c0f221878a142dd1681a63adb3f7e4569"
"04bc7d5ac048133a1aaead7ab1e8021b055359c4",1,"Design principles of an open-source language modeling microservice package for AAC text-entry applications","MzoLM, an open-source language model microservice package intended for use in AAC text-entry applications, is presented, with a particular focus on the design principles of the library.","SLPAT",2022,"Brian Roark,Alexander Gutkin",0,78,0,"https://www.semanticscholar.org/paper/04bc7d5ac048133a1aaead7ab1e8021b055359c4"
"d5c2119aec38b8e8e154ec2bf0106dfac1ecc80f",1,"Building Static Embeddings from Contextual Ones: Is It Useful for Building Distributional Thesauri?","This article proposes a new method for building word or type-level embeddings from contextual models that combines the generalization and the aggregation of token representations and evaluates it for a large set of English nouns from the perspective of the building of distributional thesauri for extracting semantic similarity relations.","LREC",2022,"Olivier Ferret",1,30,0,"https://www.semanticscholar.org/paper/d5c2119aec38b8e8e154ec2bf0106dfac1ecc80f"
"abb47fb115a420fc363ed3acbd8abb1a37f0dcd5",1,"DeepREF: A Framework for Optimized Deep Learning-based Relation Classification","A new open and optimizable framework, called DeepREF, is proposed, which is inspired by the OpenNRE and REflex existing frameworks, and allows the employment of various deep learning models, to optimize their use, to identify the best inputs and to get better results with each data set for RE.","LREC",2022,"Igor Nascimento,Rinaldo Lima,Adrian-Gabriel Chifu,B. Espinasse,S. Fournier",0,34,0,"https://www.semanticscholar.org/paper/abb47fb115a420fc363ed3acbd8abb1a37f0dcd5"
"edc17c862b074fb986f9b9a144f10decb843f5fa",1,"Décontextualiser des plongements contextuels pour construire des thésaurus distributionnels (Decontextualizing contextual embeddings for building distributional thesauri )","","JEPTALNRECITAL",2022,"Olivier Ferret",0,25,0,"https://www.semanticscholar.org/paper/edc17c862b074fb986f9b9a144f10decb843f5fa"
"420c897bc67e6f438db522d919d925df1a10aa8c",1,"AMMU - A Survey of Transformer-based Biomedical Pretrained Language Models","This survey discusses core concepts of transformer-based PLMs like pretraining methods, pretraining tasks, fine-tuning methods, and various embedding types specific to biomedical domain, and introduces a taxonomy for transformerbased BPLMs.","J. Biomed. Informatics",2021,"Katikapalli Subramanyam Kalyan,A. Rajasekharan,S. Sangeetha",12,227,0,"https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c"
"aeeb168feb05da1b3d31c0389e7a9196bc8970f6",1,"DravidianCodeMix: sentiment analysis and offensive language identification dataset for Dravidian languages in code-mixed text","A multilingual, manually annotated dataset for three under-resourced Dravidian languages generated from social media comments, which contains all types of code-mixing phenomena since it comprises user-generated content from a multilingual country.","Lang. Resour. Evaluation",2021,"Bharathi Raja Chakravarthi,R. Priyadharshini,V. Muralidaran,Navya Jose,Shardul Suryawanshi,E. Sherly,John P. McCrae",27,91,0,"https://www.semanticscholar.org/paper/aeeb168feb05da1b3d31c0389e7a9196bc8970f6"
"ec72909b75b2389efd588aa38d9c664e654d90d3",1,"Automatic Classification of Cancer Pathology Reports: A Systematic Review","This systematic review identifies the NLP systems for classifying pathology reports published between the years of 2010 and 2021 and benchmarked the systems based on methodology, complexity of the prediction task and core types of NLP models.","Journal of pathology informatics",2022,"Thiago Santos,Amara Tariq,J. Gichoya,H. Trivedi,I. Banerjee",3,70,0,"https://www.semanticscholar.org/paper/ec72909b75b2389efd588aa38d9c664e654d90d3"
"e923c415140a095711857420976b41e7a07cfe9c",1,"Towards improving the robustness of sequential labeling models against typographical adversarial examples using triplet loss","Experiments show that the proposed adversarial training framework provides better resistance against adversarial examples on all tasks, and can further improve the model’s robustness on the chunking task by including a triplet loss constraint.","Natural Language Engineering",2022,"Can Udomcharoenchaikit,P. Boonkwan,P. Vateekul",0,11,0,"https://www.semanticscholar.org/paper/e923c415140a095711857420976b41e7a07cfe9c"
"1b012686cf42baed47c11f0454b20a4d820a1db1",1,"The EMory BrEast imaging Dataset (EMBED): A Racially Diverse, Granular Dataset of 3.5M Screening and Diagnostic Mammograms","The EMory BrEast imaging Dataset (EMBED) addresses gaps by providing 3650,000 2D and DBT screening and diagnostic mammograms for 116,000 women divided equally between White and African American patients.","ArXiv",2022,"J. Jeong,B. Vey,A. Bhimireddy,Thomas Kim,Thiago Santos,R. Correa,Raman Dutt,M. Mosunjac,G. Oprea-Ilies,Geoffrey Smith,Min-Jae Woo,Christopher R. McAdams,M. Newell,I. Banerjee,J. Gichoya,H. Trivedi",1,37,0,"https://www.semanticscholar.org/paper/1b012686cf42baed47c11f0454b20a4d820a1db1"
"7746ac2e1e84c2f1d82a4593e2cdfdd4c93a04d8",1,"An open-source natural language processing toolkit to support software development: addressing automatic bug detection, code summarisation and code search","The paper focuses on the NLP tools developed and integrated in the Persistent Knowledge Monitor, namely the deep learning models developed to perform variable misuse, code summarisation and semantic parsing.","Open Research Europe",2022,"Cristian Robledo,Francesca Sallicati,G. de Chalendar,Marcos Fernández,Pablo de Castro,Eduardo Martín,Javier Gutiérrez,Yannis Bouachera",0,49,0,"https://www.semanticscholar.org/paper/7746ac2e1e84c2f1d82a4593e2cdfdd4c93a04d8"
"e4645f7d08b5bc878bd38c3db19c3b1a9978bd43",1,"Imputing Out-of-Vocabulary Embeddings with LOVE Makes LanguageModels Robust with Little Cost","A simple contrastive learning framework, LOVE, which extends the word representation of an existing pre-trained language model and makes it robust to OOV with few additional parameters, and can be used in a plug-and-play fashion with FastText and BERT, where it significantly improves their robustness.","ACL",2022,"Lihu Chen,G. Varoquaux,Fabian M. Suchanek",0,67,0,"https://www.semanticscholar.org/paper/e4645f7d08b5bc878bd38c3db19c3b1a9978bd43"
"16f0acab166e8ac756927fc8f2e8f68be641605b",1,"Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models","It is proposed that n-grams composed of random character sequences, or garble, provide a novel context for studying word meaning both within and beyond extant language, and it is shown that meaning and primitive information are intrinsically linked.","ACL",2022,"Mark Chu,Bhargav Srinivasa Desikan,E. Nadler,Ruggerio L. Sardo,Elise Darragh-Ford,Douglas Guilbeault",0,58,0,"https://www.semanticscholar.org/paper/16f0acab166e8ac756927fc8f2e8f68be641605b"
"e68d1aa5e78226bb868a43842a87a5532b813b0b",1,"CancerBERT: a cancer domain-specific language model for extracting breast cancer phenotypes from electronic health records","The results validated that using customized vocabulary may further improve the performances of domain specific BERT models in clinical NLP tasks and developed and evaluated CancerBERT models to extract the cancer phenotypes in clinical notes and pathology reports.","J. Am. Medical Informatics Assoc.",2022,"Sicheng Zhou,Nan Wang,Liwei Wang,Hongfang Liu,Rui Zhang",0,33,0,"https://www.semanticscholar.org/paper/e68d1aa5e78226bb868a43842a87a5532b813b0b"
"9af10e3d4ad1c6f8a0ff8fac8dec52703faa8e7e",1,"vTTS: visual-text to speech","Experimental results show that visual-text to speech is capable of generating speech with naturalness comparable to or better than a conventional TTS, it can transfer emphasis and emotion attributes in visual text to speech without additional labels and architectures, and it can synthesize more natural and intelligible speech from unseen and rare characters than conventional T TS.","ArXiv",2022,"Yoshifumi Nakano,Takaaki Saeki,Shinnosuke Takamichi,Katsuhito Sudoh,H. Saruwatari",2,36,1,"https://www.semanticscholar.org/paper/9af10e3d4ad1c6f8a0ff8fac8dec52703faa8e7e"
"b3b1659c992cbbd233038522ddd170887c033fe7",1,"Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech","The proposed Mixed-Phoneme BERT is a novel variant of the BERT model that uses mixed phoneme and sup-phoneme representations to enhance the learning capability and achieves 3 × inference speedup and similar voice quality to the previous TTS pre-trained model PnG BERT.","INTERSPEECH",2022,"Guangyan Zhang,Kaitao Song,Xu Tan,Daxin Tan,Yuzi Yan,Yanqing Liu,G. Wang,Wei Zhou,Tao Qin,Tan Lee,Sheng Zhao",2,30,1,"https://www.semanticscholar.org/paper/b3b1659c992cbbd233038522ddd170887c033fe7"
"bff16ea31d5bc6b25b6e48327a2d7026e92a788d",1,"CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos","This paper shows that a small character level perturbation in queries (as caused by typos) highly impacts the effectiveness of dense retrievers, and develops dense retriever methods that are robust to such queries with typos, while still being as performant as previous methods on queries without typos.","SIGIR",2022,"Shengyao Zhuang,G. Zuccon",5,47,1,"https://www.semanticscholar.org/paper/bff16ea31d5bc6b25b6e48327a2d7026e92a788d"
"6058ce3819d72c3e429bea58d78d80c719cb4bdb",1,"Data Augmentation for Biomedical Factoid Question Answering","It is shown that DA can lead to very significant performance gains, even when using large pre-trained Transformers, contributing to a broader discussion of if/when DA benefits large pre -trained models.","BIONLP",2022,"Dimitris Pappas,Prodromos Malakasiotis,Ion Androutsopoulos",0,87,0,"https://www.semanticscholar.org/paper/6058ce3819d72c3e429bea58d78d80c719cb4bdb"
"9727fa4acdc5312ff86745875ef3db8578f153ac",1,"Decorate the Examples: A Simple Method of Prompt Design for Biomedical Relation Extraction","This paper presents a simple yet effective method to systematically generate comprehensive prompts that reformulate the relation extraction task as a cloze-test task under a simple prompt formulation, and finds prompt-based learning requires fewer training examples to make reasonable predictions.","LREC",2022,"Hui-Syuan Yeh,T. Lavergne,Pierre Zweigenbaum",0,36,0,"https://www.semanticscholar.org/paper/9727fa4acdc5312ff86745875ef3db8578f153ac"
"91141410b0bd141bc3b1ac0c190c3867181d699c",1,"Multi-task learning in under-resourced Dravidian languages","Analysis of fine-tuned models indicates the preference of multi-task learning over single task learning resulting in a higher weighted F1 score on all three languages, and this framework is applicable to other sequence classification problems irrespective to the size of the datasets.","Journal of Data, Information and Management",2022,"Adeep Hande,Siddhanth U Hegde,Bharathi Raja Chakravarthi",2,27,0,"https://www.semanticscholar.org/paper/91141410b0bd141bc3b1ac0c190c3867181d699c"
"f1d8dbe4e217d2221c5276f1e0c616ba5f82d1d3",1,"A review on Natural Language Processing Models for COVID-19 research","A range of transformer-based biomedical pretrained language models are evaluated using the BLURB benchmark and the novel T-BPLM BioLinkBERT gives groundbreaking results by incorporating document link knowledge and hyperlinking into its pretraining.","Healthcare Analytics",2022,"Karl Hall,Victor Chang,Chrisina Jayne",0,104,0,"https://www.semanticscholar.org/paper/f1d8dbe4e217d2221c5276f1e0c616ba5f82d1d3"
"305062aa036ffeafdd6300303898dff8ed59a7dc",1,"Cross-lingual Approaches for the Detection of Adverse Drug Reactions in German from a Patient’s Perspective","This work presents the first corpus for German Adverse Drug Reaction (ADR) detection in patient-generated content, consisting of 4,169 binary annotated documents from a German patient forum, and provides preliminary experiments for binary classification using different methods of zero- and few-shot learning based on a multi-lingual model.","LREC",2022,"Lisa Raithel,Philippe E. Thomas,Roland Roller,Oliver Sapina,Sebastian Moller,Pierre Zweigenbaum",0,44,0,"https://www.semanticscholar.org/paper/305062aa036ffeafdd6300303898dff8ed59a7dc"
"c75b1fb2348c0f54259319b3595ececaf1d98430",1,"Medical terminology-based computing system: a lightweight post-processing solution for out-of-vocabulary multi-word terms","This study presents MedTCS—a lightweight, post-processing module—to simplify hybridized or compound terms into regular words using medical nomenclature and demonstrates that the proposed module enables the word embedding models to generate vectors of out-of-vocabulary words effectively.","Frontiers in Molecular Biosciences",2022,"Nadia Saeed ,Hammad Naveed",0,55,0,"https://www.semanticscholar.org/paper/c75b1fb2348c0f54259319b3595ececaf1d98430"
"7c206d541552a57b4c0cfe5b5113d3cc8e31df01",1,"IMSE: interaction information attention and molecular structure based drug drug interaction extraction","A model that leverages state of the art transformer architecture in conjunction with multiple features can bolster the performances of drug drug interation tasks in the biomedical domain and would be helpful in identification of potential adverse drug reactions.","BMC Bioinform.",2022,"Biao Duan,Jing Peng,Yi Zhang",0,46,0,"https://www.semanticscholar.org/paper/7c206d541552a57b4c0cfe5b5113d3cc8e31df01"
"8f541cf5b193654d262dbdaea2b8a0d7d913e8dc",1,"Survey of NLP in Pharmacology: Methodology, Tasks, Resources, Knowledge, and Tools","This work splits the coverage into categories to survey modern NLP methodology, commonly addressed tasks, relevant textual data, knowledge bases, and useful programming libraries, and presents a comprehensive overview of the area.","ArXiv",2022,"D. Trajanov,Vangel Trajkovski,Makedonka Dimitrieva,Jovana Dobreva,Milos Jovanovik,Matej Klemen,Alevs vZagar,Marko Robnik-vSikonja",0,245,0,"https://www.semanticscholar.org/paper/8f541cf5b193654d262dbdaea2b8a0d7d913e8dc"
"8297a3b52eea4e8c3ab8e97910839b4a3b917891",1,"On the State of the Art in Authorship Attribution and Authorship Verification","It is shown that through the application of hard-negative mining, AV methods are competitive alternatives to AA methods, and on the two AA datasets with the greatest number of words per author, BERT-based models perform best.","ArXiv",2022,"Jacob Tyo,Bhuwan Dhingra,Z. Lipton",1,71,0,"https://www.semanticscholar.org/paper/8297a3b52eea4e8c3ab8e97910839b4a3b917891"
"1e2a5dca6f310241dd8a9b56873edf8ca211c781",1,"Enriching Biomedical Knowledge for Low-resource Language Through Translation","A state-of-theart translation model in English-Vietnamese is made use to translate and produce both pretrained as well as supervised data in the biomedical domains, and ViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20 million translated abstracts from the high-quality public PubMed corpus is introduced.","bioRxiv",2022,"Long Phan,Tai Dang,Hieu Tran,Vy T Phan,Lam D. Chau,Trieu H. Trinh",0,51,0,"https://www.semanticscholar.org/paper/1e2a5dca6f310241dd8a9b56873edf8ca211c781"
"247328a082d86199ed5a98e1d726aa205c1da9df",1,"Neural Machine Translation","A comprehensive treatment of the topic, ranging from introduction to neural networks, computation graphs, description of the currently dominant attentional sequence-to-sequence model, recent refinements, alternative architectures and challenges.","ArXiv",2017,"Philipp Koehn",231,425,29,"https://www.semanticscholar.org/paper/247328a082d86199ed5a98e1d726aa205c1da9df"
"0ab0fda8774c303be8f8f8c8f684a890dcf5d455",1,"Lattice-Based Transformer Encoder for Neural Machine Translation","This work proposes lattice-based encoders to explore effective word or subword representation in an automatic way during training and proposes two methods: 1) lattice positional encoding and 2) lattICE-aware self-attention to further improve translation performance.","ACL",2019,"Fengshun Xiao,Jiangtong Li,Zhao Hai,Rui Wang,Kehai Chen",36,49,5,"https://www.semanticscholar.org/paper/0ab0fda8774c303be8f8f8c8f684a890dcf5d455"
"0aef56962035a79101821480f51897fdc4443945",1,"Character n-gram Embeddings to Improve RNN Language Models","A novel Recurrent Neural Network (RNN) language model that takes advantage of character information based on research in the field of word embedding construction and combines them with ordinary word embeddings is proposed.","AAAI",2019,"Sho Takase,Jun Suzuki,Masaaki Nagata",15,46,1,"https://www.semanticscholar.org/paper/0aef56962035a79101821480f51897fdc4443945"
"9f3f6deeb1f03ebd52f9ab44275ad382fe60d073",1,"Latent Part-of-Speech Sequences for Neural Machine Translation","A new latent variable model, LaSyn, is introduced that captures the co-dependence between syntax and semantics, while allowing for effective and efficient inference over the latent space.","EMNLP",2019,"Xuewen Yang,Yingru Liu,Dongliang Xie,Xin Wang,Niranjan Balasubramanian",10,48,2,"https://www.semanticscholar.org/paper/9f3f6deeb1f03ebd52f9ab44275ad382fe60d073"
"fac7f9b04fba4889b445e309d384644d15ee7e88",1,"Trends and Advances in Neural Machine Translation","This paper breaks down different models, approaches, inception, principle development, and structures utilized in NMT to discover a productive strategy to make a translation system and identifying the advances and imperfections of the equivalent.","2020 IEEE International Conference for Innovation in Technology (INOCON)",2020,"Purva Kulkarni,Pravina Bhalerao,Kuheli Nayek,R. Deolekar",0,24,0,"https://www.semanticscholar.org/paper/fac7f9b04fba4889b445e309d384644d15ee7e88"
"ca83469977ab49baae02ef1e12f419120927ecdc",1,"Mixed-Level Neural Machine Translation","A novel heterogeneous translation unit system, considering linguistic characteristics of the synthetic Russian language and the analytic Vietnamese language is proposed, which improves over the existing best homogeneous Russian-Vietnamese translation system by 1.17 BLEU.","Comput. Intell. Neurosci.",2020,"Thien Nguyen,Huu Nguyen,Phuoc Tran",7,25,0,"https://www.semanticscholar.org/paper/ca83469977ab49baae02ef1e12f419120927ecdc"
"188f913be48a218b44b0bd964662b4926fd0b0b8",1,"Neural Machine Translation: A Review of Methods, Resources, and Tools","A broad review of the methods for NMT is provided and focus on methods relating to architectures, decoding, and data augmentation, with a discussion of possible future research directions.","AI Open",2020,"Zhixing Tan,Shuo Wang,Zonghan Yang,Gang Chen,Xuancheng Huang,Maosong Sun,Yang Liu",26,169,0,"https://www.semanticscholar.org/paper/188f913be48a218b44b0bd964662b4926fd0b0b8"
"bfbfeb0613beae296d76077b2adc9e38a6b678a4",1,"Sub-Subword N-Gram Features for Subword-Level Neural Machine Translation","A novel approach that combines subword-level segmentation with character-level information in the form of character n-gram features to construct embedding matrices and softmax output projections for a standard encoderdecoder model that increases the vocabulary size for small training datasets without reducing translation quality.","Journal of Natural Language Processing",2021,"A. Martinez,Katsuhito Sudoh,Yuji Matsumoto",1,36,0,"https://www.semanticscholar.org/paper/bfbfeb0613beae296d76077b2adc9e38a6b678a4"
"c68959f4da94fd35da5e8649465177b24d0dd351",1,"Byte-based Multilingual NMT for Endangered Languages","This work proposes a byte-based multilingual neural machine translation system (BMNMT), which consistently and significantly outperforms subword/word-based baselines on twelve language pairs up to +18.5 BLEU points, an 840% relative improvement.","COLING",2022,"Mengjiao Zhang,Jia Xu",0,24,0,"https://www.semanticscholar.org/paper/c68959f4da94fd35da5e8649465177b24d0dd351"
"2e6028f8b156c8b344e1a68d15d88403a978c71d",1,"Learning Multiscale Transformer Models for Sequence Generation","This work built a multiscale Transformer model by establishing relationships among scales based on word-boundary information and phrase-level prior knowledge and yielded consistent performance gains over the strong baseline on several test sets without sacrificing the efficiency.","ICML",2022,"Bei Li,Tong Zheng,Yi Jing,Chengbo Jiao,Tong Xiao,Jingbo Zhu",0,58,0,"https://www.semanticscholar.org/paper/2e6028f8b156c8b344e1a68d15d88403a978c71d"
"f6167794a9fc7a58b05618091bf08e57a4e2507b",1,"SIT at MixMT 2022: Fluent Translation Built on Giant Pre-trained Models","","",2022,"Abdul Rafae Khan,Hrishikesh Kanade,Girish Amar Budhrani,Preet Jhanglani,Jia Xu",0,45,0,"https://www.semanticscholar.org/paper/f6167794a9fc7a58b05618091bf08e57a4e2507b"
"fbd47a815c73a83e8a47ee2ed38826c82ffc0c2a",1,"Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation","This work shows that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features.","ACL",2019,"Elizabeth Salesky,Matthias Sperber,A. Black",27,33,3,"https://www.semanticscholar.org/paper/fbd47a815c73a83e8a47ee2ed38826c82ffc0c2a"
"95236a88cc959656958d7a49422f4004015d594d",1,"Comparison between NMT and PBSMT Performance for Translating Noisy User-Generated Content","It is shown that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs, and suggests new avenue for improving NMT models.","NODALIDA",2019,"José Carlos Rosales Núñez,Djamé Seddah,Guillaume Wisniewski",9,46,1,"https://www.semanticscholar.org/paper/95236a88cc959656958d7a49422f4004015d594d"
"201fae97e51fb6aea7ed8120147e806e43834de6",1,"Neural Machine Translation: A Review and Survey","This work traces back the origins of modern NMT architectures to word and sentence embeddings and earlier examples of the encoder-decoder network family and concludes with a survey of recent trends in the field.","",2019,"Felix Stahlberg",9,537,1,"https://www.semanticscholar.org/paper/201fae97e51fb6aea7ed8120147e806e43834de6"
"4d08dcd2cc1e9691defe664a10f021424a896a1e",1,"Neural Machine Translation: A Review","This work traces back the origins of modern NMT architectures to word and sentence embeddings and earlier examples of the encoder-decoder network family and concludes with a survey of recent trends in the field.","J. Artif. Intell. Res.",2019,"Felix Stahlberg",55,641,3,"https://www.semanticscholar.org/paper/4d08dcd2cc1e9691defe664a10f021424a896a1e"
"8ab8288e3596fecfc2c5482cc8d48c54e97ab42e",1,"Adversarial Subword Regularization for Robust Neural Machine Translation","This paper presents adversarial subword regularization (ADVSR) to study whether gradient signals during training can be a substitute criterion for exposing diverse subword segmentations and experimentally shows that the model-based adversarial samples effectively encourage NMT models to be less sensitive to segmentation errors and improve the performance of N MT models in low-resource and out-domain datasets.","FINDINGS",2020,"Jungsoon Park,Mujeen Sung,Jinhyuk Lee,Jaewoo Kang",3,47,0,"https://www.semanticscholar.org/paper/8ab8288e3596fecfc2c5482cc8d48c54e97ab42e"
"405cd5cd6e056675d4545eba12742174cc75d7e7",1,"Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation","This work reviews approaches for improving neural machine translation for low-resource languages in the context of an asymmetric-resource one-to-many translation task, in which the pair of target languages are related, with one being a very low- resource and the other a higher-resource language.","Mach. Transl.",2020,"Stig-Arne Grönroos,Sami Virpioja,M. Kurimo",4,121,0,"https://www.semanticscholar.org/paper/405cd5cd6e056675d4545eba12742174cc75d7e7"
"38b40ae531ddca434de07015637b78413370d15a",1,"The Roles of Language Models and Hierarchical Models in Neural Sequence-to-Sequence Prediction","It is shown how traditional symbolic statistical machine translation models can still improve neural machine translation while reducing the risk of common pathologies of NMT such as hallucinations and neologisms.","EAMT",2020,"Felix Stahlberg",3,767,0,"https://www.semanticscholar.org/paper/38b40ae531ddca434de07015637b78413370d15a"
"8bcde747a44cbc2601175301808fe4518c9128dc",1,"Optimizing Word Segmentation for Downstream Task","The proposed method, optimizing tokenization (OpTok), is trained to assign a high probability to such appropriate tokenization based on the downstream task loss, and can be used for any downstream task which uses a vector representation of a sentence such as text classification.","FINDINGS",2020,"Tatsuya Hiraoka,Sho Takase,Kei Uchiumi,Atsushi Keyaki,Naoaki Okazaki",8,38,1,"https://www.semanticscholar.org/paper/8bcde747a44cbc2601175301808fe4518c9128dc"
"6dc710b46bc510a42af487a3ac220e4fabf4d518",1,"VOLT: Improving Vocabularization via Optimal Transport for Machine Translation","An exciting relation between an information-theoretic feature and BLEU scores is found and it is found that VOLT beats widely-used vocabularies on diverse scenarios, and one advantage of VOLT lies in its low resource consumption.","ArXiv",2020,"Jingjing Xu,Hao Zhou,Chun Gan,Zaixiang Zheng,Lei Li",1,33,0,"https://www.semanticscholar.org/paper/6dc710b46bc510a42af487a3ac220e4fabf4d518"
"b086b812c867b1d07eb65bcdd206dd0891733f9d",1,"Vocabulary Learning via Optimal Transport for Neural Machine Translation","This paper proposes VOLT, a simple and efficient solution without trial training that beats widely-used vocabularies in diverse scenarios, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation.","ACL",2020,"Jingjing Xu,Hao Zhou,Chun Gan,Zaixiang Zheng,Lei Li",28,32,1,"https://www.semanticscholar.org/paper/b086b812c867b1d07eb65bcdd206dd0891733f9d"
"24fd022750ea88e44e6790c7c7e1923635885c71",1,"Translation Mechanism of Neural Machine Algorithm for Online English Resources","This paper proposes a framework that integrates vocabulary alignment structure for neural machine translation at the vocabulary level and uses the word alignment structure of statistical machine translation as the external vocabulary alignment information and introduces it into the decoding step of Neural machine translation.","Complex.",2021,"Yanping Ye",2,25,0,"https://www.semanticscholar.org/paper/24fd022750ea88e44e6790c7c7e1923635885c71"
"e6b252ad22486c10b1b288e0a5e1ad468690be70",1,"Joint Optimization of Tokenization and Downstream Model","Experimental results show that the proposed method improves the performance by determining appropriate tokenizations and can be used to explore the appropriate tokenization for an already trained model as post-processing.","FINDINGS",2021,"Tatsuya Hiraoka,Sho Takase,Kei Uchiumi,Atsushi Keyaki,Naoaki Okazaki",10,40,0,"https://www.semanticscholar.org/paper/e6b252ad22486c10b1b288e0a5e1ad468690be70"
"8a1c54f6e2c5f1453fddb9f15e769a099286f677",1,"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey","This work surveys approaches to domain adaptation for NMT, particularly where a system may need to translate across multiple domains, and divides techniques into those revolving around data selection or generation, model architecture, parameter adaptation procedure, and inference procedure.","J. Artif. Intell. Res.",2021,"Danielle Saunders",15,366,0,"https://www.semanticscholar.org/paper/8a1c54f6e2c5f1453fddb9f15e769a099286f677"
"3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2",1,"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class","This work investigates how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains, and evaluates DIRE’s overall performance without respect to data quality.","ACM Transactions on Software Engineering and Methodology",2022,"Luke Dramko,Jeremy Lacomis,Pengcheng Yin,Edward J. Schwartz,Miltiadis Allamanis,Graham Neubig,Bogdan Vasilescu,Claire Le Goues",0,58,0,"https://www.semanticscholar.org/paper/3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2"
"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1",1,"You should evaluate your language model on marginal likelihood over tokenisations","It is argued that language models should be evaluated on their marginal likelihood over tokenisations, and it is shown that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data.","EMNLP",2021,"Kris Cao,Laura Rimell",3,42,0,"https://www.semanticscholar.org/paper/9635e3c008f7bfa80638ade7134a8fb0ef1b37e1"
"b62a56f629f77acce9ea69456d67dd77de1c7dfb",1,"Clustering Monolingual Vocabularies to Improve Cross-Lingual Generalization","This work explores the idea of learning multilingual language models based on clustering of monolingual segments and shows significant improvements over standard multilingual segmentation and training across nine languages on a question answering task, both in a small model regime and for a model of the size of BERT-base.","MRL",2021,"R. Bassani,Anders Søgaard,Tejaswini Deoskar",1,79,0,"https://www.semanticscholar.org/paper/b62a56f629f77acce9ea69456d67dd77de1c7dfb"
"0d4b5c9a071557f4eb12f63f785dbc89071d4272",1,"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models","It is found that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.","ACL",2020,"Phillip Rust,Jonas Pfeiffer,Ivan Vulic,Sebastian Ruder,Iryna Gurevych",64,102,9,"https://www.semanticscholar.org/paper/0d4b5c9a071557f4eb12f63f785dbc89071d4272"
"dca4128a33ca22c02031b5c0c28548a0df022d80",1,"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding","The Embedding Barrier is introduced, a phenomenon that limits the monolingual performance of multilingual models on low-resource languages having unique typologies and a straightforward solution by transcribing languages to a common script is proposed, which can effectively improve the performance of a multilingual model for the Bangla language.","",2021,"Abhik Bhattacharjee,Tahmid Hasan,Kazi Samin,Md. Saiful Islam,M. S. Rahman,Anindya Iqbal,Rifat Shahriyar",5,56,0,"https://www.semanticscholar.org/paper/dca4128a33ca22c02031b5c0c28548a0df022d80"
"2b9762e91305986ac8a2d624d0a69521304405f3",1,"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation","This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned, and provides a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.","EMNLP",2021,"Sebastian Ruder,Noah Constant,Jan A. Botha,Aditya Siddhant,Orhan Firat,Jinlan Fu,Pengfei Liu,Junjie Hu,Graham Neubig,Melvin Johnson",65,62,18,"https://www.semanticscholar.org/paper/2b9762e91305986ac8a2d624d0a69521304405f3"
"a20a802839d72bee1c85f4a1cb77addadacb2179",1,"Specializing Multilingual Language Models: An Empirical Study","These evaluations on part-of-speech tagging, universal dependency parsing, and named entity recognition in nine diverse low-resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low- resource settings.","MRL",2021,"Ethan C. Chau,Noah A. Smith",7,72,0,"https://www.semanticscholar.org/paper/a20a802839d72bee1c85f4a1cb77addadacb2179"
"3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5",1,"Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data","Experimental results on three multilingual semantic parsing datasets show that data augmentation with TaF reaches accuracies competitive with similar systems which rely on traditional alignment techniques.","EMNLP",2021,"M. Nicosia,Zhongdi Qu,Y. Altun",7,62,2,"https://www.semanticscholar.org/paper/3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5"
"e43a360728e34e43934707665db0f7be02c8e5a7",1,"Interpreting BERT-based stance classification: a case study about the Brazilian COVID vaccination","This paper proposes a BERT-based stance classification model and an attention-based mechanism to identify the influential words for stance classification and uses these metrics to assess if words with high attention weights correspond to domain intrinsic properties and contribute to the correct classification of stances.","SBBD",2021,"Carlos Abel Córdova Sáenz,Karin Becker",2,16,0,"https://www.semanticscholar.org/paper/e43a360728e34e43934707665db0f7be02c8e5a7"
"12fec03c538c7aaeca1a4e1ab8d66aed5f793fc0",1,"reamtchka at SemEval-2022 Task 6: Investigating the effect of different loss functions for Sarcasm detection for unbalanced datasets","A voting classifier between either multiple different BERT-based models or machine learning models is proposed, as the final model in SemEval-2022 Task 6: Intended Sarcasm Detection in English and Arabic.","SEMEVAL",2022,"Reem Abdel-Salam",1,37,0,"https://www.semanticscholar.org/paper/12fec03c538c7aaeca1a4e1ab8d66aed5f793fc0"
"b1be10b76314ea8569249f2310f1e6eead8a5adc",1,"Building Domain-specific Corpora from the Web: the Case of European Digital Service Infrastructures","This paper explores the feasibility of building an automatic classifier that allows to identify which segments in a generic corpus are relevant for a particular DSI, and uses pre-trained (multilingual) language models to perform the classification.","BUCC",2022,"Rik van Noord,Cristian García-Romero,M. Esplà-Gomis,Leopoldo Pla Sempere,Antonio Toral",0,23,0,"https://www.semanticscholar.org/paper/b1be10b76314ea8569249f2310f1e6eead8a5adc"
"e11d8663ccf2cc412f408853fa5f19ebac75df54",1,"How to encode arbitrarily complex morphology in word embeddings, no corpus needed","The word embeddings in this paper are explicitly designed to be both linguistically interpretable and fully capable of handling the broad variety found in the world’s diverse set of 7000 languages, regardless of corpus size or morphological characteristics.","FIELDMATTERS",2022,"Lane Schwartz,Coleman Haley,Francis M. Tyers",0,49,0,"https://www.semanticscholar.org/paper/e11d8663ccf2cc412f408853fa5f19ebac75df54"
"610088e1d7a8e44f921b2c4894fe1a7b204ce5a9",1,"Vicomtech at LivingNER2022","","IberLEF@SEPLN",2022,"Elena Zotova,Aitor García Pablos,Naiara Pérez,Pablo Turón,Montse Cuadros",0,13,0,"https://www.semanticscholar.org/paper/610088e1d7a8e44f921b2c4894fe1a7b204ce5a9"
"12809bcb734beafeb47876f42e7b438e27fe99fe",1,"General-purpose, long-context autoregressive modeling with Perceiver AR","Perceiver AR is developed, an modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms.","ICML",2022,"Curtis Hawthorne,Andrew Jaegle,Cătălina Cangea,Sebastian Borgeaud,Charlie Nash,Mateusz Malinowski,S. Dieleman,Oriol Vinyals,M. Botvinick,Ian Simon,Hannah R. Sheahan,Neil Zeghidour,Jean-Baptiste Alayrac,João Carreira,Jesse Engel",13,74,1,"https://www.semanticscholar.org/paper/12809bcb734beafeb47876f42e7b438e27fe99fe"
"7ed63a4f928fc126f6780d27e75fa6447a00a8c4",1,"Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference","This work proposes a simple method for predicting the performance without actually fine-tuning the model of a natural language inference model, and shows that the accuracy of the cosine similarity approach correlates strongly with theuracy of the classification approach with a Pearson correlation coefficient of 0.65.","NAACL",2022,"Emils Kadickis,Vaibhav Srivastav,Roman Klinger",0,38,0,"https://www.semanticscholar.org/paper/7ed63a4f928fc126f6780d27e75fa6447a00a8c4"
"0376c01a9320027694f2dca57236c27f0c5b36eb",1,"Multilingual Abusiveness Identification on Code-Mixed Social Media Text","This work proposes an approach for abusiveness identiﬁcation on the multilingual Moj dataset which comprises of Indic languages and tackles the common challenges of non-English social media content and can be extended to other languages as well.","ArXiv",2022,"Ekagra Ranjan,Naman Poddar",0,29,0,"https://www.semanticscholar.org/paper/0376c01a9320027694f2dca57236c27f0c5b36eb"
"34f2ed08461edc261984fc1c86a14ff7b4c3d2db",1,"KinyaBERT: a Morphology-aware Kinyarwanda Language Model","A simple yet effective two-tier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionality is proposed, naming the proposed model architecture KinyaBERT.","ACL",2022,"Antoine Nzeyimana,Andre Niyongabo Rubungo",5,70,0,"https://www.semanticscholar.org/paper/34f2ed08461edc261984fc1c86a14ff7b4c3d2db"
"ba9a592438447c2a35afc203be40f7f0a2d3fb5a",1,"A Compact Pretraining Approach for Neural Language Models","This study shows that pretrained NLMs learn in-domain information more effectively and faster from a compact subset of the data that focuses on the key information in the domain.","ArXiv",2022,"Shahriar Golchin,M. Surdeanu,N. Tavabi,A. Kiapour",0,22,0,"https://www.semanticscholar.org/paper/ba9a592438447c2a35afc203be40f7f0a2d3fb5a"
"695ea88d22e7d39fa3d0b9f53e75d41e82be81d8",1,"Transformers with Learnable Activation Functions","This paper investigates the effectiveness of using Rational Activation Function (RAF) that is a learnable activation function in the Transformer architecture and opens a new research direction for analyzing and interpreting pre-trained models according to the learned activation functions.","ArXiv",2022,"Haishuo Fang,Ji-Ung Lee,N. Moosavi,Iryna Gurevych",0,67,0,"https://www.semanticscholar.org/paper/695ea88d22e7d39fa3d0b9f53e75d41e82be81d8"
"923d2376103dbc9e9b2af4518b56299d7630b46b",1,"J URASSIC -1: T ECHNICAL D ETAILS AND E VALUATION","Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1""-Large"", and their architecture and training are described and their performance relative to GPT-3 is evaluated.","",2021,"Opher Lieber",0,21,0,"https://www.semanticscholar.org/paper/923d2376103dbc9e9b2af4518b56299d7630b46b"
"518cb6d4247bdebf21e2811f296b0c7372602a0a",1,"PMI-Masking: Principled masking of correlated spans","PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking, such as whole-word masks, entity/phrase masksing, and random-span masking.","ICLR",2020,"Yoav Levine,Barak Lenz,Opher Lieber,Omri Abend,Kevin Leyton-Brown,Moshe Tennenholtz,Y. Shoham",28,30,7,"https://www.semanticscholar.org/paper/518cb6d4247bdebf21e2811f296b0c7372602a0a"
"09c896e30d1021b1284b68ed65b93b593f2c3f4f",1,"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding","ERNIE-Gram is proposed, an explicitly n-gram masking method to enhance the integration of coarse-grained information into pre-training and outperforms previous pre- training models like XLNet and RoBERTa by a large margin, and achieves comparable results with state-of-the-art methods.","NAACL",2020,"Dongling Xiao,Yukun Li,Han Zhang,Yu Sun,Hao Tian,Hua Wu,Haifeng Wang",12,52,1,"https://www.semanticscholar.org/paper/09c896e30d1021b1284b68ed65b93b593f2c3f4f"
"964da4ed9ac61b12bc2a7adc1e94e3964cc63861",1,"Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question Answering Data","A self-teaching paradigm is proposed to better use the generated weakly-labeled MRC instances to improve a target MRC task and the effectiveness of this framework and the usefulness of large-scale subjectarea question-answering data for machine reading comprehension are demonstrated.","EMNLP",2021,"Dian Yu,Kai Sun,Dong Yu,Claire Cardie",2,80,0,"https://www.semanticscholar.org/paper/964da4ed9ac61b12bc2a7adc1e94e3964cc63861"
"a77643bff6f50ccc4f80ec081e4d078a2e788ae7",1,"Multi-view Subword Regularization","To take full advantage of different possible input segmentations, the proposed Multi-view Subword Regularization (MVR) method enforces the consistency of predictors between using inputs tokenized by the standard and probabilistic segmentations.","NAACL",2021,"Xinyi Wang,Sebastian Ruder,Graham Neubig",26,40,4,"https://www.semanticscholar.org/paper/a77643bff6f50ccc4f80ec081e4d078a2e788ae7"
"111dbe14083359ab39886790632e7f1421732a8a",1,"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models","This work proposes a novel pre-training paradigm for Chinese — Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner and achieves new state-of-the-art among base-size models on the CLUE benchmarks.","NAACL",2021,"Yuxuan Lai,Yijia Liu,Yansong Feng,Songfang Huang,Dongyan Zhao",13,33,2,"https://www.semanticscholar.org/paper/111dbe14083359ab39886790632e7f1421732a8a"
"7da82508a76698f93e733d7a13d9fb13dbafba3e",1,"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining","It is found that SHUOWEN and JIEZI tokenizers can generally outperform conventional singlecharacter tokenizers, while Chinese word segmentation shows no benefit as a preprocessing step, and exhibit significantly better robustnesses on handling noisy texts.","ArXiv",2021,"Chenglei Si,Zhengyan Zhang,Yingfa Chen,Fanchao Qi,Xiaozhi Wang,Zhiyuan Liu,Maosong Sun",5,37,1,"https://www.semanticscholar.org/paper/7da82508a76698f93e733d7a13d9fb13dbafba3e"
"4690eb050572a279f94560b6bbdccaae577b45f5",1,"MVP-BERT: Multi-Vocab Pre-training for Chinese BERT","Experiments show that MVP training strategies improve PLMs’ downstream performances, especially it can improve the PLM’s performances on span-level tasks, and the AL-MVP outperforms the recent AMBERT (CITATION) after large-scale pre-training, and it is more robust against adversarial attacks.","ACL",2021,"Wei Zhu",1,33,0,"https://www.semanticscholar.org/paper/4690eb050572a279f94560b6bbdccaae577b45f5"
"27c39dd62635791a0ec3c0c81c2690e7a9bd62ad",1,"LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization","This paper proposes a simple yet effective pretraining method named LICHEE to efficiently incorporate multi-grained information of input text that can be applied to various pretrained language models and improve their representation capability.","FINDINGS",2021,"Weidong Guo,Mingjun Zhao,Lusheng Zhang,Di Niu,Jinwen Luo,Zhenhua Liu,Zhenyang Li,J. Tang",4,23,2,"https://www.semanticscholar.org/paper/27c39dd62635791a0ec3c0c81c2690e7a9bd62ad"
"5722d101859846a6a023b8fa00830742203cd0c1",1,"Span Fine-tuning for Pre-trained Language Models","A novel span fine-tuning method for PrLMs is presented, which facilitates the span setting to be adaptively determined by specific downstream tasks during the fine- Tuning phase.","EMNLP",2021,"Rongzhou Bao,Zhuosheng Zhang,Hai Zhao",0,33,0,"https://www.semanticscholar.org/paper/5722d101859846a6a023b8fa00830742203cd0c1"
"976f47bade21fd787f029142b39631cb17f16ec2",1,"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation","The unbalanced Transformer saves the computational and storage cost, which makes CPT competitive and greatly accelerates the inference of text generation.","ArXiv",2021,"Yunfan Shao,Zhichao Geng,Yitao Liu,Junqi Dai,Fei Yang,Li Zhe,H. Bao,Xipeng Qiu",32,45,9,"https://www.semanticscholar.org/paper/976f47bade21fd787f029142b39631cb17f16ec2"
"35eeeefcbea6fa8edae4c310ee5ee717861c7e60",1,"Improving Constituent Representation with Hypertree Neural Networks","This paper aims to improve representations of constituent spans using a novel hypertree neural networks (HTNN) that is structured with constituency parse trees that incorporates both bottom-up and top-down compositional information.","NAACL",2022,"Hao Zhou,Gongshen Liu,Kewei Tu",0,33,0,"https://www.semanticscholar.org/paper/35eeeefcbea6fa8edae4c310ee5ee717861c7e60"
"18dc24da603896db2264e9174116407a95c593d5",1,"“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction","Three Chinese BERT models with standard character-level masking (CLM), WWM, and a combination of CLM and WWM are trained and it is found that when one character needs to be inserted or replaced, the model trained with CLM performs the best.","FINDINGS",2022,"Yong Dai,Linyang Li,Cong Zhou,Zhangyin Feng,Enbo Zhao,Xipeng Qiu,Pijian Li,Duyu Tang",2,26,0,"https://www.semanticscholar.org/paper/18dc24da603896db2264e9174116407a95c593d5"
"8e1227b0d58b769a0919f8debdfd093ff6f6a65a",1,"MarkBERT: Marking Word Boundaries Improves Chinese BERT","A Chinese BERT model dubbed MarkBERT that uses word information and inserts boundary markers between contiguous words, which enables the model to handle any words in the same way, no matter they are OOV words or not is presented.","ArXiv",2022,"Linyang Li,Yong Dai,Duyu Tang,Zhangyin Feng,Cong Zhou,Xipeng Qiu,Zenglin Xu,Shuming Shi",2,30,0,"https://www.semanticscholar.org/paper/8e1227b0d58b769a0919f8debdfd093ff6f6a65a"
"05ea28584e5db18c0c31d1aac40e9c1905327557",1,"Effectiveness of Fine-tuned BERT Model in Classification of Helpful and Unhelpful Online Customer Reviews","","Electronic Commerce Research",2022,"Muhammad Bilal,A. A. Almazroi",8,65,0,"https://www.semanticscholar.org/paper/05ea28584e5db18c0c31d1aac40e9c1905327557"
"f8a2428bee464ae69cdb25ce5c2259e88ce576b4",1,"Clickbait Detection of Indonesian News Headlines using Fine-Tune Bidirectional Encoder Representations from Transformers (BERT)","This study fine-tunes the Bidirectional Encoder Representations from Transformers (BERT) and uses the Indonesian news headlines dataset CLICK-ID to predict clickbait (BerT), and evaluation results indicate that all fine-tuned IndoBERT classifiers outperform all word-vectors-based machine learning classifiers in classifying Clickbait and non-clickbait IndonesianNews headlines.","Inform : Jurnal Ilmiah Bidang Teknologi Informasi dan Komunikasi",2022,"Diyah Utami Kusumaning Putri,Dinar Nugroho Pratomo",0,21,0,"https://www.semanticscholar.org/paper/f8a2428bee464ae69cdb25ce5c2259e88ce576b4"
"31d6e59dc275f33da71a72b6b86b38daeed9ffaa",1,"mmLayout: Multi-grained MultiModal Transformer for Document Understanding","Experimental results on four tasks, including information extraction and document question answering, show that the proposed method can improve the performance of multimodal Transformers based on fine-grained elements and achieve better performance with fewer parameters.","ACM Multimedia",2022,"Wenjin Wang,Zhengjie Huang,Bin Luo,Qianglong Chen,Qiming Peng,Yinxu Pan,Weichong Yin,Shi Feng,Yu Sun,Dianhai Yu,Yin Zhang",0,49,0,"https://www.semanticscholar.org/paper/31d6e59dc275f33da71a72b6b86b38daeed9ffaa"
"04cd4c224f61e8f25a405103d4210f161d091d1c",1,"Look It Up: Bilingual Dictionaries Improve Neural Machine Translation","A new method for “attaching” dictionary definitions to rare words so that the network can learn the best way to use them and demonstrate provements of up to 1.8 BLEU using bilingual dictionaries.","",2020,"X. Zhong,David Chiang",0,39,0,"https://www.semanticscholar.org/paper/04cd4c224f61e8f25a405103d4210f161d091d1c"
"0a82ffe5c889d4defbd6300fffab4f3fa3dade99",1,"Deep convolution neural network with weighted loss to detect rice seeds vigor based on hyperspectral imaging under the sample-imbalanced condition","","Comput. Electron. Agric.",2022,"Na Wu,Shizhuang Weng,Jinxin Chen,Qinlin Xiao,Chu Zhang,Yong He",1,45,0,"https://www.semanticscholar.org/paper/0a82ffe5c889d4defbd6300fffab4f3fa3dade99"
"cb48e3bcc185ae94899007e1ad3cdb49ff39428b",1,"Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video","Using egocentric video to capture the hand use of stroke survivors at home is feasible and Pose estimation to track finger movements may be beneficial to classifying hand roles in the future.","ArXiv",2022,"Meng-Fen Tsai,Rosalie H. Wang,J. Zariffa",0,84,0,"https://www.semanticscholar.org/paper/cb48e3bcc185ae94899007e1ad3cdb49ff39428b"
"e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7",1,"Checks and Strategies for Enabling Code-Switched Machine Translation","This work explores multilingual NMT models’ ability to handle code-switched text, and proposes checks to measure switching capability and investigates simple and effective data augmentation methods that can enhance an NMT model’s ability to support code- Switched text.","ArXiv",2022,"Thamme Gowda,Mozhdeh Gheini,Jonathan May",0,45,0,"https://www.semanticscholar.org/paper/e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7"
"ae7fc32df9401a86353185515f4fd5e2020ac922",1,"MutFormer: A context-dependent transformer-based model to predict pathogenic missense mutations","This study introduces MutFormer, a transformer-based model for the prediction of pathogenic missense mutations, using reference and mutated amino acid sequences from the human genome as the features, and shows that MutFormer outperforms a variety of existing tools in pathogenicity prediction.","ArXiv",2021,"Theodore Jiang,Li Fang,Kai Wang",3,40,1,"https://www.semanticscholar.org/paper/ae7fc32df9401a86353185515f4fd5e2020ac922"
"e873ddaf58ff92ae0492983cf57221fb25837f4e",1,"Strategies in subword tokenization: humans vs. algorithms","A new method to an- 010 alyze subword segmentation strategies relying on a spatial analysis of the distribution of sub- 012 words’ lengths is proposed, which shows that humans tend to balance creativity and consistency, while algorithms tend to be either strongly biased or inconsistent.","",2021,"",0,13,0,"https://www.semanticscholar.org/paper/e873ddaf58ff92ae0492983cf57221fb25837f4e"
"7175caf7568d46c857380d0e5b64653819d5cc45",1,"MultiLexNorm: A Shared Task on Multilingual Lexical Normalization","The MULTILEXNORM shared task provides the largest publicly available multilingual lexical normalization benchmark including 12 language variants and proposes a homogenized evaluation setup with both intrinsic and extrinsic evaluation.","Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)",2021,"R. Goot,Alan Ramponi,A. Zubiaga,Barbara Plank,Benjamin Muller,I. Roncal,Nikola Ljubesic,Özlem Çetinoğlu,Rahmad Mahendra,Talha Çolakoglu,Timothy Baldwin,Tommaso Caselli,Wladimir Sidorenko,Bruno Kessler",10,75,0,"https://www.semanticscholar.org/paper/7175caf7568d46c857380d0e5b64653819d5cc45"
"d13a0c8d49cb268d8d245925baee0316c1fe1875",1,"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention","This work empirically demonstrates the existence of an embedding rank bottleneck and its implications on the depth-to-width interplay of Transformer architectures, linking the architecture variability across domains to the often glossed-over usage of different vocabulary sizes or embedding ranks in different domains.","ICML",2021,"Noam Wies,Yoav Levine,Daniel Jannai,A. Shashua",10,52,0,"https://www.semanticscholar.org/paper/d13a0c8d49cb268d8d245925baee0316c1fe1875"
"06431546c21d7c2528aaa170c2e1078e0a82d12e",1,"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer","English is compared against other transfer languages for fine-tuning, and other high-resource languages such as German and Russian often transfer more effectively, especially when the set of target languages is diverse or unknown a priori.","ArXiv",2021,"Iulia Turc,Kenton Lee,Jacob Eisenstein,Ming-Wei Chang,Kristina Toutanova",22,33,3,"https://www.semanticscholar.org/paper/06431546c21d7c2528aaa170c2e1078e0a82d12e"
"14dcb7dd36960eb712fa1fad06ddf771638e02a3",1,"How Optimal is Greedy Decoding for Extractive Question Answering?","","ArXiv",2021,"Or Castel,Ori Ram,Avia Efrat,Omer Levy",2,29,0,"https://www.semanticscholar.org/paper/14dcb7dd36960eb712fa1fad06ddf771638e02a3"
"89a4f4d1f8c93da85d829a1acfe8cafea2b50c00",1,"Transfer Learning for Multi-lingual Tasks - a Survey","This survey provides a comprehensive overview of the existing literature with a focus on transfer learning techniques in multilingual tasks and identifies potential opportunities for further research in this domain.","ArXiv",2021,"Amir Reza Jafari,Behnam Heidary,R. Farahbakhsh,Mostafa Salehi,M. Jalili",0,108,0,"https://www.semanticscholar.org/paper/89a4f4d1f8c93da85d829a1acfe8cafea2b50c00"
"972808b92159a782f5ca1c1ab3a8ed3867acfb2d",1,"mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset","","ArXiv",2021,"L. Bonifacio,Israel Campiotti,R. Lotufo,Rodrigo Nogueira",12,51,2,"https://www.semanticscholar.org/paper/972808b92159a782f5ca1c1ab3a8ed3867acfb2d"
"39262814fa3e47905a2e5facf13465a1f70706b9",1,"Single-Read Reconstruction for DNA Data Storage Using Transformers","This work proposes a novel approach for single-read reconstruction using an encoder-decoder Transformer architecture for DNA based data storage and achieves lower error rates when reconstructing the original data from a single read of each DNA strand compared to state-of-the-art algorithms using 2-3 copies.","ArXiv",2021,"Yotam Nahum,Eyar Ben-Tolila,Leon Anavy",3,41,0,"https://www.semanticscholar.org/paper/39262814fa3e47905a2e5facf13465a1f70706b9"
"9d13bde760d8e77f059436d60160881becd2d2e0",1,"Predicting Ethnicity from Names with rethnicity: Methodology and Application","A new R package, rethnicity1 is provided for predicting ethnicity based on names, using the Bidirectional LSTM and Florida Voter Registration as the model and training data.","ArXiv",2021,"Fangzhou Xie",0,28,0,"https://www.semanticscholar.org/paper/9d13bde760d8e77f059436d60160881becd2d2e0"
"4318c9f87221b9f504f286540c9a6d5c1cc4e4c8",1,"Rethnicity: Predicting Ethnicity from Names","An R package, rethniicty, is provided for predicting ethnicity from names, using the Bidirectional LSTM as the model and Florida Voter Registration as training data and the availability, accuracy, and performance are compared.","",2021,"Fangzhou Xie",2,26,0,"https://www.semanticscholar.org/paper/4318c9f87221b9f504f286540c9a6d5c1cc4e4c8"
"c107835a05ca6fda6e73b64e2ed9884de4fcec0f",1,"A proposed conceptual framework for a representational approach to information retrieval","A representational approach that breaks the core text retrieval problem into a logical scoring model and a physical retrieval model that establishes connections to sentence similarity tasks in natural language processing and information access ""technologies"" prior to the dawn of computing.","SIGIR Forum",2021,"Jimmy J. Lin",17,86,0,"https://www.semanticscholar.org/paper/c107835a05ca6fda6e73b64e2ed9884de4fcec0f"
"de0e1f9980afa7949df64d53b8ae7a2f59c55579",1,"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages","This work pushes the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases, and achieves 2-3x better performance and output diversity in formality transfer and code-mixing addition across five Indian languages.","ArXiv",2021,"Kalpesh Krishna,Deepak Nathani,Xavier García,Bidisha Samanta,P. Talukdar",5,77,0,"https://www.semanticscholar.org/paper/de0e1f9980afa7949df64d53b8ae7a2f59c55579"
"7c496490abcd8d5c6e90aa3d3c82bde02680f5b0",1,"MutFormer: A context-dependent transformer-based model to predict deleterious missense mutations from protein sequences in the human genome","The introduction of MutFormer, a transformer-based model for the prediction of deleterious missense mutations that uses reference and mutated protein sequences from the human genome as the primary features, and which successfully considers sequence features that are not explored in previous studies.","",2021,"Theodore Jiang,Li Fang,Kai Wang",0,48,0,"https://www.semanticscholar.org/paper/7c496490abcd8d5c6e90aa3d3c82bde02680f5b0"
"6e2682eb2fbec93b329028b23764c1164e232c41",1,"ÚFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-tuning ByT5","This paper presents the winning entry to the Multilingual Lexical Normalization (MultiLexNorm) shared task at W-NUT 2021, which evaluates lexical-normalization systems on 12 social media datasets in 11 languages with the best performance by a wide margin.","WNUT",2021,"David Samuel,Milan Straka",5,43,0,"https://www.semanticscholar.org/paper/6e2682eb2fbec93b329028b23764c1164e232c41"
"9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e",1,"Large Dual Encoders Are Generalizable Retrievers","Experimental results show that the dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform existing sparse and dense retrievers on the BEIR dataset (Thakur et al., 2021) significantly and is very data efficient, as it only needs 10% of MS Marco supervised data to achieve the best out-of-domain performance.","ArXiv",2021,"Jianmo Ni,Chen Qu,Jing Lu,Zhuyun Dai,Gustavo Hern'andez 'Abrego,Ji Ma,Vincent Zhao,Yi Luan,K. Hall,Ming-Wei Chang,Yinfei Yang",23,32,9,"https://www.semanticscholar.org/paper/9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e"
"db3690379953f2ea4f2a015615de02e643d437ea",1,"PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers","To enable future amelio-ration efforts, introduce a novel model-driven technique, the progressive evaluation of cluster outliers (PECO) which enables both the ob-jective measurement of leakage, and the automated detection of subpopulations in the data which maximally exhibit it.","",2021,"Michael Stephen Saxon,Xinyi Wang,Wenda Xu,William Yang Wang",0,48,0,"https://www.semanticscholar.org/paper/db3690379953f2ea4f2a015615de02e643d437ea"
"5f77157038dc7f189db7e72ea58567039222d9df",1,"Examining Single Sentence Label Leakage in Natural Language Inference Datasets","This work examines how regular NLI models cheat on single sentence label leakage, and discusses how to ameliorate this.","",2022,"Michael Stephen Saxon,Xinyi Wang,Wenda Xu,William Yang Wang",0,42,0,"https://www.semanticscholar.org/paper/5f77157038dc7f189db7e72ea58567039222d9df"
"2b6b38b66fcca218cb2f381e5d4711b5c99a784f",1,"Training Text-to-Text Transformers with Privacy Guarantees","By using recent advances in JAX and XLA, this work can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE).","FINDINGS",2022,"N. Ponomareva,Jasmijn Bastings,Sergei Vassilvitskii",0,37,0,"https://www.semanticscholar.org/paper/2b6b38b66fcca218cb2f381e5d4711b5c99a784f"
"32290d428b50f64d1cfee6ea658dc6ba977b26e9",1,"Sammaan@LT-EDI-ACL2022: Ensembled Transformers Against Homophobia and Transphobia","The approach to classify homophobia and transphobia in social media comments using an ensemble of transformer-based models to build a classifier that ranked 2nd for English, 8th for Tamil and 10th for Chennai-English.","LTEDI",2022,"I. Upadhyay,K V Aditya Srivatsa,Radhika Mamidi",2,33,0,"https://www.semanticscholar.org/paper/32290d428b50f64d1cfee6ea658dc6ba977b26e9"
"b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83",1,"A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference","It is argued that the transfer learning-based loss objective is model agnostic and thus can be used with other deep learning- based architectures for cross-lingual NLI.","LREC",2022,"Dibyanayan Bandyopadhyay,Arkadipta De,Baban Gain,Tanik Saikh,Asif Ekbal",0,24,0,"https://www.semanticscholar.org/paper/b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83"
"2005311276a1a90dc17cf6e2ca7725fee2e76e1e",1,"BasqueGLUE: A Natural Language Understanding Benchmark for Basque","BasqueGLUE is presented, the first NLU benchmark for Basque, a less-resourced language, which has been elaborated from previously existing datasets and following similar criteria to those used for the construction of GLUE and SuperGLUE.","LREC",2022,"Gorka Urbizu,Iñaki San Vicente,X. Saralegi,Rodrigo Agerri,Aitor Soroa Etxabe",0,38,0,"https://www.semanticscholar.org/paper/2005311276a1a90dc17cf6e2ca7725fee2e76e1e"
"143659fcf93f33e9139f594cf8111c6bc85c04fa",1,"Overview of the EvaLatin 2022 Evaluation Campaign","This paper describes the organization and the results of the second edition of EvaLatin, the campaign for the evaluation of Natural Language Processing tools for Latin, and the three shared tasks proposed in EvaLatin 2022, i.","LT4HALA",2022,"R. Sprugnoli,Marco Passarotti,F. M. Cecchini,Margherita Fantoli,Giovanni Moretti",3,14,1,"https://www.semanticscholar.org/paper/143659fcf93f33e9139f594cf8111c6bc85c04fa"
"6102fe88a512290b80e83ed2fe17606b166e505a",1,"Transformer-based Part-of-Speech Tagging and Lemmatization for Latin","The paper features a thorough discussion of part-of-speech and lemmatization errors which shows how the system performance may be improved for Classical, Medieval and Neo-Latin texts.","LT4HALA",2022,"Krzysztof Wróbel,Krzysztof Nowak",1,12,0,"https://www.semanticscholar.org/paper/6102fe88a512290b80e83ed2fe17606b166e505a"
"0ffb0b109acddf0067aec252221e0ea04d317ddc",1,"A Framework for Sexism Detection on Social Media via ByT5 and TabNet","A combination of byte-level model ByT5 with tabular modeling via TabNet that has at its core an ability to take into account platform and language aspects of the challenging task of sexism detection is proposed.","IberLEF@SEPLN",2022,"A. Younus,M. A. Qureshi",0,16,0,"https://www.semanticscholar.org/paper/0ffb0b109acddf0067aec252221e0ea04d317ddc"
"e541fb54b8b9f2b4d8253f678a28830cd4f52d86",1,"A Survey of Text Representation Methods and Their Genealogy","This work contributes threefold to this lack of compilation, composition, and systematization by providing a survey of current approaches, arranging them in a genealogy, and conceptualizing a taxonomy of text representation methods to examine and explain the state-of-the-art.","IEEE Access",2022,"Philipp Siebers,Christian Janiesch,Patrick Zschech",0,88,0,"https://www.semanticscholar.org/paper/e541fb54b8b9f2b4d8253f678a28830cd4f52d86"
"9758782feed4b4b9bf0ec18b802462e8023a7f83",1,"AfriTeVA: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models","","DEEPLO",2022,"Odunayo Jude Ogundepo,Akintunde Oladipo,Mofetoluwa Adeyemi,Kelechi Ogueji and Jimmy Lin",1,33,0,"https://www.semanticscholar.org/paper/9758782feed4b4b9bf0ec18b802462e8023a7f83"
"5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9",1,"On the Influence of Tokenizers in NLP Pipelines","This thesis examines the influence of tokenization in NLP pipelines, by analyzing, reproducing, and quantifying claims from the token-free NLP literature, using the example of NER, and concludes that token- free models, like ByT5, offer significant advantages over their tokenizer-based alternatives.","",2022,"",0,49,0,"https://www.semanticscholar.org/paper/5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9"
"7072db6eddb85ecd2c117365d91bd694760f726e",1,"Position Information in Transformers: An Overview","An overview and theoretical comparison of existing methods to incorporate position information into Transformer models is provided and what characteristics of an application should be taken into account when selecting a position encoding is indicated.","Computational Linguistics",2021,"Philipp Dufter,Martin Schmitt,Hinrich Schütze",22,67,1,"https://www.semanticscholar.org/paper/7072db6eddb85ecd2c117365d91bd694760f726e"
"a70fc86508bd0133d5d984a4e777abef1934d76c",1,"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese","BARTpho uses the “large” architecture and the pre-training scheme of the sequence-to-sequence denoising autoencoder BART, thus it is especially suitable for generative NLP tasks and is found to be more effective than mBART on these two tasks.","INTERSPEECH",2021,"Nguyen Luong Tran,Duong Minh Le,Dat Quoc Nguyen",1,46,1,"https://www.semanticscholar.org/paper/a70fc86508bd0133d5d984a4e777abef1934d76c"
"e35357ac461a669fe7e4b877ee1fad0dfda26303",1,"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings","This work pushes the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases, and achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages.","ACL",2021,"Kalpesh Krishna,Deepak Nathani,Xavier García,Bidisha Samanta,P. Talukdar",2,85,0,"https://www.semanticscholar.org/paper/e35357ac461a669fe7e4b877ee1fad0dfda26303"
"e53f455b3300d95738dac117419c88fbde58ac4e",1,"Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics","This manuscript describes a three-stage pipeline that individually performs chemical mention detection, entity normalization and indexing in PubMed full-text articles and proposes rules for identifying the more relevant MeSH codes for each article.","Database J. Biol. Databases Curation",2022,"Tiago Almeida,Rui Antunes,João F Silva,João Rafael Almeida,Sérgio Matos",0,141,0,"https://www.semanticscholar.org/paper/e53f455b3300d95738dac117419c88fbde58ac4e"
"f357b35e068bbfbb8468b48198ab63241713629d",1,"Correcting diacritics and typos with ByT5 transformer model","This work tackles diacritics restoration and typos correction at once by employing the newly-developed universal ByT5 byte-level seq2seq transformer model that requires no language-specific model structures and strongly outperforms classical spell-checking or dictionary-based approaches.","Applied Sciences",2022,"Lukas Stankevicius,M. Lukoševičius,J. Kapočiūtė-Dzikienė,Monika Briediene,Tomas Krilavičius",3,138,0,"https://www.semanticscholar.org/paper/f357b35e068bbfbb8468b48198ab63241713629d"
"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852",1,"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation","The monolingual IT5 models are found to provide the best scale-to-performance ratio across tested models, consistently outperforming their multilingual counterparts and setting a new state-of-the-art for most Italian conditional language generation tasks.","ArXiv",2022,"Gabriele Sarti,M. Nissim",5,40,1,"https://www.semanticscholar.org/paper/19e2f3a1e0c3b8340c14cb83e9ca6878a2598852"
"8a4eec70e3d0c43abf26757252ad6210c9717f6c",1,"Towards Lithuanian grammatical error correction","This work constructs a grammatical error correction model for Lithuanian, the language rich in archaic features, by employing the recent advances in transformer architectures and shares the best trained model, achieving F0.5 = 0.92.","",2022,"Lukas Stankevivcius,Mantas Lukovsevivcius",0,32,0,"https://www.semanticscholar.org/paper/8a4eec70e3d0c43abf26757252ad6210c9717f6c"
"1ed66e048bb025e75aa5ea660545285212e5341f",1,"Scaling Up Models and Data with t5x and seqio","Two software libraries are presented: t5x simplifies the process of building and training large language models at scale while maintaining ease of use, and seqio provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines.","ArXiv",2022,"Adam Roberts,Hyung Won Chung,Anselm Levskaya,Gaurav Mishra,James Bradbury,D. Andor,Sharan Narang,Brian Lester,Colin Gaffney,Afroz Mohiuddin,Curtis Hawthorne,Aitor Lewkowycz,Alexandru Salcianu,Marc van Zee,Jacob Austin,Sebastian Goodman,Livio Baldini Soares,Haitang Hu,Sasha Tsvyashchenko,Aakanksha Chowdhery,Jasmijn Bastings,Jannis Bulian,Xavier García,Jianmo Ni,A. Chen,Kathleen Kenealy,J. Clark,Stephan Lee,Daniel H Garrette,J. Lee-Thorp,Colin Raffel,Noam M. Shazeer,Marvin Ritter,Maarten Bosma,Alexandre Passos,Jeremy B. Maitin-Shepard,Noah Fiedel,Mark Omernick,Brennan Saeta,Ryan Sepassi,A. Spiridonov,Joshua Newlan,Andrea Gesmundo",25,23,2,"https://www.semanticscholar.org/paper/1ed66e048bb025e75aa5ea660545285212e5341f"