{"papers":[{"url":"https://www.semanticscholar.org/paper/6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing","venue":"ArXiv","year":2021,"referenceCount":301,"citationCount":47,"influentialCitationCount":6,"authors":"Katikapalli Subramanyam Kalyan,A. Rajasekharan,S. Sangeetha","id":"6c761cfdb031701072582e434d8f64d436255da6","summary":"This comprehensive survey paper explains various core concepts like pretraining, Pretraining methods, pretraining tasks, embeddings and downstream adaptation methods, presents a new taxonomy of T-PTLMs and gives brief overview of various benchmarks including both intrinsic and extrinsic.","score":4},{"url":"https://www.semanticscholar.org/paper/9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation","venue":"SIGMORPHON","year":2022,"referenceCount":81,"citationCount":5,"influentialCitationCount":0,"authors":"Khuyagbaatar Batsuren,Gábor Bella,Aryaman Arora,Viktor Martinovi'c,Kyle Gorman,Zdenvek vZabokrtsk'y,A. Ganbold,vS'arka Dohnalov'a,Magda vSevvc'ikov'a,Katevrina Pelegrinov'a,Fausto Giunchiglia,Ryan Cotterell,Ekaterina Vylomova","id":"9ccad0208b50042d8378b77700146a98bd22ea3f","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models","venue":"COLING","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"authors":"Jan Jezabek,A. Singh","id":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","summary":"A novel method of retroactively adding resilience to misspellings to transformer-based NLP models without the need for re-training of the original NLP model is proposed, which significantly reduces the cost needed to evaluate a model’s resilience to adversarial attacks.","score":4},{"url":"https://www.semanticscholar.org/paper/8f248b5476666e700390f7f8ff6ca923f97d726c","title":"Advancing protein language models with linguistics: a roadmap for improved interpretability","venue":"ArXiv","year":2022,"referenceCount":137,"citationCount":3,"influentialCitationCount":0,"authors":"Mai Ha Vu,R. Akbar,Philippe A. Robert,B. Swiatczak,V. Greiff,G. K. Sandve,Dag Trygve Tryslew Haug","id":"8f248b5476666e700390f7f8ff6ca923f97d726c","summary":"It is argued that guidance drawn from linguistics, a field specialized in analytical rule extraction from natural language data, can aid with building more interpretable protein LMs that have learned relevant domain-specific rules.","score":3},{"url":"https://www.semanticscholar.org/paper/e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order","venue":"ArXiv","year":2021,"referenceCount":72,"citationCount":9,"influentialCitationCount":1,"authors":"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","summary":"The insensitivity of natural language models to word-order is investigated by quantifying perturbations and analysing their effect on neural models’ performance on language understanding tasks in GLUE benchmark and it is found that neural language models — pretrained and non-pretrained Transformers, LSTMs, and Convolutional architectures — require local ordering more than the global ordering of tokens.","score":3},{"url":"https://www.semanticscholar.org/paper/cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color","venue":"ACL","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"authors":"Sidsel Boldsen,Manex Agirrezabal,Nora Hollenstein","id":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","summary":"This cross-lingual analysis shows that textual character representations correlate strongly with sound representations for languages using an alphabetic script, while shape correlates with featural scripts.","score":3},{"url":"https://www.semanticscholar.org/paper/91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU","venue":"FINDINGS","year":2021,"referenceCount":70,"citationCount":4,"influentialCitationCount":0,"authors":"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","summary":"It is empirically shown that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure.","score":3},{"url":"https://www.semanticscholar.org/paper/9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs","venue":"International Conference on Learning Representations","year":2021,"referenceCount":105,"citationCount":152,"influentialCitationCount":23,"authors":"Andrew Jaegle,Sebastian Borgeaud,Jean-Baptiste Alayrac,Carl Doersch,Catalin Ionescu,David Ding,Skanda Koppula,Andrew Brock,Evan Shelhamer,Olivier J. H'enaff,M. Botvinick,Andrew Zisserman,Oriol Vinyals,João Carreira","id":"9933a5af7895354087baf6c96b64dc8a8973eaed","summary":"The primary focus of this work is generality, rather than speed on images, and Perceiver IO uses comparable FLOPs to attention-based image classiﬁcation models, especially for the more compact conﬂguration B pretrained on JFT.","score":3},{"url":"https://www.semanticscholar.org/paper/231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models","venue":"NAACL-HLT","year":2021,"referenceCount":38,"citationCount":12,"influentialCitationCount":1,"authors":"P. Nawrot,Szymon Tworkowski,Michal Tyrolski,Lukasz Kaiser,Yuhuai Wu,Christian Szegedy,H. Michalewski","id":"231e768f0cd280faa0f725bb353262cb4fed08d1","summary":"Hourglass is a hierarchical Transformer language model that sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efﬁciency on the widely studied enwik8 benchmark.","score":3},{"url":"https://www.semanticscholar.org/paper/c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":69,"citationCount":9,"influentialCitationCount":0,"authors":"Jonas Pfeiffer,Naman Goyal,Xi Victoria Lin,Xian Li,James Cross,Sebastian Riedel,Mikel Artetxe","id":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","summary":"This work introduces language-specific modules of their Cross-lingual Modular models from the start, which allows them to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant.","score":3},{"url":"https://www.semanticscholar.org/paper/110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward","venue":"ArXiv","year":2022,"referenceCount":85,"citationCount":1,"influentialCitationCount":0,"authors":"Alan Ramponi","id":"110250df2a6ca0e0e609eaa800a21c17abeedd77","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/9af10e3d4ad1c6f8a0ff8fac8dec52703faa8e7e","title":"vTTS: visual-text to speech","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":2,"influentialCitationCount":1,"authors":"Yoshifumi Nakano,Takaaki Saeki,Shinnosuke Takamichi,Katsuhito Sudoh,H. Saruwatari","id":"9af10e3d4ad1c6f8a0ff8fac8dec52703faa8e7e","summary":"Experimental results show that visual-text to speech is capable of generating speech with naturalness comparable to or better than a conventional TTS, it can transfer emphasis and emotion attributes in visual text to speech without additional labels and architectures, and it can synthesize more natural and intelligible speech from unseen and rare characters than conventional T TS.","score":2},{"url":"https://www.semanticscholar.org/paper/bc39d16c108057e062ad6f1d0e8154df52cafc6a","title":"CMU’s Machine Translation System for IWSLT 2019","venue":"IWSLT","year":2019,"referenceCount":31,"citationCount":3,"influentialCitationCount":0,"authors":"Tejas Srinivasan,Ramon Sanabria,Florian Metze","id":"bc39d16c108057e062ad6f1d0e8154df52cafc6a","summary":"Block Multitask Learning (BMTL) is presented, a novel NMT architecture that predicts multiple targets of different granularities simulta- neously, removing the need to search for the optimal subword segmentation strategy.","score":2},{"url":"https://www.semanticscholar.org/paper/fec6def294027a2ce9094267ce7b7d57f78daf74","title":"Multitask Learning For Different Subword Segmentations In Neural Machine Translation","venue":"IWSLT","year":2019,"referenceCount":32,"citationCount":3,"influentialCitationCount":0,"authors":"Tejas Srinivasan,Ramon Sanabria,Florian Metze","id":"fec6def294027a2ce9094267ce7b7d57f78daf74","summary":"Block Multitask Learning (BMTL), a novel NMT architecture that predicts multiple targets of different granularities simultaneously, removing the need to search for the optimal segmentation strategy, is presented.","score":2},{"url":"https://www.semanticscholar.org/paper/debb3877b778eeb8689729d37e2b90f9f000d877","title":"Neural Machine Translation with Imbalanced Classes","venue":"ArXiv","year":2020,"referenceCount":32,"citationCount":3,"influentialCitationCount":0,"authors":"Thamme Gowda,Jonathan May","id":"debb3877b778eeb8689729d37e2b90f9f000d877","summary":"This work casts neural machine translation as a classification task in an autoregressive setting and analyzes the limitations of both classification and autoregression components, and investigates the effect of class imbalance on NMT.","score":2},{"url":"https://www.semanticscholar.org/paper/e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model","venue":"Findings","year":2021,"referenceCount":40,"citationCount":11,"influentialCitationCount":0,"authors":"Tatsuya Hiraoka,Sho Takase,Kei Uchiumi,Atsushi Keyaki,Naoaki Okazaki","id":"e6b252ad22486c10b1b288e0a5e1ad468690be70","summary":"Experimental results show that the proposed method improves the performance by determining appropriate tokenizations and can be used to explore the appropriate tokenization for an already trained model as post-processing.","score":2},{"url":"https://www.semanticscholar.org/paper/8a1c54f6e2c5f1453fddb9f15e769a099286f677","title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey","venue":"Journal of Artificial Intelligence Research","year":2021,"referenceCount":366,"citationCount":19,"influentialCitationCount":0,"authors":"Danielle Saunders","id":"8a1c54f6e2c5f1453fddb9f15e769a099286f677","summary":"This work surveys approaches to domain adaptation for NMT, particularly where a system may need to translate across multiple domains, and divides techniques into those revolving around data selection or generation, model architecture, parameter adaptation procedure, and inference procedure.","score":2},{"url":"https://www.semanticscholar.org/paper/3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2","title":"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class","venue":"ACM Transactions on Software Engineering and Methodology","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"authors":"Luke Dramko,Jeremy Lacomis,Pengcheng Yin,Edward J. Schwartz,Miltiadis Allamanis,Graham Neubig,Bogdan Vasilescu,Claire Le Goues","id":"3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2","summary":"This work investigates how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains, and evaluates DIRE’s overall performance without respect to data quality.","score":2},{"url":"https://www.semanticscholar.org/paper/58403c6bc061670f7ce6267a3a0cd01ba1bb8e50","title":"Intérêt des modèles de caractères pour la détection d’événements (The interest of character-level models for event detection)","venue":"JEPTALNRECITAL","year":2021,"referenceCount":31,"citationCount":1,"influentialCitationCount":0,"authors":"Emanuela Boros,Romaric Besançon,Olivier Ferret,B. Grau","id":"58403c6bc061670f7ce6267a3a0cd01ba1bb8e50","summary":"D’intégrer des plongements de caractères, qui peuvent capturer des informations morphologiques et de forme sur les mots, à un modèle convolutif pour la détection d’événements, évaluons deux stratégies pour réaliser une telle intégration.","score":2},{"url":"https://www.semanticscholar.org/paper/2ec281d654f622da7267d7da8145e96bb77a9ede","title":"An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification","venue":"Computers & security","year":2021,"referenceCount":61,"citationCount":1,"influentialCitationCount":0,"authors":"Ferhat Demirkiran,Aykut Çayir,U. Ünal,Hasan Dag","id":"2ec281d654f622da7267d7da8145e96bb77a9ede","summary":"The experiments demonstrate that the transformer model with one transformer block layer surpass the performance of the widely used base architecture, LSTM, and BERT or CANINE, the pre-trained transformer models, outperforms in classifying highly imbalanced malware families according to evaluation metrics: F1-score and AUC score.","score":2},{"url":"https://www.semanticscholar.org/paper/e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7","title":"Checks and Strategies for Enabling Code-Switched Machine Translation","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"authors":"Thamme Gowda,Mozhdeh Gheini,Jonathan May","id":"e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7","summary":"This work explores multilingual NMT models’ ability to handle code-switched text, and proposes checks to measure switching capability and investigates simple and effective data augmentation methods that can enhance an NMT model’s ability to support code- Switched text.","score":2},{"url":"https://www.semanticscholar.org/paper/31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning","venue":"Journal of Big Data","year":2021,"referenceCount":134,"citationCount":55,"influentialCitationCount":1,"authors":"Connor Shorten,T. Khoshgoftaar,B. Furht","id":"31852f9fc732c0868af12d631c72693702d80521","summary":"The major motifs of Data Augmentation are summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form.","score":2},{"url":"https://www.semanticscholar.org/paper/7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models","venue":"International Conference on Agents and Artificial Intelligence","year":2022,"referenceCount":51,"citationCount":1,"influentialCitationCount":0,"authors":"Konstantin Todorov,Giovanni Colavizza","id":"7e3081b0d698f8abf16dee626d782f3339482fe7","summary":"It is found that OCR noise poses a significant obstacle to language modelling, with language models increasingly diverging from their noiseless targets as OCR quality lowers, and simpler models including PPMI and Word2Vec consistently outperform transformer-based models in this respect.","score":2},{"url":"https://www.semanticscholar.org/paper/265ebfc1074c73917233dbecad802c0c64921a1c","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"authors":"Gregor Geigle,Chen Liu,Jonas Pfeiffer,Iryna Gurevych","id":"265ebfc1074c73917233dbecad802c0c64921a1c","summary":"This work evaluates whether the information stored within different VEs is complementary, i.e. if providing the model with features from multiple VEs can improve the performance on a target task, and suggests that diverse VEs complement each other, resulting in improved downstream V+L task performance.","score":2},{"url":"https://www.semanticscholar.org/paper/b162638cd42b65e6add199b0b34f1b375070fc7c","title":"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models","venue":"AACL/IJCNLP","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"authors":"Syrielle Montariol,Arij Riabi,Djamé Seddah","id":"b162638cd42b65e6add199b0b34f1b375070fc7c","summary":"It is shown how hate speech detection models benefit from a cross-lingual knowledge proxy brought by auxiliary tasks fine-tuning and highlight these tasks’ positive impact on bridging the hate speech linguistic and cultural gap between languages.","score":2},{"url":"https://www.semanticscholar.org/paper/b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data","venue":"ACL","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"authors":"Colin Leong,Daniel Whitenack","id":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","summary":"Initial experiments using Swahili and Kinyarwanda data suggest the viability of the multi-modal approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6% F1-score above models that are trained from scratch.","score":2},{"url":"https://www.semanticscholar.org/paper/66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer","venue":"International Conference on Learning Representations","year":2021,"referenceCount":87,"citationCount":31,"influentialCitationCount":3,"authors":"M. Dehghani,Anurag Arnab,L. Beyer,Ashish Vaswani,Yi Tay","id":"66d735987a31d666a6459566ae026c40ab9a1c3a","summary":"It is demonstrated how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models, and suggestions to improve reporting of efficiency metrics are presented.","score":2},{"url":"https://www.semanticscholar.org/paper/8f2bca9d684005675e294b33c26481e36f528cdb","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language","venue":"International Conference on Machine Learning","year":2022,"referenceCount":87,"citationCount":177,"influentialCitationCount":35,"authors":"Alexei Baevski,Wei-Ning Hsu,Qiantong Xu,Arun Babu,Jiatao Gu,Michael Auli","id":"8f2bca9d684005675e294b33c26481e36f528cdb","summary":"Data2vec is a framework that uses the same learning method for either speech, NLP or computer vision to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture.","score":2},{"url":"https://www.semanticscholar.org/paper/7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers","venue":"KDD","year":2022,"referenceCount":37,"citationCount":5,"influentialCitationCount":1,"authors":"Alyssa Lees,V. Tran,Yi Tay,Jeffrey Scott Sorensen,Jai Gupta,Donald Metzler,Lucy Vasserman","id":"7016eb4f34611f97fe8c99176246e314678e03f4","summary":"This paper presents the fundamentals behind the next version of the Perspective API from Google Jigsaw, and presents a single multilingual token-free Charformer model that is applicable across a range of languages, domains, and tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia","venue":"ACL","year":2022,"referenceCount":204,"citationCount":4,"influentialCitationCount":0,"authors":"A. F. Aji,Genta Indra Winata,Fajri Koto,Samuel Cahyawijaya,A. Romadhony,Rahmad Mahendra,Kemal Kurniawan,David Moeljadi,Radityo Eko Prasojo,Timothy Baldwin,Jey Han Lau,Sebastian Ruder","id":"a747e8f2659df479c0092301b9658fc582423df1","summary":"An overview of the current state of NLP research for Indonesia's 700+ languages is provided and general recommendations are provided to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.","score":2},{"url":"https://www.semanticscholar.org/paper/cdc71d86bb0b822d73a8359338db0492a2c89b89","title":"Gated Character-aware Convolutional Neural Network for Effective Automated Essay Scoring","venue":"WI/IAT","year":2021,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"authors":"Huanyu Bai,Zhilin Huang,Anran Hao,S. C. Hui","id":"cdc71d86bb0b822d73a8359338db0492a2c89b89","summary":"The experimental results show that the proposed GCCNN model outperforms the baseline deep learning models and the qualitative analysis demonstrates the importance of character-level information for tackling the out-of-vocabulary problem in grading essays.","score":1},{"url":"https://www.semanticscholar.org/paper/b42e3a759348f27cca2f918a6bd0b139a5312e44","title":"A Survey of Pretrained Language Models Based Text Generation","venue":"ArXiv","year":2022,"referenceCount":237,"citationCount":10,"influentialCitationCount":0,"authors":"Junyi Li,Tianyi Tang,Wayne Xin Zhao,J. Nie,Ji-rong Wen","id":"b42e3a759348f27cca2f918a6bd0b139a5312e44","summary":"This survey presents the recent advances achieved in the topic of PLMs for text generation and introduces three key points of applying PLMs to text generation: how to encode the input data as representations preserving input semantics which can be fused into PLMs.","score":1},{"url":"https://www.semanticscholar.org/paper/0826af7c0f221878a142dd1681a63adb3f7e4569","title":"RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining","venue":"ACL","year":2022,"referenceCount":50,"citationCount":2,"influentialCitationCount":0,"authors":"Hui Su,Weiwei Shi,Xiaoyu Shen,Zhou Xiao,Tuo Ji,Jiarui Fang,Jie Zhou","id":"0826af7c0f221878a142dd1681a63adb3f7e4569","summary":"RoCBert is a pretrained Chinese Bert that is robust to various forms of adversarial attacks like word perturbation, synonyms, typos, etc, and is pretrained with the contrastive learning objective which maximizes the label consistency under different synthesized adversarial examples.","score":1},{"url":"https://www.semanticscholar.org/paper/04bc7d5ac048133a1aaead7ab1e8021b055359c4","title":"Design principles of an open-source language modeling microservice package for AAC text-entry applications","venue":"SLPAT","year":2022,"referenceCount":78,"citationCount":0,"influentialCitationCount":0,"authors":"Brian Roark,Alexander Gutkin","id":"04bc7d5ac048133a1aaead7ab1e8021b055359c4","summary":"MzoLM, an open-source language model microservice package intended for use in AAC text-entry applications, is presented, with a particular focus on the design principles of the library.","score":1},{"url":"https://www.semanticscholar.org/paper/d5c2119aec38b8e8e154ec2bf0106dfac1ecc80f","title":"Building Static Embeddings from Contextual Ones: Is It Useful for Building Distributional Thesauri?","venue":"LREC","year":2022,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"authors":"Olivier Ferret","id":"d5c2119aec38b8e8e154ec2bf0106dfac1ecc80f","summary":"This article proposes a new method for building word or type-level embeddings from contextual models that combines the generalization and the aggregation of token representations and evaluates it for a large set of English nouns from the perspective of the building of distributional thesauri for extracting semantic similarity relations.","score":1},{"url":"https://www.semanticscholar.org/paper/abb47fb115a420fc363ed3acbd8abb1a37f0dcd5","title":"DeepREF: A Framework for Optimized Deep Learning-based Relation Classification","venue":"LREC","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"authors":"Igor Nascimento,Rinaldo Lima,Adrian-Gabriel Chifu,B. Espinasse,S. Fournier","id":"abb47fb115a420fc363ed3acbd8abb1a37f0dcd5","summary":"A new open and optimizable framework, called DeepREF, is proposed, which is inspired by the OpenNRE and REflex existing frameworks, and allows the employment of various deep learning models, to optimize their use, to identify the best inputs and to get better results with each data set for RE.","score":1},{"url":"https://www.semanticscholar.org/paper/edc17c862b074fb986f9b9a144f10decb843f5fa","title":"Décontextualiser des plongements contextuels pour construire des thésaurus distributionnels (Decontextualizing contextual embeddings for building distributional thesauri )","venue":"JEPTALNRECITAL","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"authors":"Olivier Ferret","id":"edc17c862b074fb986f9b9a144f10decb843f5fa","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c","title":"AMMU - A Survey of Transformer-based Biomedical Pretrained Language Models","venue":"Journal of Biomedical Informatics","year":2021,"referenceCount":227,"citationCount":16,"influentialCitationCount":0,"authors":"Katikapalli Subramanyam Kalyan,A. Rajasekharan,S. Sangeetha","id":"420c897bc67e6f438db522d919d925df1a10aa8c","summary":"This survey discusses core concepts of transformer-based PLMs like pretraining methods, pretraining tasks, fine-tuning methods, and various embedding types specific to biomedical domain, and introduces a taxonomy for transformerbased BPLMs.","score":1},{"url":"https://www.semanticscholar.org/paper/aeeb168feb05da1b3d31c0389e7a9196bc8970f6","title":"DravidianCodeMix: sentiment analysis and offensive language identification dataset for Dravidian languages in code-mixed text","venue":"Language Resources and Evaluation","year":2021,"referenceCount":91,"citationCount":29,"influentialCitationCount":0,"authors":"Bharathi Raja Chakravarthi,R. Priyadharshini,V. Muralidaran,Navya Jose,Shardul Suryawanshi,E. Sherly,John P. McCrae","id":"aeeb168feb05da1b3d31c0389e7a9196bc8970f6","summary":"A multilingual, manually annotated dataset for three under-resourced Dravidian languages generated from social media comments, which contains all types of code-mixing phenomena since it comprises user-generated content from a multilingual country.","score":1},{"url":"https://www.semanticscholar.org/paper/ec72909b75b2389efd588aa38d9c664e654d90d3","title":"Automatic Classification of Cancer Pathology Reports: A Systematic Review","venue":"Journal of pathology informatics","year":2022,"referenceCount":66,"citationCount":3,"influentialCitationCount":0,"authors":"Thiago Santos,Amara Tariq,J. Gichoya,H. Trivedi,I. Banerjee","id":"ec72909b75b2389efd588aa38d9c664e654d90d3","summary":"This systematic review identifies the NLP systems for classifying pathology reports published between the years of 2010 and 2021 and benchmarked the systems based on methodology, complexity of the prediction task and core types of NLP models.","score":1},{"url":"https://www.semanticscholar.org/paper/e923c415140a095711857420976b41e7a07cfe9c","title":"Towards improving the robustness of sequential labeling models against typographical adversarial examples using triplet loss","venue":"Natural Language Engineering","year":2022,"referenceCount":11,"citationCount":0,"influentialCitationCount":0,"authors":"Can Udomcharoenchaikit,P. Boonkwan,P. Vateekul","id":"e923c415140a095711857420976b41e7a07cfe9c","summary":"Experiments show that the proposed adversarial training framework provides better resistance against adversarial examples on all tasks, and can further improve the model’s robustness on the chunking task by including a triplet loss constraint.","score":1},{"url":"https://www.semanticscholar.org/paper/1b012686cf42baed47c11f0454b20a4d820a1db1","title":"The EMory BrEast imaging Dataset (EMBED): A Racially Diverse, Granular Dataset of 3.5M Screening and Diagnostic Mammograms","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"authors":"J. Jeong,B. Vey,A. Bhimireddy,Thomas Kim,Thiago Santos,R. Correa,Raman Dutt,M. Mosunjac,G. Oprea-Ilies,Geoffrey Smith,Min-Jae Woo,Christopher R. McAdams,M. Newell,I. Banerjee,J. Gichoya,H. Trivedi","id":"1b012686cf42baed47c11f0454b20a4d820a1db1","summary":"The EMory BrEast imaging Dataset (EMBED) addresses gaps by providing 3650,000 2D and DBT screening and diagnostic mammograms for 116,000 women divided equally between White and African American patients.","score":1},{"url":"https://www.semanticscholar.org/paper/7746ac2e1e84c2f1d82a4593e2cdfdd4c93a04d8","title":"An open-source natural language processing toolkit to support software development: addressing automatic bug detection, code summarisation and code search","venue":"Open Research Europe","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"authors":"Cristian Robledo,Francesca Sallicati,G. de Chalendar,Marcos Fernández,Pablo de Castro,Eduardo Martín,Javier Gutiérrez,Yannis Bouachera","id":"7746ac2e1e84c2f1d82a4593e2cdfdd4c93a04d8","summary":"The paper focuses on the NLP tools developed and integrated in the Persistent Knowledge Monitor, namely the deep learning models developed to perform variable misuse, code summarisation and semantic parsing.","score":1},{"url":"https://www.semanticscholar.org/paper/e4645f7d08b5bc878bd38c3db19c3b1a9978bd43","title":"Imputing Out-of-Vocabulary Embeddings with LOVE Makes LanguageModels Robust with Little Cost","venue":"ACL","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"authors":"Lihu Chen,G. Varoquaux,Fabian M. Suchanek","id":"e4645f7d08b5bc878bd38c3db19c3b1a9978bd43","summary":"A simple contrastive learning framework, LOVE, which extends the word representation of an existing pre-trained language model and makes it robust to OOV with few additional parameters, and can be used in a plug-and-play fashion with FastText and BERT, where it significantly improves their robustness.","score":1},{"url":"https://www.semanticscholar.org/paper/16f0acab166e8ac756927fc8f2e8f68be641605b","title":"Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models","venue":"ACL","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"authors":"Mark Chu,Bhargav Srinivasa Desikan,E. Nadler,Ruggerio L. Sardo,Elise Darragh-Ford,Douglas Guilbeault","id":"16f0acab166e8ac756927fc8f2e8f68be641605b","summary":"It is proposed that n-grams composed of random character sequences, or garble, provide a novel context for studying word meaning both within and beyond extant language, and it is shown that meaning and primitive information are intrinsically linked.","score":1},{"url":"https://www.semanticscholar.org/paper/e68d1aa5e78226bb868a43842a87a5532b813b0b","title":"CancerBERT: a cancer domain-specific language model for extracting breast cancer phenotypes from electronic health records","venue":"J. Am. Medical Informatics Assoc.","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"authors":"Sicheng Zhou,Nan Wang,Liwei Wang,Hongfang Liu,Rui Zhang","id":"e68d1aa5e78226bb868a43842a87a5532b813b0b","summary":"The results validated that using customized vocabulary may further improve the performances of domain specific BERT models in clinical NLP tasks and developed and evaluated CancerBERT models to extract the cancer phenotypes in clinical notes and pathology reports.","score":1},{"url":"https://www.semanticscholar.org/paper/b3b1659c992cbbd233038522ddd170887c033fe7","title":"Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech","venue":"INTERSPEECH","year":2022,"referenceCount":30,"citationCount":2,"influentialCitationCount":1,"authors":"Guangyan Zhang,Kaitao Song,Xu Tan,Daxin Tan,Yuzi Yan,Yanqing Liu,G. Wang,Wei Zhou,Tao Qin,Tan Lee,Sheng Zhao","id":"b3b1659c992cbbd233038522ddd170887c033fe7","summary":"The proposed Mixed-Phoneme BERT is a novel variant of the BERT model that uses mixed phoneme and sup-phoneme representations to enhance the learning capability and achieves 3 × inference speedup and similar voice quality to the previous TTS pre-trained model PnG BERT.","score":1},{"url":"https://www.semanticscholar.org/paper/bff16ea31d5bc6b25b6e48327a2d7026e92a788d","title":"CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2022,"referenceCount":47,"citationCount":6,"influentialCitationCount":1,"authors":"Shengyao Zhuang,G. Zuccon","id":"bff16ea31d5bc6b25b6e48327a2d7026e92a788d","summary":"This paper shows that a small character level perturbation in queries (as caused by typos) highly impacts the effectiveness of dense retrievers, and develops dense retriever methods that are robust to such queries with typos, while still being as performant as previous methods on queries without typos.","score":1},{"url":"https://www.semanticscholar.org/paper/6058ce3819d72c3e429bea58d78d80c719cb4bdb","title":"Data Augmentation for Biomedical Factoid Question Answering","venue":"BIONLP","year":2022,"referenceCount":87,"citationCount":0,"influentialCitationCount":0,"authors":"Dimitris Pappas,Prodromos Malakasiotis,Ion Androutsopoulos","id":"6058ce3819d72c3e429bea58d78d80c719cb4bdb","summary":"It is shown that DA can lead to very significant performance gains, even when using large pre-trained Transformers, contributing to a broader discussion of if/when DA benefits large pre -trained models.","score":1},{"url":"https://www.semanticscholar.org/paper/9727fa4acdc5312ff86745875ef3db8578f153ac","title":"Decorate the Examples: A Simple Method of Prompt Design for Biomedical Relation Extraction","venue":"LREC","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"authors":"Hui-Syuan Yeh,T. Lavergne,Pierre Zweigenbaum","id":"9727fa4acdc5312ff86745875ef3db8578f153ac","summary":"This paper presents a simple yet effective method to systematically generate comprehensive prompts that reformulate the relation extraction task as a cloze-test task under a simple prompt formulation, and finds prompt-based learning requires fewer training examples to make reasonable predictions.","score":1},{"url":"https://www.semanticscholar.org/paper/91141410b0bd141bc3b1ac0c190c3867181d699c","title":"Multi-task learning in under-resourced Dravidian languages","venue":"Journal of Data, Information and Management","year":2022,"referenceCount":27,"citationCount":2,"influentialCitationCount":0,"authors":"Adeep Hande,Siddhanth U Hegde,Bharathi Raja Chakravarthi","id":"91141410b0bd141bc3b1ac0c190c3867181d699c","summary":"Analysis of fine-tuned models indicates the preference of multi-task learning over single task learning resulting in a higher weighted F1 score on all three languages, and this framework is applicable to other sequence classification problems irrespective to the size of the datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/f1d8dbe4e217d2221c5276f1e0c616ba5f82d1d3","title":"A review on Natural Language Processing Models for COVID-19 research","venue":"Healthcare Analytics","year":2022,"referenceCount":104,"citationCount":1,"influentialCitationCount":0,"authors":"Karl Hall,Victor Chang,Chrisina Jayne","id":"f1d8dbe4e217d2221c5276f1e0c616ba5f82d1d3","summary":"A range of transformer-based biomedical pretrained language models are evaluated using the BLURB benchmark and the novel T-BPLM BioLinkBERT gives groundbreaking results by incorporating document link knowledge and hyperlinking into its pretraining.","score":1},{"url":"https://www.semanticscholar.org/paper/305062aa036ffeafdd6300303898dff8ed59a7dc","title":"Cross-lingual Approaches for the Detection of Adverse Drug Reactions in German from a Patient’s Perspective","venue":"LREC","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"authors":"Lisa Raithel,Philippe E. Thomas,Roland Roller,Oliver Sapina,Sebastian Moller,Pierre Zweigenbaum","id":"305062aa036ffeafdd6300303898dff8ed59a7dc","summary":"This work presents the first corpus for German Adverse Drug Reaction (ADR) detection in patient-generated content, consisting of 4,169 binary annotated documents from a German patient forum, and provides preliminary experiments for binary classification using different methods of zero- and few-shot learning based on a multi-lingual model.","score":1},{"url":"https://www.semanticscholar.org/paper/c75b1fb2348c0f54259319b3595ececaf1d98430","title":"Medical terminology-based computing system: a lightweight post-processing solution for out-of-vocabulary multi-word terms","venue":"Frontiers in Molecular Biosciences","year":2022,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"authors":"Nadia Saeed ,Hammad Naveed","id":"c75b1fb2348c0f54259319b3595ececaf1d98430","summary":"This study presents MedTCS—a lightweight, post-processing module—to simplify hybridized or compound terms into regular words using medical nomenclature and demonstrates that the proposed module enables the word embedding models to generate vectors of out-of-vocabulary words effectively.","score":1},{"url":"https://www.semanticscholar.org/paper/7c206d541552a57b4c0cfe5b5113d3cc8e31df01","title":"IMSE: interaction information attention and molecular structure based drug drug interaction extraction","venue":"BMC Bioinform.","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"authors":"Biao Duan,Jing Peng,Yi Zhang","id":"7c206d541552a57b4c0cfe5b5113d3cc8e31df01","summary":"A model that leverages state of the art transformer architecture in conjunction with multiple features can bolster the performances of drug drug interation tasks in the biomedical domain and would be helpful in identification of potential adverse drug reactions.","score":1},{"url":"https://www.semanticscholar.org/paper/8f541cf5b193654d262dbdaea2b8a0d7d913e8dc","title":"Survey of NLP in Pharmacology: Methodology, Tasks, Resources, Knowledge, and Tools","venue":"ArXiv","year":2022,"referenceCount":245,"citationCount":0,"influentialCitationCount":0,"authors":"D. Trajanov,Vangel Trajkovski,Makedonka Dimitrieva,Jovana Dobreva,Milos Jovanovik,Matej Klemen,Alevs vZagar,Marko Robnik-vSikonja","id":"8f541cf5b193654d262dbdaea2b8a0d7d913e8dc","summary":"This work splits the coverage into categories to survey modern NLP methodology, commonly addressed tasks, relevant textual data, knowledge bases, and useful programming libraries, and presents a comprehensive overview of the area.","score":1},{"url":"https://www.semanticscholar.org/paper/8297a3b52eea4e8c3ab8e97910839b4a3b917891","title":"On the State of the Art in Authorship Attribution and Authorship Verification","venue":"ArXiv","year":2022,"referenceCount":71,"citationCount":2,"influentialCitationCount":1,"authors":"Jacob Tyo,Bhuwan Dhingra,Z. Lipton","id":"8297a3b52eea4e8c3ab8e97910839b4a3b917891","summary":"It is shown that through the application of hard-negative mining, AV methods are competitive alternatives to AA methods, and on the two AA datasets with the greatest number of words per author, BERT-based models perform best.","score":1},{"url":"https://www.semanticscholar.org/paper/1e2a5dca6f310241dd8a9b56873edf8ca211c781","title":"Enriching Biomedical Knowledge for Low-resource Language Through Translation","venue":"bioRxiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"authors":"Long Phan,Tai Dang,Hieu Tran,Vy Phan,Lam D. Chau,Trieu H. Trinh","id":"1e2a5dca6f310241dd8a9b56873edf8ca211c781","summary":"A state-of-theart translation model in English-Vietnamese is made use to translate and produce both pretrained as well as supervised data in the biomedical domains, and ViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20 million translated abstracts from the high-quality public PubMed corpus is introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/247328a082d86199ed5a98e1d726aa205c1da9df","title":"Neural Machine Translation","venue":"ArXiv","year":2017,"referenceCount":425,"citationCount":242,"influentialCitationCount":32,"authors":"Philipp Koehn","id":"247328a082d86199ed5a98e1d726aa205c1da9df","summary":"A comprehensive treatment of the topic, ranging from introduction to neural networks, computation graphs, description of the currently dominant attentional sequence-to-sequence model, recent refinements, alternative architectures and challenges.","score":1},{"url":"https://www.semanticscholar.org/paper/0ab0fda8774c303be8f8f8c8f684a890dcf5d455","title":"Lattice-Based Transformer Encoder for Neural Machine Translation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2019,"referenceCount":49,"citationCount":37,"influentialCitationCount":5,"authors":"Fengshun Xiao,Jiangtong Li,Zhao Hai,Rui Wang,Kehai Chen","id":"0ab0fda8774c303be8f8f8c8f684a890dcf5d455","summary":"This work proposes lattice-based encoders to explore effective word or subword representation in an automatic way during training and proposes two methods: 1) lattice positional encoding and 2) lattICE-aware self-attention to further improve translation performance.","score":1},{"url":"https://www.semanticscholar.org/paper/0aef56962035a79101821480f51897fdc4443945","title":"Character n-gram Embeddings to Improve RNN Language Models","venue":"AAAI","year":2019,"referenceCount":46,"citationCount":16,"influentialCitationCount":1,"authors":"Sho Takase,Jun Suzuki,Masaaki Nagata","id":"0aef56962035a79101821480f51897fdc4443945","summary":"A novel Recurrent Neural Network (RNN) language model that takes advantage of character information based on research in the field of word embedding construction and combines them with ordinary word embeddings is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/9f3f6deeb1f03ebd52f9ab44275ad382fe60d073","title":"Latent Part-of-Speech Sequences for Neural Machine Translation","venue":"EMNLP","year":2019,"referenceCount":48,"citationCount":10,"influentialCitationCount":2,"authors":"Xuewen Yang,Yingru Liu,Dongliang Xie,Xin Wang,Niranjan Balasubramanian","id":"9f3f6deeb1f03ebd52f9ab44275ad382fe60d073","summary":"A new latent variable model, LaSyn, is introduced that captures the co-dependence between syntax and semantics, while allowing for effective and efficient inference over the latent space.","score":1},{"url":"https://www.semanticscholar.org/paper/fac7f9b04fba4889b445e309d384644d15ee7e88","title":"Trends and Advances in Neural Machine Translation","venue":"2020 IEEE International Conference for Innovation in Technology (INOCON)","year":2020,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"authors":"Purva Kulkarni,Pravina Bhalerao,Kuheli Nayek,R. Deolekar","id":"fac7f9b04fba4889b445e309d384644d15ee7e88","summary":"This paper breaks down different models, approaches, inception, principle development, and structures utilized in NMT to discover a productive strategy to make a translation system and identifying the advances and imperfections of the equivalent.","score":1},{"url":"https://www.semanticscholar.org/paper/ca83469977ab49baae02ef1e12f419120927ecdc","title":"Mixed-Level Neural Machine Translation","venue":"Computational Intelligence and Neuroscience","year":2020,"referenceCount":25,"citationCount":7,"influentialCitationCount":0,"authors":"Thien Nguyen,Huu Nguyen,Phuoc Tran","id":"ca83469977ab49baae02ef1e12f419120927ecdc","summary":"A novel heterogeneous translation unit system, considering linguistic characteristics of the synthetic Russian language and the analytic Vietnamese language is proposed, which improves over the existing best homogeneous Russian-Vietnamese translation system by 1.17 BLEU.","score":1},{"url":"https://www.semanticscholar.org/paper/188f913be48a218b44b0bd964662b4926fd0b0b8","title":"Neural Machine Translation: A Review of Methods, Resources, and Tools","venue":"AI Open","year":2020,"referenceCount":169,"citationCount":27,"influentialCitationCount":0,"authors":"Zhixing Tan,Shuo Wang,Zonghan Yang,Gang Chen,Xuancheng Huang,Maosong Sun,Yang Liu","id":"188f913be48a218b44b0bd964662b4926fd0b0b8","summary":"A broad review of the methods for NMT is provided and focus on methods relating to architectures, decoding, and data augmentation, with a discussion of possible future research directions.","score":1},{"url":"https://www.semanticscholar.org/paper/bfbfeb0613beae296d76077b2adc9e38a6b678a4","title":"Sub-Subword N-Gram Features for Subword-Level Neural Machine Translation","venue":"Journal of Natural Language Processing","year":2021,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"authors":"A. Martinez,Katsuhito Sudoh,Yuji Matsumoto","id":"bfbfeb0613beae296d76077b2adc9e38a6b678a4","summary":"A novel approach that combines subword-level segmentation with character-level information in the form of character n-gram features to construct embedding matrices and softmax output projections for a standard encoderdecoder model that increases the vocabulary size for small training datasets without reducing translation quality.","score":1},{"url":"https://www.semanticscholar.org/paper/c68959f4da94fd35da5e8649465177b24d0dd351","title":"Byte-based Multilingual NMT for Endangered Languages","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"authors":"Mengjiao Zhang,Jia Xu","id":"c68959f4da94fd35da5e8649465177b24d0dd351","summary":"This work proposes a byte-based multilingual neural machine translation system (BMNMT), which consistently and significantly outperforms subword/word-based baselines on twelve language pairs up to +18.5 BLEU points, an 840% relative improvement.","score":1},{"url":"https://www.semanticscholar.org/paper/2e6028f8b156c8b344e1a68d15d88403a978c71d","title":"Learning Multiscale Transformer Models for Sequence Generation","venue":"ICML","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"authors":"Bei Li,Tong Zheng,Yi Jing,Chengbo Jiao,Tong Xiao,Jingbo Zhu","id":"2e6028f8b156c8b344e1a68d15d88403a978c71d","summary":"This work built a multiscale Transformer model by establishing relationships among scales based on word-boundary information and phrase-level prior knowledge and yielded consistent performance gains over the strong baseline on several test sets without sacrificing the efficiency.","score":1},{"url":"https://www.semanticscholar.org/paper/2dcdc4ef997507630f2c9b1b6682646ae31bdb7f","title":"SIT at MixMT 2022: Fluent Translation Built on Giant Pre-trained Models","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"authors":"Abdul Rafae Khan,Hrishikesh Kanade,Girish Amar Budhrani,Preet Jhanglani,Jia Xu","id":"2dcdc4ef997507630f2c9b1b6682646ae31bdb7f","summary":"This paper describes the Stevens Institute of Technology’s submission for the WMT 2022 Shared Task: Code-mixed Machine Translation (MixMT), which consisted of two subtasks, subtask 1 Hindi/English to Hinglish and subtask 2 Hinglit to English translation.","score":1},{"url":"https://www.semanticscholar.org/paper/fbd47a815c73a83e8a47ee2ed38826c82ffc0c2a","title":"Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation","venue":"ACL","year":2019,"referenceCount":33,"citationCount":27,"influentialCitationCount":3,"authors":"Elizabeth Salesky,Matthias Sperber,A. Black","id":"fbd47a815c73a83e8a47ee2ed38826c82ffc0c2a","summary":"This work shows that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features.","score":1},{"url":"https://www.semanticscholar.org/paper/95236a88cc959656958d7a49422f4004015d594d","title":"Comparison between NMT and PBSMT Performance for Translating Noisy User-Generated Content","venue":"NODALIDA","year":2019,"referenceCount":46,"citationCount":9,"influentialCitationCount":1,"authors":"José Carlos Rosales Núñez,Djamé Seddah,Guillaume Wisniewski","id":"95236a88cc959656958d7a49422f4004015d594d","summary":"It is shown that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs, and suggests new avenue for improving NMT models.","score":1},{"url":"https://www.semanticscholar.org/paper/201fae97e51fb6aea7ed8120147e806e43834de6","title":"Neural Machine Translation: A Review and Survey","venue":"","year":2019,"referenceCount":537,"citationCount":10,"influentialCitationCount":1,"authors":"Felix Stahlberg","id":"201fae97e51fb6aea7ed8120147e806e43834de6","summary":"This work traces back the origins of modern NMT architectures to word and sentence embeddings and earlier examples of the encoder-decoder network family and concludes with a survey of recent trends in the field.","score":1},{"url":"https://www.semanticscholar.org/paper/4d08dcd2cc1e9691defe664a10f021424a896a1e","title":"Neural Machine Translation: A Review","venue":"Journal of Artificial Intelligence Research","year":2019,"referenceCount":641,"citationCount":64,"influentialCitationCount":3,"authors":"Felix Stahlberg","id":"4d08dcd2cc1e9691defe664a10f021424a896a1e","summary":"This work traces back the origins of modern NMT architectures to word and sentence embeddings and earlier examples of the encoder-decoder network family and concludes with a survey of recent trends in the field.","score":1},{"url":"https://www.semanticscholar.org/paper/8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","title":"Adversarial Subword Regularization for Robust Neural Machine Translation","venue":"FINDINGS","year":2020,"referenceCount":47,"citationCount":3,"influentialCitationCount":0,"authors":"Jungsoon Park,Mujeen Sung,Jinhyuk Lee,Jaewoo Kang","id":"8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","summary":"This paper presents adversarial subword regularization (ADVSR) to study whether gradient signals during training can be a substitute criterion for exposing diverse subword segmentations and experimentally shows that the model-based adversarial samples effectively encourage NMT models to be less sensitive to segmentation errors and improve the performance of N MT models in low-resource and out-domain datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/405cd5cd6e056675d4545eba12742174cc75d7e7","title":"Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation","venue":"Machine Translation","year":2020,"referenceCount":114,"citationCount":4,"influentialCitationCount":0,"authors":"Stig-Arne Grönroos,Sami Virpioja,M. Kurimo","id":"405cd5cd6e056675d4545eba12742174cc75d7e7","summary":"These approaches for improving neural machine translation for low-resource languages are reviewed in the context of an asymmetric-resource one-to-many translation task, in which the pair of target languages are related, with one being a very low- resource and the other a higher-resource language.","score":1},{"url":"https://www.semanticscholar.org/paper/38b40ae531ddca434de07015637b78413370d15a","title":"The Roles of Language Models and Hierarchical Models in Neural Sequence-to-Sequence Prediction","venue":"EAMT","year":2020,"referenceCount":767,"citationCount":3,"influentialCitationCount":0,"authors":"Felix Stahlberg","id":"38b40ae531ddca434de07015637b78413370d15a","summary":"It is shown how traditional symbolic statistical machine translation models can still improve neural machine translation while reducing the risk of common pathologies of NMT such as hallucinations and neologisms.","score":1},{"url":"https://www.semanticscholar.org/paper/8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task","venue":"FINDINGS","year":2020,"referenceCount":38,"citationCount":8,"influentialCitationCount":1,"authors":"Tatsuya Hiraoka,Sho Takase,Kei Uchiumi,Atsushi Keyaki,Naoaki Okazaki","id":"8bcde747a44cbc2601175301808fe4518c9128dc","summary":"The proposed method, optimizing tokenization (OpTok), is trained to assign a high probability to such appropriate tokenization based on the downstream task loss, and can be used for any downstream task which uses a vector representation of a sentence such as text classification.","score":1},{"url":"https://www.semanticscholar.org/paper/6dc710b46bc510a42af487a3ac220e4fabf4d518","title":"VOLT: Improving Vocabularization via Optimal Transport for Machine Translation","venue":"ArXiv","year":2020,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"authors":"Jingjing Xu,Hao Zhou,Chun Gan,Zaixiang Zheng,Lei Li","id":"6dc710b46bc510a42af487a3ac220e4fabf4d518","summary":"An exciting relation between an information-theoretic feature and BLEU scores is found and it is found that VOLT beats widely-used vocabularies on diverse scenarios, and one advantage of VOLT lies in its low resource consumption.","score":1},{"url":"https://www.semanticscholar.org/paper/b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":32,"citationCount":32,"influentialCitationCount":3,"authors":"Jingjing Xu,Hao Zhou,Chun Gan,Zaixiang Zheng,Lei Li","id":"b086b812c867b1d07eb65bcdd206dd0891733f9d","summary":"This paper proposes VOLT, a simple and efficient solution without trial training that beats widely-used vocabularies in diverse scenarios, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation.","score":1},{"url":"https://www.semanticscholar.org/paper/24fd022750ea88e44e6790c7c7e1923635885c71","title":"Translation Mechanism of Neural Machine Algorithm for Online English Resources","venue":"Complex.","year":2021,"referenceCount":25,"citationCount":2,"influentialCitationCount":0,"authors":"Yanping Ye","id":"24fd022750ea88e44e6790c7c7e1923635885c71","summary":"This paper proposes a framework that integrates vocabulary alignment structure for neural machine translation at the vocabulary level and uses the word alignment structure of statistical machine translation as the external vocabulary alignment information and introduces it into the decoding step of Neural machine translation.","score":1},{"url":"https://www.semanticscholar.org/paper/9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations","venue":"EMNLP","year":2021,"referenceCount":42,"citationCount":3,"influentialCitationCount":0,"authors":"Kris Cao,Laura Rimell","id":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","summary":"It is argued that language models should be evaluated on their marginal likelihood over tokenisations, and it is shown that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data.","score":1},{"url":"https://www.semanticscholar.org/paper/b62a56f629f77acce9ea69456d67dd77de1c7dfb","title":"Clustering Monolingual Vocabularies to Improve Cross-Lingual Generalization","venue":"MRL","year":2021,"referenceCount":79,"citationCount":1,"influentialCitationCount":0,"authors":"R. Bassani,Anders Søgaard,Tejaswini Deoskar","id":"b62a56f629f77acce9ea69456d67dd77de1c7dfb","summary":"This work explores the idea of learning multilingual language models based on clustering of monolingual segments and shows significant improvements over standard multilingual segmentation and training across nine languages on a question answering task, both in a small model regime and for a model of the size of BERT-base.","score":1},{"url":"https://www.semanticscholar.org/paper/0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":102,"citationCount":68,"influentialCitationCount":9,"authors":"Phillip Rust,Jonas Pfeiffer,Ivan Vulic,Sebastian Ruder,Iryna Gurevych","id":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","summary":"It is found that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.","score":1},{"url":"https://www.semanticscholar.org/paper/dca4128a33ca22c02031b5c0c28548a0df022d80","title":"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding","venue":"","year":2021,"referenceCount":56,"citationCount":6,"influentialCitationCount":0,"authors":"Abhik Bhattacharjee,Tahmid Hasan,Kazi Samin,Md. Saiful Islam,M. S. Rahman,Anindya Iqbal,Rifat Shahriyar","id":"dca4128a33ca22c02031b5c0c28548a0df022d80","summary":"The Embedding Barrier is introduced, a phenomenon that limits the monolingual performance of multilingual models on low-resource languages having unique typologies and a straightforward solution by transcribing languages to a common script is proposed, which can effectively improve the performance of a multilingual model for the Bangla language.","score":1},{"url":"https://www.semanticscholar.org/paper/2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":62,"citationCount":67,"influentialCitationCount":18,"authors":"Sebastian Ruder,Noah Constant,Jan A. Botha,Aditya Siddhant,Orhan Firat,Jinlan Fu,Pengfei Liu,Junjie Hu,Graham Neubig,Melvin Johnson","id":"2b9762e91305986ac8a2d624d0a69521304405f3","summary":"This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned, and provides a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.","score":1},{"url":"https://www.semanticscholar.org/paper/a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study","venue":"MRL","year":2021,"referenceCount":72,"citationCount":9,"influentialCitationCount":0,"authors":"Ethan C. Chau,Noah A. Smith","id":"a20a802839d72bee1c85f4a1cb77addadacb2179","summary":"These evaluations on part-of-speech tagging, universal dependency parsing, and named entity recognition in nine diverse low-resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low- resource settings.","score":1},{"url":"https://www.semanticscholar.org/paper/3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5","title":"Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":62,"citationCount":9,"influentialCitationCount":2,"authors":"M. Nicosia,Zhongdi Qu,Y. Altun","id":"3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5","summary":"Experimental results on three multilingual semantic parsing datasets show that data augmentation with TaF reaches accuracies competitive with similar systems which rely on traditional alignment techniques.","score":1},{"url":"https://www.semanticscholar.org/paper/e43a360728e34e43934707665db0f7be02c8e5a7","title":"Interpreting BERT-based stance classification: a case study about the Brazilian COVID vaccination","venue":"SBBD","year":2021,"referenceCount":16,"citationCount":2,"influentialCitationCount":0,"authors":"Carlos Abel Córdova Sáenz,Karin Becker","id":"e43a360728e34e43934707665db0f7be02c8e5a7","summary":"This paper proposes a BERT-based stance classification model and an attention-based mechanism to identify the influential words for stance classification and uses these metrics to assess if words with high attention weights correspond to domain intrinsic properties and contribute to the correct classification of stances.","score":1},{"url":"https://www.semanticscholar.org/paper/12fec03c538c7aaeca1a4e1ab8d66aed5f793fc0","title":"reamtchka at SemEval-2022 Task 6: Investigating the effect of different loss functions for Sarcasm detection for unbalanced datasets","venue":"International Workshop on Semantic Evaluation","year":2022,"referenceCount":36,"citationCount":3,"influentialCitationCount":0,"authors":"Reem Abdel-Salam","id":"12fec03c538c7aaeca1a4e1ab8d66aed5f793fc0","summary":"A voting classifier between either multiple different BERT-based models or machine learning models is proposed, as the final model in SemEval-2022 Task 6: Intended Sarcasm Detection in English and Arabic.","score":1},{"url":"https://www.semanticscholar.org/paper/b1be10b76314ea8569249f2310f1e6eead8a5adc","title":"Building Domain-specific Corpora from the Web: the Case of European Digital Service Infrastructures","venue":"BUCC","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"authors":"Rik van Noord,Cristian García-Romero,M. Esplà-Gomis,Leopoldo Pla Sempere,Antonio Toral","id":"b1be10b76314ea8569249f2310f1e6eead8a5adc","summary":"This paper explores the feasibility of building an automatic classifier that allows to identify which segments in a generic corpus are relevant for a particular DSI, and uses pre-trained (multilingual) language models to perform the classification.","score":1},{"url":"https://www.semanticscholar.org/paper/e11d8663ccf2cc412f408853fa5f19ebac75df54","title":"How to encode arbitrarily complex morphology in word embeddings, no corpus needed","venue":"FIELDMATTERS","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"authors":"Lane Schwartz,Coleman Haley,Francis M. Tyers","id":"e11d8663ccf2cc412f408853fa5f19ebac75df54","summary":"The word embeddings in this paper are explicitly designed to be both linguistically interpretable and fully capable of handling the broad variety found in the world’s diverse set of 7000 languages, regardless of corpus size or morphological characteristics.","score":1},{"url":"https://www.semanticscholar.org/paper/610088e1d7a8e44f921b2c4894fe1a7b204ce5a9","title":"Vicomtech at LivingNER2022","venue":"IberLEF@SEPLN","year":2022,"referenceCount":13,"citationCount":0,"influentialCitationCount":0,"authors":"Elena Zotova,Aitor García Pablos,Naiara Pérez,Pablo Turón,Montse Cuadros","id":"610088e1d7a8e44f921b2c4894fe1a7b204ce5a9","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/12809bcb734beafeb47876f42e7b438e27fe99fe","title":"General-purpose, long-context autoregressive modeling with Perceiver AR","venue":"International Conference on Machine Learning","year":2022,"referenceCount":74,"citationCount":16,"influentialCitationCount":3,"authors":"Curtis Hawthorne,Andrew Jaegle,Cătălina Cangea,Sebastian Borgeaud,C. Nash,Mateusz Malinowski,S. Dieleman,Oriol Vinyals,M. Botvinick,Ian Simon,Hannah R. Sheahan,Neil Zeghidour,Jean-Baptiste Alayrac,João Carreira,Jesse Engel","id":"12809bcb734beafeb47876f42e7b438e27fe99fe","summary":"Perceiver AR is developed, an modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms.","score":1},{"url":"https://www.semanticscholar.org/paper/7ed63a4f928fc126f6780d27e75fa6447a00a8c4","title":"Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference","venue":"NAACL","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"authors":"Emils Kadickis,Vaibhav Srivastav,Roman Klinger","id":"7ed63a4f928fc126f6780d27e75fa6447a00a8c4","summary":"This work proposes a simple method for predicting the performance without actually fine-tuning the model of a natural language inference model, and shows that the accuracy of the cosine similarity approach correlates strongly with theuracy of the classification approach with a Pearson correlation coefficient of 0.65.","score":1},{"url":"https://www.semanticscholar.org/paper/0376c01a9320027694f2dca57236c27f0c5b36eb","title":"Multilingual Abusiveness Identification on Code-Mixed Social Media Text","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"authors":"Ekagra Ranjan,Naman Poddar","id":"0376c01a9320027694f2dca57236c27f0c5b36eb","summary":"This work proposes an approach for abusiveness identiﬁcation on the multilingual Moj dataset which comprises of Indic languages and tackles the common challenges of non-English social media content and can be extended to other languages as well.","score":1},{"url":"https://www.semanticscholar.org/paper/34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":70,"citationCount":7,"influentialCitationCount":0,"authors":"Antoine Nzeyimana,Andre Niyongabo Rubungo","id":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","summary":"A simple yet effective two-tier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionality is proposed, naming the proposed model architecture KinyaBERT.","score":1},{"url":"https://www.semanticscholar.org/paper/ba9a592438447c2a35afc203be40f7f0a2d3fb5a","title":"A Compact Pretraining Approach for Neural Language Models","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"authors":"Shahriar Golchin,M. Surdeanu,N. Tavabi,A. Kiapour","id":"ba9a592438447c2a35afc203be40f7f0a2d3fb5a","summary":"This study shows that pretrained NLMs learn in-domain information more effectively and faster from a compact subset of the data that focuses on the key information in the domain.","score":1},{"url":"https://www.semanticscholar.org/paper/695ea88d22e7d39fa3d0b9f53e75d41e82be81d8","title":"Transformers with Learnable Activation Functions","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"authors":"Haishuo Fang,Ji-Ung Lee,N. Moosavi,Iryna Gurevych","id":"695ea88d22e7d39fa3d0b9f53e75d41e82be81d8","summary":"This paper investigates the effectiveness of using Rational Activation Function (RAF) that is a learnable activation function in the Transformer architecture and opens a new research direction for analyzing and interpreting pre-trained models according to the learned activation functions.","score":1},{"url":"https://www.semanticscholar.org/paper/923d2376103dbc9e9b2af4518b56299d7630b46b","title":"J URASSIC -1: T ECHNICAL D ETAILS AND E VALUATION","venue":"","year":2021,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"authors":"Opher Lieber","id":"923d2376103dbc9e9b2af4518b56299d7630b46b","summary":"Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1\"-Large\", and their architecture and training are described and their performance relative to GPT-3 is evaluated.","score":1},{"url":"https://www.semanticscholar.org/paper/518cb6d4247bdebf21e2811f296b0c7372602a0a","title":"PMI-Masking: Principled masking of correlated spans","venue":"International Conference on Learning Representations","year":2020,"referenceCount":30,"citationCount":31,"influentialCitationCount":7,"authors":"Yoav Levine,Barak Lenz,Opher Lieber,Omri Abend,Kevin Leyton-Brown,Moshe Tennenholtz,Y. Shoham","id":"518cb6d4247bdebf21e2811f296b0c7372602a0a","summary":"PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking, such as whole-word masks, entity/phrase masksing, and random-span masking.","score":1},{"url":"https://www.semanticscholar.org/paper/09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding","venue":"North American Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":52,"citationCount":13,"influentialCitationCount":2,"authors":"Dongling Xiao,Yukun Li,Han Zhang,Yu Sun,Hao Tian,Hua Wu,Haifeng Wang","id":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","summary":"ERNIE-Gram is proposed, an explicitly n-gram masking method to enhance the integration of coarse-grained information into pre-training and outperforms previous pre- training models like XLNet and RoBERTa by a large margin, and achieves comparable results with state-of-the-art methods.","score":1},{"url":"https://www.semanticscholar.org/paper/964da4ed9ac61b12bc2a7adc1e94e3964cc63861","title":"Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question Answering Data","venue":"EMNLP","year":2021,"referenceCount":80,"citationCount":2,"influentialCitationCount":0,"authors":"Dian Yu,Kai Sun,Dong Yu,Claire Cardie","id":"964da4ed9ac61b12bc2a7adc1e94e3964cc63861","summary":"A self-teaching paradigm is proposed to better use the generated weakly-labeled MRC instances to improve a target MRC task and the effectiveness of this framework and the usefulness of large-scale subjectarea question-answering data for machine reading comprehension are demonstrated.","score":1},{"url":"https://www.semanticscholar.org/paper/a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":40,"citationCount":27,"influentialCitationCount":4,"authors":"Xinyi Wang,Sebastian Ruder,Graham Neubig","id":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","summary":"To take full advantage of different possible input segmentations, the proposed Multi-view Subword Regularization (MVR) method enforces the consistency of predictors between using inputs tokenized by the standard and probabilistic segmentations.","score":1},{"url":"https://www.semanticscholar.org/paper/111dbe14083359ab39886790632e7f1421732a8a","title":"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":33,"citationCount":13,"influentialCitationCount":2,"authors":"Yuxuan Lai,Yijia Liu,Yansong Feng,Songfang Huang,Dongyan Zhao","id":"111dbe14083359ab39886790632e7f1421732a8a","summary":"This work proposes a novel pre-training paradigm for Chinese — Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner and achieves new state-of-the-art among base-size models on the CLUE benchmarks.","score":1},{"url":"https://www.semanticscholar.org/paper/7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining","venue":"ArXiv","year":2021,"referenceCount":37,"citationCount":6,"influentialCitationCount":1,"authors":"Chenglei Si,Zhengyan Zhang,Yingfa Chen,Fanchao Qi,Xiaozhi Wang,Zhiyuan Liu,Maosong Sun","id":"7da82508a76698f93e733d7a13d9fb13dbafba3e","summary":"It is found that SHUOWEN and JIEZI tokenizers can generally outperform conventional singlecharacter tokenizers, while Chinese word segmentation shows no benefit as a preprocessing step, and exhibit significantly better robustnesses on handling noisy texts.","score":1},{"url":"https://www.semanticscholar.org/paper/4690eb050572a279f94560b6bbdccaae577b45f5","title":"MVP-BERT: Multi-Vocab Pre-training for Chinese BERT","venue":"ACL","year":2021,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"authors":"Wei Zhu","id":"4690eb050572a279f94560b6bbdccaae577b45f5","summary":"Experiments show that MVP training strategies improve PLMs’ downstream performances, especially it can improve the PLM’s performances on span-level tasks, and the AL-MVP outperforms the recent AMBERT (CITATION) after large-scale pre-training, and it is more robust against adversarial attacks.","score":1},{"url":"https://www.semanticscholar.org/paper/27c39dd62635791a0ec3c0c81c2690e7a9bd62ad","title":"LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization","venue":"Findings","year":2021,"referenceCount":23,"citationCount":4,"influentialCitationCount":2,"authors":"Weidong Guo,Mingjun Zhao,Lusheng Zhang,Di Niu,Jinwen Luo,Zhenhua Liu,Zhenyang Li,J. Tang","id":"27c39dd62635791a0ec3c0c81c2690e7a9bd62ad","summary":"This paper proposes a simple yet effective pretraining method named LICHEE to efficiently incorporate multi-grained information of input text that can be applied to various pretrained language models and improve their representation capability.","score":1},{"url":"https://www.semanticscholar.org/paper/5722d101859846a6a023b8fa00830742203cd0c1","title":"Span Fine-tuning for Pre-trained Language Models","venue":"EMNLP","year":2021,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"authors":"Rongzhou Bao,Zhuosheng Zhang,Hai Zhao","id":"5722d101859846a6a023b8fa00830742203cd0c1","summary":"A novel span fine-tuning method for PrLMs is presented, which facilitates the span setting to be adaptively determined by specific downstream tasks during the fine- Tuning phase.","score":1},{"url":"https://www.semanticscholar.org/paper/976f47bade21fd787f029142b39631cb17f16ec2","title":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation","venue":"ArXiv","year":2021,"referenceCount":45,"citationCount":39,"influentialCitationCount":11,"authors":"Yunfan Shao,Zhichao Geng,Yitao Liu,Junqi Dai,Fei Yang,Li Zhe,H. Bao,Xipeng Qiu","id":"976f47bade21fd787f029142b39631cb17f16ec2","summary":"The unbalanced Transformer saves the computational and storage cost, which makes CPT competitive and greatly accelerates the inference of text generation.","score":1},{"url":"https://www.semanticscholar.org/paper/35eeeefcbea6fa8edae4c310ee5ee717861c7e60","title":"Improving Constituent Representation with Hypertree Neural Networks","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"authors":"Hao Zhou,Gongshen Liu,Kewei Tu","id":"35eeeefcbea6fa8edae4c310ee5ee717861c7e60","summary":"This paper aims to improve representations of constituent spans using a novel hypertree neural networks (HTNN) that is structured with constituency parse trees that incorporates both bottom-up and top-down compositional information.","score":1},{"url":"https://www.semanticscholar.org/paper/18dc24da603896db2264e9174116407a95c593d5","title":"“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction","venue":"FINDINGS","year":2022,"referenceCount":26,"citationCount":2,"influentialCitationCount":0,"authors":"Yong Dai,Linyang Li,Cong Zhou,Zhangyin Feng,Enbo Zhao,Xipeng Qiu,Pijian Li,Duyu Tang","id":"18dc24da603896db2264e9174116407a95c593d5","summary":"Three Chinese BERT models with standard character-level masking (CLM), WWM, and a combination of CLM and WWM are trained and it is found that when one character needs to be inserted or replaced, the model trained with CLM performs the best.","score":1},{"url":"https://www.semanticscholar.org/paper/8e1227b0d58b769a0919f8debdfd093ff6f6a65a","title":"MarkBERT: Marking Word Boundaries Improves Chinese BERT","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":2,"influentialCitationCount":0,"authors":"Linyang Li,Yong Dai,Duyu Tang,Zhangyin Feng,Cong Zhou,Xipeng Qiu,Zenglin Xu,Shuming Shi","id":"8e1227b0d58b769a0919f8debdfd093ff6f6a65a","summary":"A Chinese BERT model dubbed MarkBERT that uses word information and inserts boundary markers between contiguous words, which enables the model to handle any words in the same way, no matter they are OOV words or not is presented.","score":1},{"url":"https://www.semanticscholar.org/paper/05ea28584e5db18c0c31d1aac40e9c1905327557","title":"Effectiveness of Fine-tuned BERT Model in Classification of Helpful and Unhelpful Online Customer Reviews","venue":"Electronic Commerce Research","year":2022,"referenceCount":65,"citationCount":8,"influentialCitationCount":0,"authors":"Muhammad Bilal,A. A. Almazroi","id":"05ea28584e5db18c0c31d1aac40e9c1905327557","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/f8a2428bee464ae69cdb25ce5c2259e88ce576b4","title":"Clickbait Detection of Indonesian News Headlines using Fine-Tune Bidirectional Encoder Representations from Transformers (BERT)","venue":"Inform : Jurnal Ilmiah Bidang Teknologi Informasi dan Komunikasi","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"authors":"Diyah Utami Kusumaning Putri,Dinar Nugroho Pratomo","id":"f8a2428bee464ae69cdb25ce5c2259e88ce576b4","summary":"This study fine-tunes the Bidirectional Encoder Representations from Transformers (BERT) and uses the Indonesian news headlines dataset CLICK-ID to predict clickbait (BerT), and evaluation results indicate that all fine-tuned IndoBERT classifiers outperform all word-vectors-based machine learning classifiers in classifying Clickbait and non-clickbait IndonesianNews headlines.","score":1},{"url":"https://www.semanticscholar.org/paper/31d6e59dc275f33da71a72b6b86b38daeed9ffaa","title":"mmLayout: Multi-grained MultiModal Transformer for Document Understanding","venue":"ACM Multimedia","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"authors":"Wenjin Wang,Zhengjie Huang,Bin Luo,Qianglong Chen,Qiming Peng,Yinxu Pan,Weichong Yin,Shi Feng,Yu Sun,Dianhai Yu,Yin Zhang","id":"31d6e59dc275f33da71a72b6b86b38daeed9ffaa","summary":"Experimental results on four tasks, including information extraction and document question answering, show that the proposed method can improve the performance of multimodal Transformers based on fine-grained elements and achieve better performance with fewer parameters.","score":1},{"url":"https://www.semanticscholar.org/paper/04cd4c224f61e8f25a405103d4210f161d091d1c","title":"Look It Up: Bilingual Dictionaries Improve Neural Machine Translation","venue":"","year":2020,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"authors":"X. Zhong,David Chiang","id":"04cd4c224f61e8f25a405103d4210f161d091d1c","summary":"A new method for “attaching” dictionary definitions to rare words so that the network can learn the best way to use them and demonstrate provements of up to 1.8 BLEU using bilingual dictionaries.","score":1}]}